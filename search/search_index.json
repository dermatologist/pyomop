{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pyomop: OMOP Swiss Army Knife \ud83d\udd27","text":""},{"location":"#overview","title":"\u2728 Overview","text":"<p>pyomop is your OMOP Swiss Army Knife \ud83d\udd27 for working with OHDSI OMOP Common Data Model (CDM) v5.4 or v6 compliant databases using SQLAlchemy as the ORM. It supports converting query results to pandas DataFrames for machine learning pipelines and provides utilities for working with OMOP vocabularies. Table definitions are based on the omop-cdm library. Pyomop is designed to be a lightweight, easy-to-use library for researchers and developers experimenting and testing with OMOP CDM databases. It can be used both as a commandline tool and as an imported library in your code.</p> <ul> <li>Supports SQLite, PostgreSQL, and MySQL. CDM and Vocab tables are created in the same schema. (See usage below for more details)</li> <li>LLM-based natural language queries via llama-index. Usage.</li> <li>\ud83d\udd25 FHIR to OMOP conversion utilities. (See usage below for more details)</li> <li>Execute QueryLibrary. (See usage below for more details)</li> </ul> <p>Please \u2b50\ufe0f If you find this project useful!</p>"},{"location":"#installation","title":"Installation","text":"<p>Stable release:</p> <pre><code>pip install pyomop\n</code></pre> <p>Development version:</p> <pre><code>git clone https://github.com/dermatologist/pyomop.git\ncd pyomop\npip install -e .\n</code></pre> <p>LLM support:</p> <pre><code>pip install pyomop[llm]\n</code></pre> <p>See llm_example.py for usage.</p>"},{"location":"#docker","title":"Docker","text":"<ul> <li>A docker-compose is provided to quickly set up an environment with postgrs, webapi, atlas and a sql script to create a source in webapi. The script can be run using the <code>psql</code> command line tool or via the webapi UI. Please refresh after running the script by sending a request to /WebAPI/source/refresh.</li> </ul>"},{"location":"#usage","title":"\ud83d\udd27 Usage","text":"<pre><code>from pyomop import CdmEngineFactory, CdmVocabulary, CdmVector\n# cdm6 and cdm54 are supported\nfrom pyomop.cdm54 import Person, Cohort, Vocabulary, Base\nfrom sqlalchemy.future import select\nimport datetime\nimport asyncio\n\nasync def main():\n    cdm = CdmEngineFactory() # Creates SQLite database by default for fast testing\n    # cdm = CdmEngineFactory(db='pgsql', host='', port=5432,\n    #                       user='', pw='',\n    #                       name='', schema='')\n    # cdm = CdmEngineFactory(db='mysql', host='', port=3306,\n    #                       user='', pw='',\n    #                       name='')\n    engine = cdm.engine\n    # Comment the following line if using an existing database. Both cdm6 and cdm54 are supported, see the import statements above\n    await cdm.init_models(Base.metadata) # Initializes the database with the OMOP CDM tables\n    vocab = CdmVocabulary(cdm, version='cdm54') # or 'cdm6' for v6\n    # Uncomment the following line to create a new vocabulary from CSV files\n    # vocab.create_vocab('/path/to/csv/files')\n\n    # Add Persons\n    async with cdm.session() as session:  # type: ignore\n        async with session.begin():\n            session.add(\n                Person(\n                    person_id=100,\n                    gender_concept_id=8532,\n                    gender_source_concept_id=8512,\n                    year_of_birth=1980,\n                    month_of_birth=1,\n                    day_of_birth=1,\n                    birth_datetime=datetime.datetime(1980, 1, 1),\n                    race_concept_id=8552,\n                    race_source_concept_id=8552,\n                    ethnicity_concept_id=38003564,\n                    ethnicity_source_concept_id=38003564,\n                )\n            )\n            session.add(\n                Person(\n                    person_id=101,\n                    gender_concept_id=8532,\n                    gender_source_concept_id=8512,\n                    year_of_birth=1980,\n                    month_of_birth=1,\n                    day_of_birth=1,\n                    birth_datetime=datetime.datetime(1980, 1, 1),\n                    race_concept_id=8552,\n                    race_source_concept_id=8552,\n                    ethnicity_concept_id=38003564,\n                    ethnicity_source_concept_id=38003564,\n                )\n            )\n        await session.commit()\n\n    # Query the Person\n    stmt = select(Person).where(Person.person_id == 100)\n    result = await session.execute(stmt)\n    for row in result.scalars():\n        print(row)\n        assert row.person_id == 100\n\n    # Query the person pattern 2\n    person = await session.get(Person, 100)\n    print(person)\n    assert person.person_id == 100  # type: ignore\n\n    # Convert result to a pandas dataframe\n    vec = CdmVector()\n\n    # https://github.com/OHDSI/QueryLibrary/blob/master/inst/shinyApps/QueryLibrary/queries/person/PE02.md\n    result = await vec.query_library(cdm, resource='person', query_name='PE02')\n    df = vec.result_to_df(result)\n    print(\"DataFrame from result:\")\n    print(df.head())\n\n    result = await vec.execute(cdm, query='SELECT * from person;')\n    print(\"Executing custom query:\")\n    df = vec.result_to_df(result)\n    print(\"DataFrame from result:\")\n    print(df.head())\n\n    # access sqlalchemy result directly\n    for row in result:\n        print(row)\n\n    # Close session\n    await session.close()\n    await engine.dispose() # type: ignore\n\n# Run the main function\nasyncio.run(main())\n</code></pre>"},{"location":"#fhir-to-omop-mapping","title":"\ud83d\udd25 FHIR to OMOP mapping","text":"<p>pyomop can load FHIR Bulk Export (NDJSON) files into an OMOP CDM database.</p> <ul> <li>Sample datasets: https://github.com/smart-on-fhir/sample-bulk-fhir-datasets</li> <li>Remove any non-FHIR files (for example, <code>log.ndjson</code>) from the input folder.</li> <li>Download OMOP vocabulary CSV files (for example from OHDSI Athena) and place them in a folder.</li> </ul> <p>Run:</p> <pre><code>pyomop --create --vocab ~/Downloads/omop-vocab/ --input ~/Downloads/fhir/\n</code></pre> <p>This will create an OMOP CDM in SQLite, load the vocabulary files, and import the FHIR data from the input folder and reconcile vocabulary, mapping source_value to concept_id. The mapping is defined in the <code>mapping.example.json</code> file. The default mapping is here. Mapping happens in 5 steps as implemented here.</p> <ul> <li>Example using postgres (Docker)</li> </ul> <pre><code>pyomop --dbtype pgsql --host localhost --user postgres --pw mypass  --create --vocab ~/Downloads/omop-vocab/ --input ~/Downloads/fhir/\n</code></pre> <ul> <li>FHIR to data frame mapping is done with FHIRy</li> <li>Most of the code for this functionality was written by an LLM agent. The prompts used are here</li> </ul>"},{"location":"#command-line","title":"Command-line","text":"<pre><code>  -c, --create                Create CDM tables (see --version).\n  -t, --dbtype TEXT           Database Type for creating CDM (sqlite, mysql or\n                              pgsql)\n  -h, --host TEXT             Database host\n  -p, --port TEXT             Database port\n  -u, --user TEXT             Database user\n  -w, --pw TEXT               Database password\n  -v, --version TEXT          CDM version (cdm54 (default) or cdm6)\n  -n, --name TEXT             Database name\n  -s, --schema TEXT           Database schema (for pgsql)\n  -i, --vocab TEXT            Folder with vocabulary files (csv) to import\n  -f, --input DIRECTORY       Input folder with FHIR bundles or ndjson files.\n  -e, --eunomia-dataset TEXT  Download and load Eunomia dataset (e.g.,\n                              'GiBleed', 'Synthea')\n  --eunomia-path TEXT         Path to store/find Eunomia datasets (uses\n                              EUNOMIA_DATA_FOLDER env var if not specified)\n  --connection-info           Display connection information for the database (For R package compatibility)\n  --mcp-server                Start MCP server for stdio interaction\n  --pyhealth-path TEXT        Path to export PyHealth compatible CSV files\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"#mcp-server","title":"MCP Server","text":"<p>pyomop includes an MCP (Model Context Protocol) server that exposes tools for interacting with OMOP CDM databases. This allows MCP clients to create databases, load data, and execute SQL statements.</p> <p> </p>"},{"location":"#starting-the-mcp-server","title":"Starting the MCP Server","text":"<p>To start the MCP server for stdio interaction:</p> <pre><code># Using the main CLI\npyomop --mcp-server\n\n\n\n### Available MCP Tools\n\n- **create_cdm**: Create an empty CDM database\n- **create_eunomia**: Add Eunomia sample dataset\n- **get_table_columns**: Get column names for a specific table\n- **get_single_table_info**: Get detailed table information including foreign keys\n- **get_usable_table_names**: Get list of all available table names\n- **run_sql**: Execute SQL statements with error handling\n\n* create_cdm and create_eunomia supports only local sqlite databases to avoid inadvertent data loss in production databases.\n### Available Prompts\n\n- **query_execution_steps**: Provides step-by-step guidance for executing database queries based on free text instructions\n\n### Usage with MCP Clients\n\nThe server communicates via stdio and can be used with any MCP-compatible client. Example configuration for [vscode](/.vscode/mcp.json):\n\n```json\n{\n  \"servers\": {\n      \"pyomop\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"pyomop\", \"--mcp-server\"]\n    }\n  }\n}\n</code></pre> <ul> <li>If the vocabulary is not installed locally or advanced vocabulary support is required from Athena, it is recommended to combine omop_mcp with PyOMOP.</li> </ul>"},{"location":"#eunomia-import-and-cohort-creation","title":"Eunomia import and cohort creation","text":"<pre><code>pyomop -e Synthea27Nj -v 5.4 --connection-info\npyomop -e GiBleed -v 5.3 --connection-info\n</code></pre>"},{"location":"#pyhealth-and-plp-compatibility-for-machine-learning-pipelines","title":"PyHealth and PLP Compatibility (For Machine Learning pipelines)","text":"<p>pyomop supports exporting OMOP CDM data in a format compatible with PyHealth, a machine learning library for healthcare data analysis using <code>--export-pyhealth</code> option (See Notebook). Additionally, you can export the connection information for use with the various R packages such as PatientLevelPrediction using the <code>--connection-info</code> option.</p> <pre><code>pyomop -e GiBleed -v 5.3 --connection-info --pyhealth-path ~/pyhealth\n</code></pre>"},{"location":"#additional-tools","title":"Additional Tools","text":"<ul> <li>Convert FHIR to pandas DataFrame: fhiry</li> <li>.NET and Golang OMOP CDM: .NET, Golang</li> </ul>"},{"location":"#supported-databases","title":"Supported Databases","text":"<ul> <li>PostgreSQL</li> <li>MySQL</li> <li>SQLite</li> </ul>"},{"location":"#environment-variables-for-database-connection","title":"Environment Variables for Database Connection","text":"<p>You can configure database connection parameters using environment variables. These will be used as defaults by pyomop and the MCP server:</p> <ul> <li><code>PYOMOP_DB</code>: Database type (<code>sqlite</code>, <code>mysql</code>, <code>pgsql</code>)</li> <li><code>PYOMOP_HOST</code>: Database host</li> <li><code>PYOMOP_PORT</code>: Database port</li> <li><code>PYOMOP_USER</code>: Database user</li> <li><code>PYOMOP_PW</code>: Database password</li> <li><code>PYOMOP_SCHEMA</code>: Database schema (for PostgreSQL)</li> </ul> <p>Example usage:</p> <pre><code>export PYOMOP_DB=pgsql\nexport PYOMOP_HOST=localhost\nexport PYOMOP_PORT=5432\nexport PYOMOP_USER=postgres\nexport PYOMOP_PW=mypass\nexport PYOMOP_SCHEMA=omop\n</code></pre> <p>These environment variables will be checked before assigning default values for database connection in pyomop and MCP server tools.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Pull requests are welcome! See CONTRIBUTING.md.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Bell Eapen </li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request \u2013 Add PyHealth compatible export function #242</li> <li>Add PyHealth compatible export function #243 (Copilot)</li> </ul> <p>Merged pull requests:</p> <ul> <li>Add MCP server feature for CDM database interaction #241 (Copilot)</li> <li>Add OMOP CDM sample data functionality via Eunomia port #237 (Copilot)</li> <li>[GitHub Dependents Info] Updated markdown file #234 (github-actions[bot])</li> <li>[Automated] Dependencies upgrade #233 (github-actions[bot])</li> <li>Feature/hot fix 3 #232 (dermatologist)</li> <li>Feature/docker 1 #230 (dermatologist)</li> <li>Feature/hotfix 2 #229 (dermatologist)</li> <li>Feature/fhir to omop 2 #228 (dermatologist)</li> <li>Feature/docker db #225 (dermatologist)</li> <li>feat: update CLI options and add version support; introduce test for \u2026 #224 (dermatologist)</li> <li>Feature/new model #223 (dermatologist)</li> <li>Feature/refactor 1 #222 (dermatologist)</li> <li>build: update Python version to 3.11 in Dockerfile and devcontainer.j\u2026 #221 (dermatologist)</li> <li>build\\(deps\\): bump astral-sh/setup-uv from 5 to 6 #220 (dependabot[bot])</li> <li>Feature/llm update 1 #219 (dermatologist)</li> <li>Feature/pr new 1 #218 (dermatologist)</li> <li>refactor: clean up code formatting and remove unnecessary newlines #204 (dermatologist)</li> <li>build\\(deps\\): bump jinja2 from 3.1.4 to 3.1.5 #200 (dependabot[bot])</li> <li>build\\(deps-dev\\): bump wheel from 0.37.1 to 0.45.1 #195 (dependabot[bot])</li> <li>Feature/update deps 1 #194 (dermatologist)</li> <li>build\\(deps\\): bump actions/setup-python from 5.0.0 to 5.3.0 #190 (dependabot[bot])</li> <li>build\\(deps\\): bump certifi from 2023.7.22 to 2024.7.4 #178 (dependabot[bot])</li> <li>build\\(deps\\): bump jinja2 from 3.1.3 to 3.1.4 #169 (dependabot[bot])</li> <li>build\\(deps\\): bump requests from 2.31.0 to 2.32.2 #168 (dependabot[bot])</li> <li>build\\(deps\\): bump urllib3 from 1.26.18 to 1.26.19 #167 (dependabot[bot])</li> <li>build\\(deps\\): bump jinja2 from 3.0.1 to 3.1.3 #161 (dependabot[bot])</li> <li>Feature/llm llama index 1 #159 (dermatologist)</li> <li>build\\(deps\\): bump actions/setup-python from 2.3.1 to 5.0.0 #155 (dependabot[bot])</li> <li>Feature/update deps 1 #151 (dermatologist)</li> <li>SQLAlchemy &gt;2 fix #6 #150 (dermatologist)</li> <li>build\\(deps\\): bump actions/checkout from 3 to 4 #143 (dependabot[bot])</li> </ul>"},{"location":"changelog/#v600-2025-09-18","title":"v6.0.0 (2025-09-18)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature Request: Create a mcp-server feature #240</li> </ul>"},{"location":"changelog/#v550-2025-09-17","title":"v5.5.0 (2025-09-17)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Provide OMOP CDM sample data sets #236</li> </ul> <p>Closed issues:</p> <ul> <li>\u2728 Set up Copilot instructions #238</li> </ul>"},{"location":"changelog/#v541-2025-08-18","title":"v5.4.1 (2025-08-18)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v540-2025-08-17","title":"v5.4.0 (2025-08-17)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v530-2025-08-16","title":"v5.3.0 (2025-08-16)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v521-2025-08-16","title":"v5.2.1 (2025-08-16)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v520-2025-08-16","title":"v5.2.0 (2025-08-16)","text":"<p>Full Changelog</p>"},{"location":"changelog/#510-2025-06-22","title":"5.1.0 (2025-06-22)","text":"<p>Full Changelog</p>"},{"location":"changelog/#500-2025-06-22","title":"5.0.0 (2025-06-22)","text":"<p>Full Changelog</p>"},{"location":"changelog/#441-2025-05-29","title":"4.4.1 (2025-05-29)","text":"<p>Full Changelog</p>"},{"location":"changelog/#440-2025-05-09","title":"4.4.0 (2025-05-09)","text":"<p>Full Changelog</p>"},{"location":"changelog/#430-2024-07-11","title":"4.3.0 (2024-07-11)","text":"<p>Full Changelog</p>"},{"location":"changelog/#420-2023-11-19","title":"4.2.0 (2023-11-19)","text":"<p>Full Changelog</p>"},{"location":"changelog/#410-2023-11-19","title":"4.1.0 (2023-11-19)","text":"<p>Full Changelog</p>"},{"location":"changelog/#400-2023-11-19","title":"4.0.0 (2023-11-19)","text":"<p>Full Changelog</p>"},{"location":"changelog/#320-2023-01-20","title":"3.2.0 (2023-01-20)","text":"<p>Full Changelog</p>"},{"location":"changelog/#310-2021-09-17","title":"3.1.0 (2021-09-17)","text":"<p>Full Changelog</p>"},{"location":"changelog/#300-2020-10-23","title":"3.0.0 (2020-10-23)","text":"<p>Full Changelog</p>"},{"location":"changelog/#200-2020-06-27","title":"2.0.0 (2020-06-27)","text":"<p>Full Changelog</p>"},{"location":"changelog/#120-2020-05-23","title":"1.2.0 (2020-05-23)","text":"<p>Full Changelog</p>"},{"location":"changelog/#111-2020-05-03","title":"1.1.1 (2020-05-03)","text":"<p>Full Changelog</p>"},{"location":"changelog/#110-2020-05-03","title":"1.1.0 (2020-05-03)","text":"<p>Full Changelog</p>"},{"location":"changelog/#100-2020-05-03","title":"1.0.0 (2020-05-03)","text":"<p>Full Changelog</p>"},{"location":"changelog/#010-2020-05-03","title":"0.1.0 (2020-05-03)","text":"<p>Full Changelog</p> <p>* This Changelog was automatically generated by github_changelog_generator</p>"},{"location":"contributing/","title":"How to contribute","text":""},{"location":"contributing/#please-note","title":"Please note:","text":"<ul> <li>(Optional) We adopt Git Flow. Most feature branches are pushed to the repository and deleted when merged to develop branch.</li> <li>(Important): Submit pull requests to the develop branch or feature/ branches</li> <li>Use GitHub Issues for feature requests and bug reports. Include as much information as possible while reporting bugs.  </li> </ul>"},{"location":"contributing/#contributing-step-by-step","title":"Contributing (Step-by-step)","text":"<ol> <li> <p>Fork the repo and clone it to your local computer, and set up the upstream remote:</p> <pre><code>git clone https://github.com/YourGithubUsername/pyomop.git\ncd pyomop\ngit remote add upstream https://github.com/dermatologist/pyomop.git\n</code></pre> </li> <li> <p>Checkout out a new local branch based on your master and update it to the latest (TRUNK-123 is the branch name, You can name it whatever you want. Try to give it a meaningful name. If you are fixing an issue, please include the issue #).</p> <pre><code>git checkout -b BRANCH-123 develop\ngit clean -df\ngit pull --rebase upstream develop\n</code></pre> </li> </ol> <p>Please keep your code clean. If you find another bug, you want to fix while being in a new branch, please fix it in a separated branch instead.</p> <ol> <li> <p>Push the branch to your fork. Treat it as a backup.</p> <pre><code>git push origin BRANCH-123\n</code></pre> </li> <li> <p>Code</p> </li> <li> <p>Adhere to common conventions you see in the existing code.</p> </li> <li> <p>Include tests as much as possible, and ensure they pass.</p> </li> <li> <p>Commit to your branch</p> <pre><code> git commit -m \"BRANCH-123: Put change summary here (can be a ticket title)\"\n</code></pre> </li> </ol> <p>NEVER leave the commit message blank! Provide a detailed, clear, and complete description of your commit!</p> <ol> <li> <p>Update your branch to the latest code.</p> <pre><code>git pull --rebase upstream develop\n</code></pre> </li> <li> <p>Important If you have made many commits, please squash them into atomic units of work. (Most Git GUIs such as sourcetree and smartgit offer a squash option)</p> <pre><code>git checkout develop\ngit pull --rebase upstream develop\ngit merge --squash BRANCH-123\ngit commit -m \"fix: 123\"\n</code></pre> </li> </ol> <p>Push changes to your fork:</p> <pre><code>    git push\n</code></pre> <ol> <li>Issue a Pull Request</li> </ol> <p>In order to make a pull request:   * Click \"Pull Request\".   * Choose the develop branch   * Click 'Create pull request'   * Fill in some details about your potential patch including a meaningful title.   * Click \"Create pull request\".</p> <p>Thanks for that -- we'll get to your pull request ASAP. We love pull requests!</p>"},{"location":"contributing/#feedback","title":"Feedback","text":"<p>If you need to contact me, see my contact details on my profile page.</p>"},{"location":"github-dependents-info/","title":"Dependents stats for dermatologist/pyomop","text":"Repository Stars alphavector / all 5 Lyn4ever29 / pipy_server 3 <p>Generated using github-dependents-info, by Nicolas Vuillamy</p>"},{"location":"modules/","title":"Modules","text":"<p>Derivative based on the original work here: https://github.com/thehyve/omop-cdm/blob/main/src/omop_cdm/regular/cdm600/tables.py Modifications made to this file: - Removed support for schema. - Added new tables</p> <ul> <li>Licensed under the Apache License, Version 2.0 (the \"License\");</li> <li>you may not use this file except in compliance with the License.</li> <li>You may obtain a copy of the License at *</li> <li>https://www.apache.org/licenses/LICENSE-2.0 *</li> <li>Unless required by applicable law or agreed to in writing, software</li> <li>distributed under the License is distributed on an \"AS IS\" BASIS,</li> <li>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</li> <li>See the License for the specific language governing permissions and</li> <li>limitations under the License.</li> </ul> <p>Derivative based on the original work here: https://github.com/thehyve/omop-cdm/blob/main/src/omop_cdm/regular/cdm54/tables.py Modifications made to this file: - Removed support for schema. - Added new tables</p> <ul> <li>Licensed under the Apache License, Version 2.0 (the \"License\");</li> <li>you may not use this file except in compliance with the License.</li> <li>You may obtain a copy of the License at *</li> <li>https://www.apache.org/licenses/LICENSE-2.0 *</li> <li>Unless required by applicable law or agreed to in writing, software</li> <li>distributed under the License is distributed on an \"AS IS\" BASIS,</li> <li>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</li> <li>See the License for the specific language governing permissions and</li> <li>limitations under the License.</li> </ul> <p>Engine and session factory for OMOP CDM databases.</p> <p>This module provides an asynchronous SQLAlchemy engine factory with helpers to create/init CDM schemas and obtain async sessions across supported backends (SQLite, MySQL, PostgreSQL).</p> <p>LLM-oriented SQLDatabase wrapper for OMOP CDM.</p> <p>This module provides a thin wrapper around <code>llama_index.core.SQLDatabase</code> that is aware of the OMOP CDM metadata reflected from this package's SQLAlchemy models. It enables LLM-powered query components to reason about available tables, columns, and foreign keys using the OMOP metadata directly.</p> <p>This file is import-safe even when the optional LLM extras are not installed; in that case, attempting to instantiate <code>CDMDatabase</code> will raise a clear ImportError directing you to install <code>pyomop[llm]</code>.</p> <p>LLM query utilities over the OMOP CDM schema.</p> <p>This module wires llama-index components to an OMOP-aware <code>CDMDatabase</code> so you can build semantic and SQL-first query engines that know about your CDM tables. All LLM-related imports are optional and performed lazily at runtime.</p> <p>CSV-to-OMOP loader.</p> <p>This module implements a flexible CSV loader that can populate multiple OMOP CDM tables according to a JSON mapping file. It also performs helpful cleanup operations like foreign key normalization, birthdate backfilling, gender mapping, and concept code lookups.</p> <p>Command-line interface for pyomop.</p> <p>Provides commands to create CDM tables, load vocabulary CSVs, and import FHIR Bulk Export data into an OMOP database.</p> <p>Vocabulary utilities for loading and querying OMOP vocab tables.</p> <p>Provides helpers to import vocabulary CSVs into the database and to look up concepts by id or code. Uses async SQLAlchemy sessions.</p> <p>Utilities to execute queries and convert results to DataFrames.</p> <p>Exposes a small helper class around async SQLAlchemy execution and integration with OHDSI QueryLibrary.</p> <p>Predefined OMOP SQL snippets from the OHDSI Query Library.</p> <p>This module exposes a small dictionary of named SQL queries that can be used for demos, tests, or quick analytics over a CDM instance.</p> <p>Source: https://github.com/OHDSI/QueryLibrary/tree/master/inst/shinyApps/QueryLibrary/queries</p>"},{"location":"modules/#engine_factory.CdmEngineFactory","title":"<code>CdmEngineFactory</code>","text":"<p>               Bases: <code>object</code></p> <p>Factory to create async SQLAlchemy engines and sessions for OMOP CDM.</p> <p>Supports SQLite (default), MySQL, and PostgreSQL. Exposes convenience properties for the configured engine and async session maker.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <p>Database type: \"sqlite\", \"mysql\", or \"pgsql\".</p> <code>None</code> <code>host</code> <p>Database host (ignored for SQLite).</p> <code>None</code> <code>port</code> <p>Database port (ignored for SQLite).</p> <code>None</code> <code>user</code> <p>Database user (ignored for SQLite).</p> <code>None</code> <code>pw</code> <p>Database password (ignored for SQLite).</p> <code>None</code> <code>name</code> <p>Database name or SQLite filename.</p> <code>None</code> <code>schema</code> <p>PostgreSQL schema to use for CDM.</p> <code>None</code> Source code in <code>src/pyomop/engine_factory.py</code> <pre><code>class CdmEngineFactory(object):\n    \"\"\"Factory to create async SQLAlchemy engines and sessions for OMOP CDM.\n\n    Supports SQLite (default), MySQL, and PostgreSQL. Exposes convenience\n    properties for the configured engine and async session maker.\n\n    Args:\n        db: Database type: \"sqlite\", \"mysql\", or \"pgsql\".\n        host: Database host (ignored for SQLite).\n        port: Database port (ignored for SQLite).\n        user: Database user (ignored for SQLite).\n        pw: Database password (ignored for SQLite).\n        name: Database name or SQLite filename.\n        schema: PostgreSQL schema to use for CDM.\n    \"\"\"\n\n    def __init__(\n        self,\n        db=None,\n        host=None,\n        port=None,\n        user=None,\n        pw=None,\n        name=None,\n        schema=None,\n    ):\n        import os\n\n        self._db = db or os.environ.get(\"PYOMOP_DB\", \"sqlite\")\n        self._name = name or os.environ.get(\"PYOMOP_NAME\", \"cdm.sqlite\")\n        self._host = host or os.environ.get(\"PYOMOP_HOST\", \"localhost\")\n        self._port = (\n            port if port is not None else int(os.environ.get(\"PYOMOP_PORT\", \"5432\"))\n        )\n        self._user = user or os.environ.get(\"PYOMOP_USER\", \"root\")\n        self._pw = pw or os.environ.get(\"PYOMOP_PW\", \"pass\")\n        self._schema = schema or os.environ.get(\"PYOMOP_SCHEMA\", \"\")\n        self._engine = None\n        self._base = None\n\n    async def init_models(self, metadata):\n        \"\"\"Drop and re-create all tables from provided metadata.\n\n        This is mainly used for tests and quick local setups.\n\n        Args:\n            metadata: SQLAlchemy ``MetaData`` containing table definitions.\n\n        Raises:\n            ValueError: If the engine has not been initialized.\n        \"\"\"\n        if self._engine is None:\n            raise ValueError(\"Database engine is not initialized.\")\n        async with self._engine.begin() as conn:\n            await conn.run_sync(metadata.drop_all)\n            await conn.run_sync(metadata.create_all)\n\n    @property\n    def db(self):\n        \"\"\"Return the configured database type (sqlite/mysql/pgsql).\"\"\"\n        return self._db\n\n    @property\n    def host(self):\n        \"\"\"Return the configured database host (if applicable).\"\"\"\n        return self._host\n\n    @property\n    def port(self):\n        \"\"\"Return the configured database port (if applicable).\"\"\"\n        return self._port\n\n    @property\n    def name(self):\n        \"\"\"Return the configured database name or SQLite filename.\"\"\"\n        return self._name\n\n    @property\n    def user(self):\n        \"\"\"Return the configured database user (if applicable).\"\"\"\n        return self._user\n\n    @property\n    def pw(self):\n        \"\"\"Return the configured database password (if applicable).\"\"\"\n        return self._pw\n\n    @property\n    def schema(self):\n        \"\"\"Return the configured schema (PostgreSQL).\"\"\"\n        return self._schema\n\n    @property\n    def base(self):\n        \"\"\"Return automapped classes when an engine exists, otherwise None.\"\"\"\n        if self.engine is not None:  # Not self_engine\n            Base = automap_base()\n            Base.prepare(self.engine, reflect=True)\n            return Base.classes\n        return None\n\n    @property\n    def engine(self):\n        \"\"\"Create or return the async engine for the configured backend.\n\n        Returns:\n            Async engine instance bound to the configured database.\n        \"\"\"\n        if self._db == \"sqlite\":\n            # Schemas are not supported for SQLite; warn if a non-default schema was provided\n            if self._schema and self._schema not in (\"\", \"public\"):\n                logger.warning(\n                    \"Schema is not supported for SQLite; ignoring schema='%s'\",\n                    self._schema,\n                )\n            self._engine = create_async_engine(\"sqlite+aiosqlite:///\" + self._name)\n        elif self._db == \"mysql\":\n            # Schemas are not supported for MySQL in the same way as PostgreSQL; warn and ignore\n            if self._schema and self._schema not in (\"\", \"public\"):\n                logger.warning(\n                    \"Schema is not supported for MySQL; ignoring schema='%s'\",\n                    self._schema,\n                )\n            mysql_url = \"mysql://{}:{}@{}:{}/{}\"\n            mysql_url = mysql_url.format(\n                self._user, self._pw, self._host, self._port, self._name\n            )\n            self._engine = create_async_engine(\n                mysql_url, isolation_level=\"READ UNCOMMITTED\"\n            )\n        elif self._db == \"pgsql\":\n            pgsql_url = \"postgresql+asyncpg://{}:{}@{}:{}/{}\"\n            pgsql_url = pgsql_url.format(\n                self._user, self._pw, self._host, self._port, self._name\n            )\n            connect_args = {}\n            # If a schema is provided, set the PostgreSQL search_path so that all\n            # operations (reflection, DDL/DML) use this schema by default.\n            if self._schema and self._schema != \"\":\n                connect_args = {\"server_settings\": {\"search_path\": self._schema}}\n            self._engine = create_async_engine(pgsql_url, connect_args=connect_args)\n        else:\n            # Unknown DB type; create no engine and warn\n            logger.warning(\"Unknown database type '%s'\u2014no engine created.\", self._db)\n            return None\n        return self._engine\n\n    @property\n    def session(self):\n        \"\"\"Return an async_sessionmaker for creating AsyncSession objects.\"\"\"\n        if self._engine is not None:\n            async_session = async_sessionmaker(\n                self._engine, expire_on_commit=False, class_=AsyncSession\n            )\n            return async_session\n        return None\n\n    @property\n    def async_session(self):\n        \"\"\"Alias for session to maintain backward compatibility.\"\"\"\n        if self._engine is not None:\n            async_session = async_sessionmaker(\n                self._engine, expire_on_commit=False, class_=AsyncSession\n            )\n            return async_session\n        return None\n\n    @db.setter\n    def db(self, value):\n        self._db = value\n\n    @name.setter\n    def name(self, value):\n        self._name = value\n\n    @port.setter\n    def port(self, value):\n        self._port = value\n\n    @host.setter\n    def host(self, value):\n        self._host = value\n\n    @user.setter\n    def user(self, value):\n        self._user = value\n\n    @pw.setter\n    def pw(self, value):\n        self._pw = value\n\n    @schema.setter\n    def schema(self, value):\n        self._schema = value\n\n    def print_connection_info(self):\n        \"\"\"Return a string with the connection details (for logging/display).\"\"\"\n        if self._db == \"sqlite\":\n            current_directory = os.getcwd()\n            _server = os.path.join(current_directory, \"cdm.sqlite\")\n            print(\n                f\"connectionDetails &lt;- DatabaseConnector::createConnectionDetails(\\n\"\n                f'    dbms = \"sqlite\", server = \"{_server}\")\\n'\n            )\n        elif self._db == \"mysql\":\n            return f\"MySQL database '{self._name}' on {self._host}:{self._port} as user '{self._user}'\"\n        elif self._db == \"pgsql\":\n            schema_info = f\", schema '{self._schema}'\" if self._schema else \"\"\n            return f\"PostgreSQL database '{self._name}' on {self._host}:{self._port} as user '{self._user}'{schema_info}\"\n        else:\n            return \"No valid database connection configured.\"\n</code></pre>"},{"location":"modules/#engine_factory.CdmEngineFactory.async_session","title":"<code>async_session</code>  <code>property</code>","text":"<p>Alias for session to maintain backward compatibility.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.base","title":"<code>base</code>  <code>property</code>","text":"<p>Return automapped classes when an engine exists, otherwise None.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.db","title":"<code>db</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database type (sqlite/mysql/pgsql).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.engine","title":"<code>engine</code>  <code>property</code>","text":"<p>Create or return the async engine for the configured backend.</p> <p>Returns:</p> Type Description <p>Async engine instance bound to the configured database.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.host","title":"<code>host</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database host (if applicable).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.name","title":"<code>name</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database name or SQLite filename.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.port","title":"<code>port</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database port (if applicable).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.pw","title":"<code>pw</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database password (if applicable).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.schema","title":"<code>schema</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured schema (PostgreSQL).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.session","title":"<code>session</code>  <code>property</code>","text":"<p>Return an async_sessionmaker for creating AsyncSession objects.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.user","title":"<code>user</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database user (if applicable).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.init_models","title":"<code>init_models(metadata)</code>  <code>async</code>","text":"<p>Drop and re-create all tables from provided metadata.</p> <p>This is mainly used for tests and quick local setups.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>SQLAlchemy <code>MetaData</code> containing table definitions.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the engine has not been initialized.</p> Source code in <code>src/pyomop/engine_factory.py</code> <pre><code>async def init_models(self, metadata):\n    \"\"\"Drop and re-create all tables from provided metadata.\n\n    This is mainly used for tests and quick local setups.\n\n    Args:\n        metadata: SQLAlchemy ``MetaData`` containing table definitions.\n\n    Raises:\n        ValueError: If the engine has not been initialized.\n    \"\"\"\n    if self._engine is None:\n        raise ValueError(\"Database engine is not initialized.\")\n    async with self._engine.begin() as conn:\n        await conn.run_sync(metadata.drop_all)\n        await conn.run_sync(metadata.create_all)\n</code></pre>"},{"location":"modules/#engine_factory.CdmEngineFactory.print_connection_info","title":"<code>print_connection_info()</code>","text":"<p>Return a string with the connection details (for logging/display).</p> Source code in <code>src/pyomop/engine_factory.py</code> <pre><code>def print_connection_info(self):\n    \"\"\"Return a string with the connection details (for logging/display).\"\"\"\n    if self._db == \"sqlite\":\n        current_directory = os.getcwd()\n        _server = os.path.join(current_directory, \"cdm.sqlite\")\n        print(\n            f\"connectionDetails &lt;- DatabaseConnector::createConnectionDetails(\\n\"\n            f'    dbms = \"sqlite\", server = \"{_server}\")\\n'\n        )\n    elif self._db == \"mysql\":\n        return f\"MySQL database '{self._name}' on {self._host}:{self._port} as user '{self._user}'\"\n    elif self._db == \"pgsql\":\n        schema_info = f\", schema '{self._schema}'\" if self._schema else \"\"\n        return f\"PostgreSQL database '{self._name}' on {self._host}:{self._port} as user '{self._user}'{schema_info}\"\n    else:\n        return \"No valid database connection configured.\"\n</code></pre>"},{"location":"modules/#llm_engine.CDMDatabase","title":"<code>CDMDatabase</code>","text":"<p>               Bases: <code>SQLDatabase</code></p> <p>OMOP-aware SQLDatabase for LLM query engines.</p> <p>This class adapts llama-index's <code>SQLDatabase</code> to use the OMOP CDM SQLAlchemy metadata bundled with this package, making it easy to expose concise schema information to LLM components.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy <code>Engine</code> connected to the OMOP database.</p> required <code>schema</code> <code>Optional[str]</code> <p>Optional database schema name.</p> <code>None</code> <code>ignore_tables</code> <code>Optional[List[str]]</code> <p>Tables to hide from the LLM context.</p> <code>None</code> <code>include_tables</code> <code>Optional[List[str]]</code> <p>Explicit subset of tables to expose.</p> <code>None</code> <code>sample_rows_in_table_info</code> <code>int</code> <p>Kept for API parity (unused here).</p> <code>3</code> <code>indexes_in_table_info</code> <code>bool</code> <p>Kept for API parity (unused here).</p> <code>False</code> <code>custom_table_info</code> <code>Optional[dict]</code> <p>Optional overrides for table descriptions.</p> <code>None</code> <code>view_support</code> <code>bool</code> <p>Whether to reflect views as well (unused here).</p> <code>False</code> <code>max_string_length</code> <code>int</code> <p>Max length of generated descriptions.</p> <code>300</code> <code>version</code> <code>str</code> <p>OMOP CDM version label (\"cdm54\" or \"cdm6\").</p> <code>'cdm54'</code> Source code in <code>src/pyomop/llm_engine.py</code> <pre><code>class CDMDatabase(SQLDatabase):\n    \"\"\"OMOP-aware SQLDatabase for LLM query engines.\n\n    This class adapts llama-index's ``SQLDatabase`` to use the OMOP CDM\n    SQLAlchemy metadata bundled with this package, making it easy to expose\n    concise schema information to LLM components.\n\n    Args:\n        engine: SQLAlchemy ``Engine`` connected to the OMOP database.\n        schema: Optional database schema name.\n        ignore_tables: Tables to hide from the LLM context.\n        include_tables: Explicit subset of tables to expose.\n        sample_rows_in_table_info: Kept for API parity (unused here).\n        indexes_in_table_info: Kept for API parity (unused here).\n        custom_table_info: Optional overrides for table descriptions.\n        view_support: Whether to reflect views as well (unused here).\n        max_string_length: Max length of generated descriptions.\n        version: OMOP CDM version label (\"cdm54\" or \"cdm6\").\n    \"\"\"\n\n    def __init__(\n        self,\n        engine: Engine,\n        schema: Optional[str] = None,\n        ignore_tables: Optional[List[str]] = None,\n        include_tables: Optional[List[str]] = None,\n        sample_rows_in_table_info: int = 3,\n        indexes_in_table_info: bool = False,\n        custom_table_info: Optional[dict] = None,\n        view_support: bool = False,\n        max_string_length: int = 300,\n        version: str = \"cdm54\",\n    ) -&gt; None:\n        if not _LLM_AVAILABLE:  # pragma: no cover - import-safe guard\n            raise ImportError(\"Install 'pyomop[llm]' to use LLM features.\")\n\n        # Basic configuration\n        self._engine = engine\n        self._schema = schema\n\n        if include_tables and ignore_tables:\n            raise ValueError(\"Cannot specify both include_tables and ignore_tables\")\n\n        # Load OMOP metadata for the chosen version\n        if version == \"cdm6\":\n            from .cdm6 import Base\n        else:\n            from .cdm54 import Base\n        metadata: MetaData = Base.metadata\n\n        # All known tables (no view reflection here)\n        self._all_tables = set(metadata.tables.keys())\n\n        # Validate include/ignore lists\n        self._include_tables = set(include_tables) if include_tables else set()\n        if self._include_tables:\n            missing = self._include_tables - self._all_tables\n            if missing:\n                raise ValueError(f\"include_tables {missing} not found in OMOP metadata\")\n\n        self._ignore_tables = set(ignore_tables) if ignore_tables else set()\n        if self._ignore_tables:\n            missing = self._ignore_tables - self._all_tables\n            if missing:\n                raise ValueError(f\"ignore_tables {missing} not found in OMOP metadata\")\n\n        if self._include_tables:\n            usable = set(self._include_tables)\n        elif self._ignore_tables:\n            usable = self._all_tables - self._ignore_tables\n        else:\n            usable = set(self._all_tables)\n        self._usable_tables = usable\n\n        if not isinstance(sample_rows_in_table_info, int):\n            raise TypeError(\"sample_rows_in_table_info must be an integer\")\n        self._sample_rows_in_table_info = sample_rows_in_table_info\n        self._indexes_in_table_info = indexes_in_table_info\n\n        # Optional custom descriptions\n        self._custom_table_info = custom_table_info\n        if self._custom_table_info is not None:\n            if not isinstance(self._custom_table_info, dict):\n                raise TypeError(\n                    \"custom_table_info must be a dict of {table_name: description}\"\n                )\n            self._custom_table_info = {\n                t: info\n                for t, info in self._custom_table_info.items()\n                if t in self._all_tables\n            }\n\n        self._max_string_length = max_string_length\n        self._metadata = metadata\n\n        # Initialize parent so llama-index internals are configured too.\n        # llama-index expects a synchronous SQLAlchemy Engine. If an AsyncEngine\n        # is provided, create a synchronous engine from the same URL.\n        parent_engine: Engine\n        if AsyncEngine is not None and isinstance(self._engine, AsyncEngine):\n            url_str = str(self._engine.url)\n            # Convert common async driver URLs to sync variants.\n            # This is conservative and primarily targets SQLite used in tests.\n            url_str = (\n                url_str.replace(\"+aiosqlite\", \"\")\n                .replace(\"+asyncpg\", \"\")\n                .replace(\"+psycopg_async\", \"+psycopg2\")\n            )\n            parent_engine = create_engine(url_str)\n        else:\n            parent_engine = self._engine  # type: ignore[assignment]\n\n        super().__init__(\n            engine=parent_engine,\n            schema=schema,\n            include_tables=sorted(self._usable_tables) if self._usable_tables else None,\n        )\n\n    # --- llama-index compatibility helpers (use OMOP metadata directly) ---\n    def get_table_columns(self, table_name: str) -&gt; List[str]:\n        \"\"\"Return list of column names for a table.\n\n        This uses the OMOP SQLAlchemy ``MetaData`` instead of DB inspector.\n        \"\"\"\n\n        return [col.name for col in self._metadata.tables[table_name].columns]\n\n    def get_single_table_info(self, table_name: str) -&gt; str:\n        \"\"\"Return a concise description of columns and foreign keys for a table.\n\n        The format matches what llama-index expects when building table context.\n        \"\"\"\n\n        template = \"Table '{table_name}' has columns: {columns}, and foreign keys: {foreign_keys}.\"\n        columns: List[str] = []\n        foreign_keys: List[str] = []\n        for column in self._metadata.tables[table_name].columns:\n            columns.append(f\"{column.name} ({column.type!s})\")\n            for fk in column.foreign_keys:\n                foreign_keys.append(\n                    f\"{column.name} -&gt; {fk.column.table.name}.{fk.column.name}\"\n                )\n        column_str = \", \".join(columns)\n        fk_str = \", \".join(foreign_keys)\n        return template.format(\n            table_name=table_name, columns=column_str, foreign_keys=fk_str\n        )\n\n    def usable_tables(self) -&gt; List[str]:\n        \"\"\"Return the sorted list of tables exposed to the LLM.\n\n        This respects include/ignore settings passed at initialization.\n        \"\"\"\n\n        return sorted(self._usable_tables)\n\n    # Backwards/compat helper name used in some code paths\n    def get_usable_table_names(self) -&gt; List[str]:  # pragma: no cover - thin wrapper\n        return self.usable_tables()\n</code></pre>"},{"location":"modules/#llm_engine.CDMDatabase.get_single_table_info","title":"<code>get_single_table_info(table_name)</code>","text":"<p>Return a concise description of columns and foreign keys for a table.</p> <p>The format matches what llama-index expects when building table context.</p> Source code in <code>src/pyomop/llm_engine.py</code> <pre><code>def get_single_table_info(self, table_name: str) -&gt; str:\n    \"\"\"Return a concise description of columns and foreign keys for a table.\n\n    The format matches what llama-index expects when building table context.\n    \"\"\"\n\n    template = \"Table '{table_name}' has columns: {columns}, and foreign keys: {foreign_keys}.\"\n    columns: List[str] = []\n    foreign_keys: List[str] = []\n    for column in self._metadata.tables[table_name].columns:\n        columns.append(f\"{column.name} ({column.type!s})\")\n        for fk in column.foreign_keys:\n            foreign_keys.append(\n                f\"{column.name} -&gt; {fk.column.table.name}.{fk.column.name}\"\n            )\n    column_str = \", \".join(columns)\n    fk_str = \", \".join(foreign_keys)\n    return template.format(\n        table_name=table_name, columns=column_str, foreign_keys=fk_str\n    )\n</code></pre>"},{"location":"modules/#llm_engine.CDMDatabase.get_table_columns","title":"<code>get_table_columns(table_name)</code>","text":"<p>Return list of column names for a table.</p> <p>This uses the OMOP SQLAlchemy <code>MetaData</code> instead of DB inspector.</p> Source code in <code>src/pyomop/llm_engine.py</code> <pre><code>def get_table_columns(self, table_name: str) -&gt; List[str]:\n    \"\"\"Return list of column names for a table.\n\n    This uses the OMOP SQLAlchemy ``MetaData`` instead of DB inspector.\n    \"\"\"\n\n    return [col.name for col in self._metadata.tables[table_name].columns]\n</code></pre>"},{"location":"modules/#llm_engine.CDMDatabase.usable_tables","title":"<code>usable_tables()</code>","text":"<p>Return the sorted list of tables exposed to the LLM.</p> <p>This respects include/ignore settings passed at initialization.</p> Source code in <code>src/pyomop/llm_engine.py</code> <pre><code>def usable_tables(self) -&gt; List[str]:\n    \"\"\"Return the sorted list of tables exposed to the LLM.\n\n    This respects include/ignore settings passed at initialization.\n    \"\"\"\n\n    return sorted(self._usable_tables)\n</code></pre>"},{"location":"modules/#llm_query.CdmLLMQuery","title":"<code>CdmLLMQuery</code>","text":"<p>Helper that prepares an LLM-backed SQL query engine for OMOP.</p> <p>It constructs an object index of selected CDM tables and exposes a retriever-backed query engine that can generate SQL or run SQL-only queries depending on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>sql_database</code> <code>CDMDatabase</code> <p>A <code>CDMDatabase</code> instance connected to the OMOP DB.</p> required <code>llm</code> <code>Any</code> <p>Optional LLM implementation to plug into llama-index settings.</p> <code>None</code> <code>similarity_top_k</code> <code>int</code> <p>Top-k tables to retrieve for each query.</p> <code>1</code> <code>embed_model</code> <code>str</code> <p>HuggingFace embedding model name.</p> <code>'BAAI/bge-small-en-v1.5'</code> <code>**kwargs</code> <code>Any</code> <p>Reserved for future expansion.</p> <code>{}</code> Source code in <code>src/pyomop/llm_query.py</code> <pre><code>class CdmLLMQuery:\n    \"\"\"Helper that prepares an LLM-backed SQL query engine for OMOP.\n\n    It constructs an object index of selected CDM tables and exposes a\n    retriever-backed query engine that can generate SQL or run SQL-only queries\n    depending on configuration.\n\n    Args:\n        sql_database: A ``CDMDatabase`` instance connected to the OMOP DB.\n        llm: Optional LLM implementation to plug into llama-index settings.\n        similarity_top_k: Top-k tables to retrieve for each query.\n        embed_model: HuggingFace embedding model name.\n        **kwargs: Reserved for future expansion.\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: CDMDatabase,\n        llm: Any = None,  # FIXME: type\n        similarity_top_k: int = 1,\n        embed_model: str = \"BAAI/bge-small-en-v1.5\",\n        **kwargs: Any,\n    ):\n        # Lazy import optional dependencies so the package imports without them\n        try:\n            sql_query_mod = importlib.import_module(\n                \"llama_index.core.indices.struct_store.sql_query\"\n            )\n            objects_mod = importlib.import_module(\"llama_index.core.objects\")\n            core_mod = importlib.import_module(\"llama_index.core\")\n            hf_mod = importlib.import_module(\"langchain_huggingface\")\n\n            SQLTableRetrieverQueryEngine = getattr(\n                sql_query_mod, \"SQLTableRetrieverQueryEngine\"\n            )\n            SQLTableNodeMapping = getattr(objects_mod, \"SQLTableNodeMapping\")\n            ObjectIndex = getattr(objects_mod, \"ObjectIndex\")\n            SQLTableSchema = getattr(objects_mod, \"SQLTableSchema\")\n            VectorStoreIndex = getattr(core_mod, \"VectorStoreIndex\")\n            Settings = getattr(core_mod, \"Settings\")\n            HuggingFaceEmbeddings = getattr(hf_mod, \"HuggingFaceEmbeddings\")\n        except Exception as e:  # pragma: no cover\n            raise ImportError(\"Install 'pyomop[llm]' to use LLM query features.\") from e\n        self._sql_database = sql_database\n        self._similarity_top_k = similarity_top_k\n        self._embed_model = HuggingFaceEmbeddings(model_name=embed_model)\n        self._llm = llm\n        Settings.llm = llm\n        Settings.embed_model = self._embed_model\n        self._table_node_mapping = SQLTableNodeMapping(sql_database)\n        usable_tables = []\n        if hasattr(sql_database, \"usable_tables\"):\n            usable_tables = list(sql_database.usable_tables())  # type: ignore[attr-defined]\n        elif hasattr(sql_database, \"get_usable_table_names\"):\n            usable_tables = list(sql_database.get_usable_table_names())  # type: ignore[attr-defined]\n        self._table_schema_objs = [\n            SQLTableSchema(table_name=t) for t in sorted(set(usable_tables))\n        ]\n\n        self._object_index = ObjectIndex.from_objects(\n            self._table_schema_objs,\n            self._table_node_mapping,\n            VectorStoreIndex,  # type: ignore\n        )\n\n        self._query_engine = SQLTableRetrieverQueryEngine(\n            self._sql_database,\n            self._object_index.as_retriever(similarity_top_k=1),\n            sql_only=True,\n        )\n\n    @property\n    def table_node_mapping(self) -&gt; Any:\n        \"\"\"Mapping between tables and nodes used by the object index.\"\"\"\n        return self._table_node_mapping\n\n    @property\n    def table_schema_objs(self) -&gt; list[Any]:\n        \"\"\"List of table schema objects indexed for retrieval.\"\"\"\n        return self._table_schema_objs\n\n    @property\n    def object_index(self) -&gt; Any:\n        \"\"\"The underlying llama-index object index used for retrieval.\"\"\"\n        return self._object_index\n\n    @property\n    def query_engine(self) -&gt; Any:\n        \"\"\"A retriever-backed SQL query engine over the CDM tables.\"\"\"\n        return self._query_engine\n</code></pre>"},{"location":"modules/#llm_query.CdmLLMQuery.object_index","title":"<code>object_index</code>  <code>property</code>","text":"<p>The underlying llama-index object index used for retrieval.</p>"},{"location":"modules/#llm_query.CdmLLMQuery.query_engine","title":"<code>query_engine</code>  <code>property</code>","text":"<p>A retriever-backed SQL query engine over the CDM tables.</p>"},{"location":"modules/#llm_query.CdmLLMQuery.table_node_mapping","title":"<code>table_node_mapping</code>  <code>property</code>","text":"<p>Mapping between tables and nodes used by the object index.</p>"},{"location":"modules/#llm_query.CdmLLMQuery.table_schema_objs","title":"<code>table_schema_objs</code>  <code>property</code>","text":"<p>List of table schema objects indexed for retrieval.</p>"},{"location":"modules/#loader.CdmCsvLoader","title":"<code>CdmCsvLoader</code>","text":"<p>Load a single CSV into multiple OMOP CDM tables using a JSON mapping file.</p> <p>Mapping file format (JSON):</p> <p>{   \"csv_key\": \"patient_id\",            # optional, CSV column that contains the patient/person identifier   \"tables\": [     {       \"name\": \"cohort\",              # target table name as in the database       \"filters\": [                     # optional row filters applied to CSV before mapping         {\"column\": \"resourceType\", \"equals\": \"Encounter\"}       ],       \"columns\": {                     # mapping of target_table_column -&gt; value         \"cohort_definition_id\": {\"const\": 1},              # constant value         \"subject_id\": \"patient_id\",                         # copy from CSV column         \"cohort_start_date\": \"period.start\",                # copy from CSV column         \"cohort_end_date\": \"period.end\"                     # copy from CSV column       }     }   ] }</p> Notes <ul> <li>Constants are provided via {\"const\": value}.</li> <li>If a required column is missing from mapping, it's left as None (DB default or nullable required).</li> <li>Primary keys that are Integer types will autoincrement where supported (SQLite/PostgreSQL typical behavior).</li> <li>Dates/times are converted to proper Python types where possible based on reflected column types.</li> </ul> Source code in <code>src/pyomop/loader.py</code> <pre><code>class CdmCsvLoader:\n    \"\"\"\n    Load a single CSV into multiple OMOP CDM tables using a JSON mapping file.\n\n    Mapping file format (JSON):\n\n    {\n      \"csv_key\": \"patient_id\",            # optional, CSV column that contains the patient/person identifier\n      \"tables\": [\n        {\n          \"name\": \"cohort\",              # target table name as in the database\n          \"filters\": [                     # optional row filters applied to CSV before mapping\n            {\"column\": \"resourceType\", \"equals\": \"Encounter\"}\n          ],\n          \"columns\": {                     # mapping of target_table_column -&gt; value\n            \"cohort_definition_id\": {\"const\": 1},              # constant value\n            \"subject_id\": \"patient_id\",                         # copy from CSV column\n            \"cohort_start_date\": \"period.start\",                # copy from CSV column\n            \"cohort_end_date\": \"period.end\"                     # copy from CSV column\n          }\n        }\n      ]\n    }\n\n    Notes:\n      - Constants are provided via {\"const\": value}.\n      - If a required column is missing from mapping, it's left as None (DB default or nullable required).\n      - Primary keys that are Integer types will autoincrement where supported (SQLite/PostgreSQL typical behavior).\n      - Dates/times are converted to proper Python types where possible based on reflected column types.\n    \"\"\"\n\n    def __init__(self, cdm_engine_factory, version: str = \"cdm54\") -&gt; None:\n        \"\"\"Create a loader bound to a specific database engine.\n\n        Args:\n            cdm_engine_factory: An initialized ``CdmEngineFactory``.\n            version: OMOP CDM version label (\"cdm54\" or \"cdm6\").\n        \"\"\"\n        self._cdm = cdm_engine_factory\n        self._engine = cdm_engine_factory.engine\n        self._maker = async_sessionmaker(self._engine, class_=AsyncSession)\n        self._scope = async_scoped_session(self._maker, scopefunc=asyncio.current_task)\n        self._version = version\n\n    @asynccontextmanager\n    async def _get_session(self) -&gt; AsyncGenerator[AsyncSession, None]:\n        \"\"\"Yield a scoped async session bound to the engine.\"\"\"\n        async with self._scope() as session:\n            yield session\n\n    async def _prepare_automap(self, conn: AsyncConnection) -&gt; AutomapBase:\n        \"\"\"Reflect the database and return an automapped base.\"\"\"\n        automap: AutomapBase = automap_base()\n\n        def _prepare(sync_conn):\n            automap.prepare(autoload_with=sync_conn)\n\n        await conn.run_sync(_prepare)\n        return automap\n\n    def _coerce_record_to_table_types(\n        self,\n        table,\n        rec: Dict[str, Any],\n        force_text_fields: Optional[Set[str]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Coerce a record's values to the SQL types defined by the target OMOP table.\n\n        Rules:\n        - Strings/Text: cast to str; lists/tuples joined by comma; dicts JSON-serialized; enforce max length if defined.\n        - Integers/Numerics: tolerant numeric parsing; None if unparsable.\n        - Dates/DateTimes: parsed via pandas; DateTime normalized to UTC-naive.\n        - Forced TEXT fields: certain columns are always stringified (e.g., codes arrays). The list comes\n          from mapping[\"force_text_fields\"].\n\n        This is applied just before insert to ensure DB type compatibility.\n        \"\"\"\n        # Columns that should always be treated as TEXT regardless of inferred type\n        force_text: Set[str] = set(force_text_fields or [])\n\n        # Local helper: stringify numbers without trailing .0 (e.g., 1.0 -&gt; \"1\")\n        def _s(v: Any) -&gt; str:\n            try:\n                if isinstance(v, float):\n                    return str(int(v)) if v.is_integer() else str(v)\n                if isinstance(v, Decimal):\n                    if v == v.to_integral_value():\n                        return str(int(v))\n                    # Normalize to drop insignificant trailing zeros\n                    return format(v.normalize(), \"f\")\n                return \"\" if v is None else str(v)\n            except Exception:\n                return str(v)\n\n        for col in table.columns:\n            name = col.name\n            if name not in rec:\n                continue\n            val = rec[name]\n            if val is None:\n                continue\n\n            t = col.type\n\n            # Force certain fields to TEXT\n            if name in force_text:\n                if isinstance(val, (list, tuple)):\n                    sval = \",\".join([_s(v) for v in val])\n                elif isinstance(val, dict):\n                    try:\n                        import json as _json\n\n                        sval = _json.dumps(val, ensure_ascii=False)\n                    except Exception:\n                        sval = _s(val)\n                else:\n                    sval = _s(val)\n                max_len = getattr(t, \"length\", None)\n                rec[name] = sval[: max_len or 255]\n                continue\n\n            # Type-driven coercion\n            try:\n                from pandas import to_datetime as _to_datetime\n                from pandas import to_numeric as _to_numeric\n\n                if isinstance(t, Date):\n                    dt = _to_datetime(val, errors=\"coerce\")\n                    rec[name] = None if pd.isna(dt) else dt.date()\n                elif isinstance(t, DateTime):\n                    ts = _to_datetime(val, errors=\"coerce\", utc=True)\n                    if pd.isna(ts):\n                        rec[name] = None\n                    else:\n                        py = ts.to_pydatetime()\n                        rec[name] = py.astimezone(timezone.utc).replace(tzinfo=None)\n                elif isinstance(t, (Integer, BigInteger)):\n                    num = _to_numeric(val, errors=\"coerce\")\n                    rec[name] = None if pd.isna(num) else int(num)\n                elif isinstance(t, Numeric):\n                    num = _to_numeric(val, errors=\"coerce\")\n                    rec[name] = None if pd.isna(num) else Decimal(str(num))\n                elif isinstance(t, (String, Text)):\n                    if isinstance(val, (list, tuple)):\n                        sval = \",\".join([_s(v) for v in val])\n                    elif isinstance(val, dict):\n                        try:\n                            import json as _json\n\n                            sval = _json.dumps(val, ensure_ascii=False)\n                        except Exception:\n                            sval = _s(val)\n                    else:\n                        sval = _s(val)\n                    max_len = getattr(t, \"length\", None)\n                    rec[name] = sval[: max_len or 255]\n                else:\n                    # Leave other types as-is\n                    pass\n            except Exception:\n                # Last resort, stringify\n                try:\n                    s = _s(val)\n                    max_len = getattr(getattr(col, \"type\", None), \"length\", None)\n                    rec[name] = s[: max_len or 255]\n                except Exception:\n                    rec[name] = val\n\n        return rec\n\n    async def _list_tables_with_person_id(self, conn: AsyncConnection) -&gt; List[str]:\n        \"\"\"Return table names (in default schema) that contain a person_id column, excluding 'person'.\"\"\"\n\n        def _inner(sync_conn):\n            from sqlalchemy import inspect as _inspect\n\n            insp = _inspect(sync_conn)\n            tables = insp.get_table_names()\n            result: List[str] = []\n            for t in tables:\n                try:\n                    cols = insp.get_columns(t)\n                except Exception:\n                    continue\n                if any(c.get(\"name\") == \"person_id\" for c in cols) and t != \"person\":\n                    result.append(t)\n            return result\n\n        return await conn.run_sync(_inner)\n\n    async def _add_person_id_text_columns(self, session: AsyncSession) -&gt; None:\n        \"\"\"Add a temporary person_id_text TEXT column to all non-person tables that have person_id.\n\n        This avoids fragile type-alter and FK juggling. We'll populate this column\n        when incoming person identifiers are not numeric, then resolve to integer\n        person_id in step 2 and drop the column.\n        \"\"\"\n        conn = await session.connection()\n        tables = await self._list_tables_with_person_id(conn)\n        dialect = self._engine.dialect.name\n        for t in tables:\n            if dialect == \"postgresql\":\n                ddl = f'ALTER TABLE \"{t}\" ADD COLUMN IF NOT EXISTS person_id_text TEXT'\n            else:\n                # SQLite (and others): try without IF NOT EXISTS\n                ddl = f'ALTER TABLE \"{t}\" ADD COLUMN person_id_text TEXT'\n            try:\n                await session.execute(text(ddl))\n            except Exception:\n                # Column may already exist or backend may not support IF NOT EXISTS; ignore\n                pass\n\n    async def _drop_person_id_text_columns(self, session: AsyncSession) -&gt; None:\n        \"\"\"Drop the temporary person_id_text column from all non-person tables.\"\"\"\n        conn = await session.connection()\n        tables = await self._list_tables_with_person_id(conn)\n        dialect = self._engine.dialect.name\n        for t in tables:\n            if dialect == \"postgresql\":\n                ddl = f'ALTER TABLE \"{t}\" DROP COLUMN IF EXISTS person_id_text'\n            else:\n                ddl = f'ALTER TABLE \"{t}\" DROP COLUMN person_id_text'\n            try:\n                await session.execute(text(ddl))\n            except Exception:\n                # Some backends or versions (older SQLite) may not support DROP COLUMN; ignore\n                pass\n\n    def _load_mapping(self, mapping_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Load a JSON mapping file from disk.\"\"\"\n        with open(mapping_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    def _apply_filters(self, df: pd.DataFrame, filters: Optional[List[Dict[str, Any]]]):\n        \"\"\"Apply optional row filters to a DataFrame prior to mapping.\"\"\"\n        if not filters:\n            return df\n        mask = pd.Series([True] * len(df), index=df.index)\n        for flt in filters:\n            col = flt.get(\"column\")\n            if col is None or col not in df.columns:\n                continue\n            if \"equals\" in flt:\n                mask &amp;= (df[col] == flt[\"equals\"]) | (\n                    df[col].astype(str) == str(flt[\"equals\"])\n                )\n            elif \"not_empty\" in flt and flt[\"not_empty\"]:\n                mask &amp;= df[col].notna() &amp; (df[col].astype(str).str.len() &gt; 0)\n        result = df.loc[mask, :].copy()\n        return result\n\n    def _convert_value(self, sa_type: Any, value: Any) -&gt; Any:\n        \"\"\"Coerce a CSV value into an appropriate Python type for insert.\n\n        The conversion is guided by the SQLAlchemy column type.\n        \"\"\"\n        if pd.isna(value) or value == \"\":\n            return None\n        try:\n            if isinstance(sa_type, Date):\n                # Accept many input formats\n                dt = pd.to_datetime(value, errors=\"coerce\")\n                return None if pd.isna(dt) else dt.date()\n            if isinstance(sa_type, DateTime):\n                # Normalize to UTC-naive for Postgres compatibility\n                ts = pd.to_datetime(value, errors=\"coerce\", utc=True)\n                if pd.isna(ts):\n                    return None\n                py = ts.to_pydatetime()\n                if getattr(py, \"tzinfo\", None) is not None:\n                    py = py.astimezone(timezone.utc).replace(tzinfo=None)\n                return py\n            if isinstance(sa_type, (Integer, BigInteger)):\n                return int(value)\n            if isinstance(sa_type, Numeric):\n                return Decimal(str(value))\n        except Exception:\n            return value\n        # Trim string values to 50 characters before insert\n        if isinstance(value, str):\n            return value[:50]\n        return value\n\n    async def load(\n        self, csv_path: str, mapping_path: str | None = None, chunk_size: int = 1000\n    ) -&gt; None:\n        \"\"\"Load a CSV into multiple OMOP tables based on a mapping file.\n\n        Args:\n            csv_path: Path to the input CSV file.\n            mapping_path: Path to the JSON mapping file. Defaults to the\n                package's ``mapping.default.json`` when not provided.\n            chunk_size: Batch size for INSERT statements.\n        \"\"\"\n        # If mapping path is None, load mapping.default.json from the current directory\n        logger.info(f\"Loading CSV data from {csv_path}\")\n        if mapping_path is None:\n            mapping_path = str(Path(__file__).parent / \"mapping.default.json\")\n        mapping = self._load_mapping(mapping_path)\n        # Use low_memory=False to avoid DtypeWarning for mixed-type columns\n        df = pd.read_csv(csv_path, low_memory=False)\n\n        async with self._get_session() as session:\n            # Relax constraint enforcement during bulk load on Postgres\n            is_pg = False\n            try:\n                is_pg = str(self._engine.dialect.name).startswith(\"postgres\")\n            except Exception:\n                is_pg = False\n            if is_pg:\n                try:\n                    await session.execute(\n                        text(\"SET session_replication_role = replica\")\n                    )\n                except Exception:\n                    pass\n\n            try:\n                conn = await session.connection()\n                # Before reflecting, add a temporary person_id_text column to accept non-numeric IDs\n                await self._add_person_id_text_columns(session)\n                automap = await self._prepare_automap(conn)\n\n                for tbl in mapping.get(\"tables\", []):\n                    table_name = tbl.get(\"name\")\n                    if not table_name:\n                        continue\n                    # obtain mapped class\n                    try:\n                        mapper = getattr(automap.classes, table_name)\n                    except AttributeError:\n                        raise ValueError(f\"Table '{table_name}' not found in database.\")\n\n                    # compute filtered dataframe\n                    df_tbl = self._apply_filters(df, tbl.get(\"filters\"))\n                    if df_tbl.empty:\n                        continue\n\n                    col_map: Dict[str, Any] = tbl.get(\"columns\", {})\n                    # Gather target SQLA column metadata\n                    sa_cols = {c.name: c.type for c in mapper.__table__.columns}\n                    sa_col_objs = {c.name: c for c in mapper.__table__.columns}\n\n                    # Build records\n                    records: List[Dict[str, Any]] = []\n                    for _, row in df_tbl.iterrows():\n                        rec: Dict[str, Any] = {}\n                        for target_col, src in col_map.items():\n                            if isinstance(src, dict) and \"const\" in src:\n                                value = src[\"const\"]\n                            elif isinstance(src, str):\n                                value = row.get(src)\n                            else:\n                                value = None\n\n                            # IMPORTANT: keep person_id raw for staging logic below\n                            if target_col != \"person_id\":\n                                # Convert based on SA type if available\n                                sa_t = sa_cols.get(target_col)\n                                if sa_t is not None:\n                                    value = self._convert_value(sa_t, value)\n                            rec[target_col] = value\n\n                        # If person_id exists in the record, route non-numeric values into person_id_text\n                        if \"person_id\" in rec:\n                            pid = rec.get(\"person_id\")\n                            if pid is None:\n                                # If NOT NULL, use placeholder to avoid constraint errors\n                                col = sa_col_objs.get(\"person_id\")\n                                if col is not None and not getattr(\n                                    col, \"nullable\", True\n                                ):\n                                    rec[\"person_id\"] = 0\n                            elif isinstance(pid, int):\n                                pass\n                            elif isinstance(pid, str) and pid.strip().isdigit():\n                                try:\n                                    rec[\"person_id\"] = int(pid.strip())\n                                except Exception:\n                                    # If conversion unexpectedly fails, send to text column\n                                    if \"person_id_text\" in sa_cols:\n                                        rec[\"person_id_text\"] = str(pid)\n                                    # Respect NOT NULL with placeholder when required\n                                    col = sa_col_objs.get(\"person_id\")\n                                    if col is not None and not getattr(\n                                        col, \"nullable\", True\n                                    ):\n                                        rec[\"person_id\"] = 0\n                                    else:\n                                        rec[\"person_id\"] = None\n                            else:\n                                # Non-numeric content: place into person_id_text if available\n                                if \"person_id_text\" in sa_cols:\n                                    rec[\"person_id_text\"] = str(pid)\n                                # Respect NOT NULL with placeholder when required\n                                col = sa_col_objs.get(\"person_id\")\n                                if col is not None and not getattr(\n                                    col, \"nullable\", True\n                                ):\n                                    rec[\"person_id\"] = 0\n                                else:\n                                    rec[\"person_id\"] = None\n                        # Finally coerce all fields to the table's schema (string lengths, forced TEXT, datetimes)\n                        rec = self._coerce_record_to_table_types(\n                            mapper.__table__,\n                            rec,\n                            set(mapping.get(\"force_text_fields\", [])),\n                        )\n                        records.append(rec)\n\n                    if not records:\n                        continue\n\n                    stmt = insert(mapper)\n                    # Chunked insert\n                    for i in range(0, len(records), chunk_size):\n                        batch = records[i : i + chunk_size]\n                        await session.execute(stmt, batch)\n\n                # Step 2: Normalize person_id FKs using person.person_id (not person_source_value)\n                logger.info(\"Normalizing person_id foreign keys\")\n                await self.fix_person_id(session, automap)\n\n                # Drop the temporary person_id_text columns now that person_id has been normalized\n                await self._drop_person_id_text_columns(session)\n\n                # Step 3: Backfill year/month/day of birth from birth_datetime where missing or zero\n                logger.info(\"Backfilling person birth fields\")\n                await self.backfill_person_birth_fields(session, automap)\n\n                # Step 4: Set gender_concept_id from gender_source_value using standard IDs\n                logger.info(\"Setting person.gender_concept_id from gender_source_value\")\n                await self.update_person_gender_concept_id(session, automap)\n\n                # Step 5: Apply concept mappings defined in the JSON mapping\n                logger.info(\"Applying concept mappings\")\n                await self.apply_concept_mappings(session, automap, mapping)\n\n                await session.commit()\n            finally:\n                if is_pg:\n                    try:\n                        await session.execute(\n                            text(\"SET session_replication_role = origin\")\n                        )\n                    except Exception:\n                        pass\n                await session.close()\n\n    async def fix_person_id(self, session: AsyncSession, automap: AutomapBase) -&gt; None:\n        \"\"\"\n        Update all tables so that person_id foreign keys store the canonical\n        person.person_id (integer), replacing any rows where person_id currently\n        contains the person_source_value (string/UUID).\n\n        Approach:\n        - Build a mapping from person_source_value -&gt; person_id from the person table.\n        - For each table (except person) having a person_id column, run updates:\n                    SET person_id = person.person_id WHERE CAST(person_id AS TEXT) = person_source_value.\n                - This is safe for SQLite (used in examples). For stricter RDBMS, ensure types\n                    are compatible or adjust as needed.\n        \"\"\"\n        # Resolve person table from automap\n        try:\n            person_cls = getattr(automap.classes, \"person\")\n        except AttributeError:\n            return  # No person table; nothing to do\n\n        person_table = person_cls.__table__\n\n        # Build mapping of person_source_value -&gt; person_id\n        res = await session.execute(\n            select(person_table.c.person_source_value, person_table.c.person_id).where(\n                person_table.c.person_source_value.isnot(None)\n            )\n        )\n        pairs = res.fetchall()\n        if not pairs:\n            return\n\n        psv_to_id: Dict[str, int] = {}\n        for psv, pid in pairs:\n            if psv is None or pid is None:\n                continue\n            psv_to_id[str(psv)] = int(pid)\n\n        if not psv_to_id:\n            return\n\n        # Iterate all tables and update person_id where it matches a known person_source_value\n        # Also handle rows that staged a non-numeric value in person_id_text.\n        # Avoid metadata.sorted_tables to prevent SAWarning about unresolvable cycles in vocab tables.\n        for tbl_name, table in automap.metadata.tables.items():\n            if tbl_name == person_table.name:\n                continue\n            if \"person_id\" not in table.c:\n                continue\n\n            # If person_id_text exists, prefer it for matching\n            has_text = \"person_id_text\" in table.c\n\n            # Run per-psv updates; small and explicit for clarity\n            for psv, pid in psv_to_id.items():\n                if has_text:\n                    # Match on person_id_text\n                    stmt = (\n                        update(table)\n                        .where(table.c.person_id_text == psv)\n                        .values(person_id=pid)\n                    )\n                    await session.execute(stmt)\n                # Also try matching where person_id was staged as string\n                stmt2 = (\n                    update(table)\n                    .where(cast(table.c.person_id, String()) == psv)\n                    .values(person_id=pid)\n                )\n                await session.execute(stmt2)\n\n            # Clear person_id_text after normalization when column exists\n            if has_text:\n                try:\n                    await session.execute(\n                        update(table)\n                        .where(table.c.person_id_text.isnot(None))\n                        .values(person_id_text=None)\n                    )\n                except Exception:\n                    pass\n\n    async def update_person_gender_concept_id(\n        self, session: AsyncSession, automap: AutomapBase\n    ) -&gt; None:\n        \"\"\"\n            Update person.gender_concept_id from person.gender_source_value using static mapping:\n            - male (or 'm')   -&gt; 8507\n            - female (or 'f') -&gt; 8532\n            - anything else   -&gt; 0 (unknown)\n\n        Only updates rows where the computed value differs from the current value\n        or where gender_concept_id is NULL.\n        \"\"\"\n        try:\n            person_cls = getattr(automap.classes, \"person\")\n        except AttributeError:\n            return\n\n        person_table = person_cls.__table__\n\n        # Fetch rows to evaluate. We consider all rows with a non-null gender_source_value\n        res = await session.execute(\n            select(\n                person_table.c.person_id,\n                person_table.c.gender_source_value,\n                person_table.c.gender_concept_id,\n            ).where(person_table.c.gender_source_value.isnot(None))\n        )\n\n        rows = res.fetchall()\n        if not rows:\n            return\n\n        def map_gender(val: str | None) -&gt; int:\n            if val is None:\n                return 0\n            s = str(val).strip().lower()\n            if s in {\"male\", \"m\"}:\n                return 8507\n            if s in {\"female\", \"f\"}:\n                return 8532\n            return 0\n\n        for pid, gsrc, gcid in rows:\n            target = map_gender(gsrc)\n            # Skip if already correct\n            if gcid == target:\n                continue\n            stmt = (\n                update(person_table)\n                .where(person_table.c.person_id == pid)\n                .values(gender_concept_id=target)\n            )\n            await session.execute(stmt)\n\n    async def backfill_person_birth_fields(\n        self, session: AsyncSession, automap: AutomapBase\n    ) -&gt; None:\n        \"\"\"\n            In the person table, replace 0 or NULL values in year_of_birth, month_of_birth,\n            and day_of_birth with values derived from birth_datetime.\n\n        This runs in Python for portability across backends.\n        \"\"\"\n        # Resolve person table from automap\n        try:\n            person_cls = getattr(automap.classes, \"person\")\n        except AttributeError:\n            return\n\n        person_table = person_cls.__table__\n\n        # Fetch necessary columns\n        res = await session.execute(\n            select(\n                person_table.c.person_id,\n                person_table.c.birth_datetime,\n                person_table.c.year_of_birth,\n                person_table.c.month_of_birth,\n                person_table.c.day_of_birth,\n            ).where(person_table.c.birth_datetime.isnot(None))\n        )\n\n        rows = res.fetchall()\n        if not rows:\n            return\n\n        for pid, birth_dt, y, m, d in rows:\n            # Parse birth_dt to a datetime if needed\n            bd: Optional[datetime]\n            if isinstance(birth_dt, datetime):\n                # Normalize timezone-aware to UTC-naive\n                if birth_dt.tzinfo is not None:\n                    bd = birth_dt.astimezone(timezone.utc).replace(tzinfo=None)\n                else:\n                    bd = birth_dt\n            elif isinstance(birth_dt, date):\n                bd = datetime(birth_dt.year, birth_dt.month, birth_dt.day)\n            else:\n                try:\n                    tmp = pd.to_datetime(birth_dt, errors=\"coerce\", utc=True)\n                    if pd.isna(tmp):\n                        bd = None\n                    else:\n                        py = tmp.to_pydatetime()\n                        bd = py.astimezone(timezone.utc).replace(tzinfo=None)\n                except Exception:\n                    bd = None\n\n            if bd is None:\n                continue\n\n            new_y = y if (y is not None and int(y or 0) != 0) else bd.year\n            new_m = m if (m is not None and int(m or 0) != 0) else bd.month\n            new_d = d if (d is not None and int(d or 0) != 0) else bd.day\n\n            # Only update when something changes\n            if new_y != y or new_m != m or new_d != d:\n                stmt = (\n                    update(person_table)\n                    .where(person_table.c.person_id == pid)\n                    .values(\n                        year_of_birth=new_y,\n                        month_of_birth=new_m,\n                        day_of_birth=new_d,\n                    )\n                )\n                await session.execute(stmt)\n\n    async def apply_concept_mappings(\n        self,\n        session: AsyncSession,\n        automap: AutomapBase,\n        mapping: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n            Based on the \"concept\" key in the mapping JSON, populate target *_concept_id columns\n            by looking up concept.concept_id using codes found in the specified source column.\n\n            Rules:\n        - If the source value is a comma-separated string, use only the first element for lookup.\n        - Find by equality on concept.concept_code.\n        - Update the target column with the matching concept.concept_id.\n        \"\"\"\n        if not mapping or \"concept\" not in mapping:\n            return\n\n        # Resolve concept table\n        try:\n            concept_cls = getattr(automap.classes, \"concept\")\n        except AttributeError:\n            return\n\n        concept_table = concept_cls.__table__\n\n        # Simple in-memory code to cid mapping\n        code_to_cid: Dict[str, Optional[int]] = {}\n\n        async def lookup_concept_id(code: str) -&gt; Optional[int]:\n            if code in code_to_cid:\n                return code_to_cid[code]\n            res = await session.execute(\n                select(concept_table.c.concept_id).where(\n                    concept_table.c.concept_code == code\n                )\n            )\n            row = res.first()\n            cid = int(row[0]) if row and row[0] is not None else None\n            code_to_cid[code] = cid\n            return cid\n\n        for item in mapping.get(\"concept\", []):\n            table_name = item.get(\"table\")\n            if not table_name:\n                continue\n            try:\n                mapper = getattr(automap.classes, table_name)\n            except AttributeError:\n                # Target table not found; skip\n                continue\n\n            table = mapper.__table__\n            pk_cols = list(table.primary_key.columns)\n            if not pk_cols:\n                # Cannot safely update without a primary key\n                continue\n\n            for m in item.get(\"mappings\", []):\n                source_col = m.get(\"source\")\n                target_col = m.get(\"target\")\n                if not source_col or not target_col:\n                    continue\n                if source_col not in table.c or target_col not in table.c:\n                    continue\n\n                # Fetch candidate rows: target is NULL or 0, and source is not NULL/empty\n                res = await session.execute(\n                    select(\n                        *pk_cols,\n                        table.c[source_col].label(\"_src\"),\n                        table.c[target_col].label(\"_tgt\"),\n                    ).where(\n                        or_(\n                            table.c[target_col].is_(None),\n                            table.c[target_col] == 0,\n                        ),\n                        table.c[source_col].isnot(None),\n                    )\n                )\n\n                rows = res.fetchall()\n                if not rows:\n                    continue\n\n                for row in rows:\n                    # row is a tuple: (*pk_vals, _src, _tgt)\n                    pk_vals = row[: len(pk_cols)]\n                    src_val = row[len(pk_cols)]\n\n                    # Only care about non-empty strings; if comma-separated, take first element\n                    code: Optional[str] = None\n                    if isinstance(src_val, str):\n                        # Split on comma and strip whitespace\n                        first = src_val.split(\",\")[0].strip()\n                        code = first if first else None\n                    elif isinstance(src_val, list) and src_val:\n                        # If a list somehow made it into the DB, use first element's string\n                        code = str(src_val[0])\n                    else:\n                        # Fallback to simple string conversion if it's a scalar\n                        code = str(src_val) if src_val is not None else None\n\n                    if not code:\n                        continue\n\n                    cid = await lookup_concept_id(code)\n                    if cid is None:\n                        continue\n\n                    # Build WHERE with PK columns\n                    where_clause = and_(\n                        *[\n                            (pk_col == pk_val)\n                            for pk_col, pk_val in zip(pk_cols, pk_vals)\n                        ]\n                    )\n\n                    stmt = update(table).where(where_clause).values({target_col: cid})\n                    await session.execute(stmt)\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.__init__","title":"<code>__init__(cdm_engine_factory, version='cdm54')</code>","text":"<p>Create a loader bound to a specific database engine.</p> <p>Parameters:</p> Name Type Description Default <code>cdm_engine_factory</code> <p>An initialized <code>CdmEngineFactory</code>.</p> required <code>version</code> <code>str</code> <p>OMOP CDM version label (\"cdm54\" or \"cdm6\").</p> <code>'cdm54'</code> Source code in <code>src/pyomop/loader.py</code> <pre><code>def __init__(self, cdm_engine_factory, version: str = \"cdm54\") -&gt; None:\n    \"\"\"Create a loader bound to a specific database engine.\n\n    Args:\n        cdm_engine_factory: An initialized ``CdmEngineFactory``.\n        version: OMOP CDM version label (\"cdm54\" or \"cdm6\").\n    \"\"\"\n    self._cdm = cdm_engine_factory\n    self._engine = cdm_engine_factory.engine\n    self._maker = async_sessionmaker(self._engine, class_=AsyncSession)\n    self._scope = async_scoped_session(self._maker, scopefunc=asyncio.current_task)\n    self._version = version\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.apply_concept_mappings","title":"<code>apply_concept_mappings(session, automap, mapping)</code>  <code>async</code>","text":"<pre><code>Based on the \"concept\" key in the mapping JSON, populate target *_concept_id columns\nby looking up concept.concept_id using codes found in the specified source column.\n\nRules:\n</code></pre> <ul> <li>If the source value is a comma-separated string, use only the first element for lookup.</li> <li>Find by equality on concept.concept_code.</li> <li>Update the target column with the matching concept.concept_id.</li> </ul> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def apply_concept_mappings(\n    self,\n    session: AsyncSession,\n    automap: AutomapBase,\n    mapping: Dict[str, Any],\n) -&gt; None:\n    \"\"\"\n        Based on the \"concept\" key in the mapping JSON, populate target *_concept_id columns\n        by looking up concept.concept_id using codes found in the specified source column.\n\n        Rules:\n    - If the source value is a comma-separated string, use only the first element for lookup.\n    - Find by equality on concept.concept_code.\n    - Update the target column with the matching concept.concept_id.\n    \"\"\"\n    if not mapping or \"concept\" not in mapping:\n        return\n\n    # Resolve concept table\n    try:\n        concept_cls = getattr(automap.classes, \"concept\")\n    except AttributeError:\n        return\n\n    concept_table = concept_cls.__table__\n\n    # Simple in-memory code to cid mapping\n    code_to_cid: Dict[str, Optional[int]] = {}\n\n    async def lookup_concept_id(code: str) -&gt; Optional[int]:\n        if code in code_to_cid:\n            return code_to_cid[code]\n        res = await session.execute(\n            select(concept_table.c.concept_id).where(\n                concept_table.c.concept_code == code\n            )\n        )\n        row = res.first()\n        cid = int(row[0]) if row and row[0] is not None else None\n        code_to_cid[code] = cid\n        return cid\n\n    for item in mapping.get(\"concept\", []):\n        table_name = item.get(\"table\")\n        if not table_name:\n            continue\n        try:\n            mapper = getattr(automap.classes, table_name)\n        except AttributeError:\n            # Target table not found; skip\n            continue\n\n        table = mapper.__table__\n        pk_cols = list(table.primary_key.columns)\n        if not pk_cols:\n            # Cannot safely update without a primary key\n            continue\n\n        for m in item.get(\"mappings\", []):\n            source_col = m.get(\"source\")\n            target_col = m.get(\"target\")\n            if not source_col or not target_col:\n                continue\n            if source_col not in table.c or target_col not in table.c:\n                continue\n\n            # Fetch candidate rows: target is NULL or 0, and source is not NULL/empty\n            res = await session.execute(\n                select(\n                    *pk_cols,\n                    table.c[source_col].label(\"_src\"),\n                    table.c[target_col].label(\"_tgt\"),\n                ).where(\n                    or_(\n                        table.c[target_col].is_(None),\n                        table.c[target_col] == 0,\n                    ),\n                    table.c[source_col].isnot(None),\n                )\n            )\n\n            rows = res.fetchall()\n            if not rows:\n                continue\n\n            for row in rows:\n                # row is a tuple: (*pk_vals, _src, _tgt)\n                pk_vals = row[: len(pk_cols)]\n                src_val = row[len(pk_cols)]\n\n                # Only care about non-empty strings; if comma-separated, take first element\n                code: Optional[str] = None\n                if isinstance(src_val, str):\n                    # Split on comma and strip whitespace\n                    first = src_val.split(\",\")[0].strip()\n                    code = first if first else None\n                elif isinstance(src_val, list) and src_val:\n                    # If a list somehow made it into the DB, use first element's string\n                    code = str(src_val[0])\n                else:\n                    # Fallback to simple string conversion if it's a scalar\n                    code = str(src_val) if src_val is not None else None\n\n                if not code:\n                    continue\n\n                cid = await lookup_concept_id(code)\n                if cid is None:\n                    continue\n\n                # Build WHERE with PK columns\n                where_clause = and_(\n                    *[\n                        (pk_col == pk_val)\n                        for pk_col, pk_val in zip(pk_cols, pk_vals)\n                    ]\n                )\n\n                stmt = update(table).where(where_clause).values({target_col: cid})\n                await session.execute(stmt)\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.backfill_person_birth_fields","title":"<code>backfill_person_birth_fields(session, automap)</code>  <code>async</code>","text":"<pre><code>In the person table, replace 0 or NULL values in year_of_birth, month_of_birth,\nand day_of_birth with values derived from birth_datetime.\n</code></pre> <p>This runs in Python for portability across backends.</p> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def backfill_person_birth_fields(\n    self, session: AsyncSession, automap: AutomapBase\n) -&gt; None:\n    \"\"\"\n        In the person table, replace 0 or NULL values in year_of_birth, month_of_birth,\n        and day_of_birth with values derived from birth_datetime.\n\n    This runs in Python for portability across backends.\n    \"\"\"\n    # Resolve person table from automap\n    try:\n        person_cls = getattr(automap.classes, \"person\")\n    except AttributeError:\n        return\n\n    person_table = person_cls.__table__\n\n    # Fetch necessary columns\n    res = await session.execute(\n        select(\n            person_table.c.person_id,\n            person_table.c.birth_datetime,\n            person_table.c.year_of_birth,\n            person_table.c.month_of_birth,\n            person_table.c.day_of_birth,\n        ).where(person_table.c.birth_datetime.isnot(None))\n    )\n\n    rows = res.fetchall()\n    if not rows:\n        return\n\n    for pid, birth_dt, y, m, d in rows:\n        # Parse birth_dt to a datetime if needed\n        bd: Optional[datetime]\n        if isinstance(birth_dt, datetime):\n            # Normalize timezone-aware to UTC-naive\n            if birth_dt.tzinfo is not None:\n                bd = birth_dt.astimezone(timezone.utc).replace(tzinfo=None)\n            else:\n                bd = birth_dt\n        elif isinstance(birth_dt, date):\n            bd = datetime(birth_dt.year, birth_dt.month, birth_dt.day)\n        else:\n            try:\n                tmp = pd.to_datetime(birth_dt, errors=\"coerce\", utc=True)\n                if pd.isna(tmp):\n                    bd = None\n                else:\n                    py = tmp.to_pydatetime()\n                    bd = py.astimezone(timezone.utc).replace(tzinfo=None)\n            except Exception:\n                bd = None\n\n        if bd is None:\n            continue\n\n        new_y = y if (y is not None and int(y or 0) != 0) else bd.year\n        new_m = m if (m is not None and int(m or 0) != 0) else bd.month\n        new_d = d if (d is not None and int(d or 0) != 0) else bd.day\n\n        # Only update when something changes\n        if new_y != y or new_m != m or new_d != d:\n            stmt = (\n                update(person_table)\n                .where(person_table.c.person_id == pid)\n                .values(\n                    year_of_birth=new_y,\n                    month_of_birth=new_m,\n                    day_of_birth=new_d,\n                )\n            )\n            await session.execute(stmt)\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.fix_person_id","title":"<code>fix_person_id(session, automap)</code>  <code>async</code>","text":"<p>Update all tables so that person_id foreign keys store the canonical person.person_id (integer), replacing any rows where person_id currently contains the person_source_value (string/UUID).</p> <p>Approach: - Build a mapping from person_source_value -&gt; person_id from the person table. - For each table (except person) having a person_id column, run updates:             SET person_id = person.person_id WHERE CAST(person_id AS TEXT) = person_source_value.         - This is safe for SQLite (used in examples). For stricter RDBMS, ensure types             are compatible or adjust as needed.</p> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def fix_person_id(self, session: AsyncSession, automap: AutomapBase) -&gt; None:\n    \"\"\"\n    Update all tables so that person_id foreign keys store the canonical\n    person.person_id (integer), replacing any rows where person_id currently\n    contains the person_source_value (string/UUID).\n\n    Approach:\n    - Build a mapping from person_source_value -&gt; person_id from the person table.\n    - For each table (except person) having a person_id column, run updates:\n                SET person_id = person.person_id WHERE CAST(person_id AS TEXT) = person_source_value.\n            - This is safe for SQLite (used in examples). For stricter RDBMS, ensure types\n                are compatible or adjust as needed.\n    \"\"\"\n    # Resolve person table from automap\n    try:\n        person_cls = getattr(automap.classes, \"person\")\n    except AttributeError:\n        return  # No person table; nothing to do\n\n    person_table = person_cls.__table__\n\n    # Build mapping of person_source_value -&gt; person_id\n    res = await session.execute(\n        select(person_table.c.person_source_value, person_table.c.person_id).where(\n            person_table.c.person_source_value.isnot(None)\n        )\n    )\n    pairs = res.fetchall()\n    if not pairs:\n        return\n\n    psv_to_id: Dict[str, int] = {}\n    for psv, pid in pairs:\n        if psv is None or pid is None:\n            continue\n        psv_to_id[str(psv)] = int(pid)\n\n    if not psv_to_id:\n        return\n\n    # Iterate all tables and update person_id where it matches a known person_source_value\n    # Also handle rows that staged a non-numeric value in person_id_text.\n    # Avoid metadata.sorted_tables to prevent SAWarning about unresolvable cycles in vocab tables.\n    for tbl_name, table in automap.metadata.tables.items():\n        if tbl_name == person_table.name:\n            continue\n        if \"person_id\" not in table.c:\n            continue\n\n        # If person_id_text exists, prefer it for matching\n        has_text = \"person_id_text\" in table.c\n\n        # Run per-psv updates; small and explicit for clarity\n        for psv, pid in psv_to_id.items():\n            if has_text:\n                # Match on person_id_text\n                stmt = (\n                    update(table)\n                    .where(table.c.person_id_text == psv)\n                    .values(person_id=pid)\n                )\n                await session.execute(stmt)\n            # Also try matching where person_id was staged as string\n            stmt2 = (\n                update(table)\n                .where(cast(table.c.person_id, String()) == psv)\n                .values(person_id=pid)\n            )\n            await session.execute(stmt2)\n\n        # Clear person_id_text after normalization when column exists\n        if has_text:\n            try:\n                await session.execute(\n                    update(table)\n                    .where(table.c.person_id_text.isnot(None))\n                    .values(person_id_text=None)\n                )\n            except Exception:\n                pass\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.load","title":"<code>load(csv_path, mapping_path=None, chunk_size=1000)</code>  <code>async</code>","text":"<p>Load a CSV into multiple OMOP tables based on a mapping file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>str</code> <p>Path to the input CSV file.</p> required <code>mapping_path</code> <code>str | None</code> <p>Path to the JSON mapping file. Defaults to the package's <code>mapping.default.json</code> when not provided.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>Batch size for INSERT statements.</p> <code>1000</code> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def load(\n    self, csv_path: str, mapping_path: str | None = None, chunk_size: int = 1000\n) -&gt; None:\n    \"\"\"Load a CSV into multiple OMOP tables based on a mapping file.\n\n    Args:\n        csv_path: Path to the input CSV file.\n        mapping_path: Path to the JSON mapping file. Defaults to the\n            package's ``mapping.default.json`` when not provided.\n        chunk_size: Batch size for INSERT statements.\n    \"\"\"\n    # If mapping path is None, load mapping.default.json from the current directory\n    logger.info(f\"Loading CSV data from {csv_path}\")\n    if mapping_path is None:\n        mapping_path = str(Path(__file__).parent / \"mapping.default.json\")\n    mapping = self._load_mapping(mapping_path)\n    # Use low_memory=False to avoid DtypeWarning for mixed-type columns\n    df = pd.read_csv(csv_path, low_memory=False)\n\n    async with self._get_session() as session:\n        # Relax constraint enforcement during bulk load on Postgres\n        is_pg = False\n        try:\n            is_pg = str(self._engine.dialect.name).startswith(\"postgres\")\n        except Exception:\n            is_pg = False\n        if is_pg:\n            try:\n                await session.execute(\n                    text(\"SET session_replication_role = replica\")\n                )\n            except Exception:\n                pass\n\n        try:\n            conn = await session.connection()\n            # Before reflecting, add a temporary person_id_text column to accept non-numeric IDs\n            await self._add_person_id_text_columns(session)\n            automap = await self._prepare_automap(conn)\n\n            for tbl in mapping.get(\"tables\", []):\n                table_name = tbl.get(\"name\")\n                if not table_name:\n                    continue\n                # obtain mapped class\n                try:\n                    mapper = getattr(automap.classes, table_name)\n                except AttributeError:\n                    raise ValueError(f\"Table '{table_name}' not found in database.\")\n\n                # compute filtered dataframe\n                df_tbl = self._apply_filters(df, tbl.get(\"filters\"))\n                if df_tbl.empty:\n                    continue\n\n                col_map: Dict[str, Any] = tbl.get(\"columns\", {})\n                # Gather target SQLA column metadata\n                sa_cols = {c.name: c.type for c in mapper.__table__.columns}\n                sa_col_objs = {c.name: c for c in mapper.__table__.columns}\n\n                # Build records\n                records: List[Dict[str, Any]] = []\n                for _, row in df_tbl.iterrows():\n                    rec: Dict[str, Any] = {}\n                    for target_col, src in col_map.items():\n                        if isinstance(src, dict) and \"const\" in src:\n                            value = src[\"const\"]\n                        elif isinstance(src, str):\n                            value = row.get(src)\n                        else:\n                            value = None\n\n                        # IMPORTANT: keep person_id raw for staging logic below\n                        if target_col != \"person_id\":\n                            # Convert based on SA type if available\n                            sa_t = sa_cols.get(target_col)\n                            if sa_t is not None:\n                                value = self._convert_value(sa_t, value)\n                        rec[target_col] = value\n\n                    # If person_id exists in the record, route non-numeric values into person_id_text\n                    if \"person_id\" in rec:\n                        pid = rec.get(\"person_id\")\n                        if pid is None:\n                            # If NOT NULL, use placeholder to avoid constraint errors\n                            col = sa_col_objs.get(\"person_id\")\n                            if col is not None and not getattr(\n                                col, \"nullable\", True\n                            ):\n                                rec[\"person_id\"] = 0\n                        elif isinstance(pid, int):\n                            pass\n                        elif isinstance(pid, str) and pid.strip().isdigit():\n                            try:\n                                rec[\"person_id\"] = int(pid.strip())\n                            except Exception:\n                                # If conversion unexpectedly fails, send to text column\n                                if \"person_id_text\" in sa_cols:\n                                    rec[\"person_id_text\"] = str(pid)\n                                # Respect NOT NULL with placeholder when required\n                                col = sa_col_objs.get(\"person_id\")\n                                if col is not None and not getattr(\n                                    col, \"nullable\", True\n                                ):\n                                    rec[\"person_id\"] = 0\n                                else:\n                                    rec[\"person_id\"] = None\n                        else:\n                            # Non-numeric content: place into person_id_text if available\n                            if \"person_id_text\" in sa_cols:\n                                rec[\"person_id_text\"] = str(pid)\n                            # Respect NOT NULL with placeholder when required\n                            col = sa_col_objs.get(\"person_id\")\n                            if col is not None and not getattr(\n                                col, \"nullable\", True\n                            ):\n                                rec[\"person_id\"] = 0\n                            else:\n                                rec[\"person_id\"] = None\n                    # Finally coerce all fields to the table's schema (string lengths, forced TEXT, datetimes)\n                    rec = self._coerce_record_to_table_types(\n                        mapper.__table__,\n                        rec,\n                        set(mapping.get(\"force_text_fields\", [])),\n                    )\n                    records.append(rec)\n\n                if not records:\n                    continue\n\n                stmt = insert(mapper)\n                # Chunked insert\n                for i in range(0, len(records), chunk_size):\n                    batch = records[i : i + chunk_size]\n                    await session.execute(stmt, batch)\n\n            # Step 2: Normalize person_id FKs using person.person_id (not person_source_value)\n            logger.info(\"Normalizing person_id foreign keys\")\n            await self.fix_person_id(session, automap)\n\n            # Drop the temporary person_id_text columns now that person_id has been normalized\n            await self._drop_person_id_text_columns(session)\n\n            # Step 3: Backfill year/month/day of birth from birth_datetime where missing or zero\n            logger.info(\"Backfilling person birth fields\")\n            await self.backfill_person_birth_fields(session, automap)\n\n            # Step 4: Set gender_concept_id from gender_source_value using standard IDs\n            logger.info(\"Setting person.gender_concept_id from gender_source_value\")\n            await self.update_person_gender_concept_id(session, automap)\n\n            # Step 5: Apply concept mappings defined in the JSON mapping\n            logger.info(\"Applying concept mappings\")\n            await self.apply_concept_mappings(session, automap, mapping)\n\n            await session.commit()\n        finally:\n            if is_pg:\n                try:\n                    await session.execute(\n                        text(\"SET session_replication_role = origin\")\n                    )\n                except Exception:\n                    pass\n            await session.close()\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.update_person_gender_concept_id","title":"<code>update_person_gender_concept_id(session, automap)</code>  <code>async</code>","text":"<pre><code>Update person.gender_concept_id from person.gender_source_value using static mapping:\n- male (or 'm')   -&gt; 8507\n- female (or 'f') -&gt; 8532\n- anything else   -&gt; 0 (unknown)\n</code></pre> <p>Only updates rows where the computed value differs from the current value or where gender_concept_id is NULL.</p> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def update_person_gender_concept_id(\n    self, session: AsyncSession, automap: AutomapBase\n) -&gt; None:\n    \"\"\"\n        Update person.gender_concept_id from person.gender_source_value using static mapping:\n        - male (or 'm')   -&gt; 8507\n        - female (or 'f') -&gt; 8532\n        - anything else   -&gt; 0 (unknown)\n\n    Only updates rows where the computed value differs from the current value\n    or where gender_concept_id is NULL.\n    \"\"\"\n    try:\n        person_cls = getattr(automap.classes, \"person\")\n    except AttributeError:\n        return\n\n    person_table = person_cls.__table__\n\n    # Fetch rows to evaluate. We consider all rows with a non-null gender_source_value\n    res = await session.execute(\n        select(\n            person_table.c.person_id,\n            person_table.c.gender_source_value,\n            person_table.c.gender_concept_id,\n        ).where(person_table.c.gender_source_value.isnot(None))\n    )\n\n    rows = res.fetchall()\n    if not rows:\n        return\n\n    def map_gender(val: str | None) -&gt; int:\n        if val is None:\n            return 0\n        s = str(val).strip().lower()\n        if s in {\"male\", \"m\"}:\n            return 8507\n        if s in {\"female\", \"f\"}:\n            return 8532\n        return 0\n\n    for pid, gsrc, gcid in rows:\n        target = map_gender(gsrc)\n        # Skip if already correct\n        if gcid == target:\n            continue\n        stmt = (\n            update(person_table)\n            .where(person_table.c.person_id == pid)\n            .values(gender_concept_id=target)\n        )\n        await session.execute(stmt)\n</code></pre>"},{"location":"modules/#main.main_routine","title":"<code>main_routine()</code>","text":"<p>Top-level runner used by <code>python -m pyomop</code>.</p> Source code in <code>src/pyomop/main.py</code> <pre><code>def main_routine():\n    \"\"\"Top-level runner used by ``python -m pyomop``.\"\"\"\n    click.echo(\"_________________________________________\")\n    click.echo(\"Pyomop v\" + __version__ + \" by Bell Eapen ( https://nuchange.ca ) \")\n    cli()  # run the main function\n    click.echo(\"Pyomop done.\")\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary","title":"<code>CdmVocabulary</code>","text":"<p>               Bases: <code>object</code></p> <p>Helpers for OMOP Vocabulary management and lookups.</p> <p>Parameters:</p> Name Type Description Default <code>cdm</code> <p>An initialized <code>CdmEngineFactory</code> instance.</p> required <code>version</code> <p>CDM version string (\"cdm54\" or \"cdm6\"). Defaults to \"cdm54\".</p> <code>'cdm54'</code> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>class CdmVocabulary(object):\n    \"\"\"Helpers for OMOP Vocabulary management and lookups.\n\n    Args:\n        cdm: An initialized ``CdmEngineFactory`` instance.\n        version: CDM version string (\"cdm54\" or \"cdm6\"). Defaults to \"cdm54\".\n    \"\"\"\n\n    def __init__(self, cdm, version=\"cdm54\"):\n        self._concept_id = 0\n        self._concept_name = \"\"\n        self._domain_id = \"\"\n        self._vocabulary_id = \"\"\n        self._concept_class_id = \"\"\n        self._concept_code = \"\"\n        self._cdm = cdm\n        self._engine = cdm.engine\n        self._maker = async_sessionmaker(self._engine, class_=AsyncSession)\n        self._scope = async_scoped_session(self._maker, scopefunc=asyncio.current_task)\n        self._version = version\n\n    @property\n    def concept_id(self):\n        \"\"\"Current concept_id for this helper (if set).\"\"\"\n        return self._concept_id\n\n    @property\n    def concept_code(self):\n        \"\"\"Current concept_code for this helper (if set).\"\"\"\n        return self._concept_code\n\n    @property\n    def concept_name(self):\n        \"\"\"Current concept_name for this helper (if set).\"\"\"\n        return self._concept_name\n\n    @property\n    def vocabulary_id(self):\n        \"\"\"Current vocabulary_id for this helper (if set).\"\"\"\n        return self._vocabulary_id\n\n    @property\n    def domain_id(self):\n        \"\"\"Current domain_id for this helper (if set).\"\"\"\n        return self._domain_id\n\n    @concept_id.setter\n    def concept_id(self, concept_id):\n        \"\"\"Set the active concept context by concept_id.\n\n        Side effects: populates concept name, domain, vocabulary, class, and code\n        on this helper instance for convenience.\n\n        Args:\n            concept_id: The concept_id to fetch and set.\n        \"\"\"\n        self._concept_id = concept_id\n        _concept = asyncio.run(self.get_concept(concept_id))\n        self._concept_name = _concept.concept_name\n        self._domain_id = _concept.domain_id\n        self._vocabulary_id = _concept.vocabulary_id\n        self._concept_class_id = _concept.concept_class_id\n        self._concept_code = _concept.concept_code\n\n    async def get_concept(self, concept_id):\n        \"\"\"Fetch a concept row by id.\n\n        Args:\n            concept_id: Concept identifier.\n\n        Returns:\n            The ORM Concept instance.\n        \"\"\"\n        if self._version == \"cdm6\":\n            from .cdm6 import Concept\n        else:\n            from .cdm54 import Concept\n        stmt = select(Concept).where(Concept.concept_id == concept_id)\n        async with self._cdm.session() as session:\n            _concept = await session.execute(stmt)\n        return _concept.scalar_one()\n\n    async def get_concept_by_code(self, concept_code, vocabulary_id):\n        \"\"\"Fetch a concept by code within a vocabulary.\n\n        Args:\n            concept_code: The vocabulary-specific code string.\n            vocabulary_id: Vocabulary identifier (e.g., 'SNOMED', 'LOINC').\n\n        Returns:\n            The ORM Concept instance.\n        \"\"\"\n        if self._version == \"cdm6\":\n            from .cdm6 import Concept\n        else:\n            from .cdm54 import Concept\n        stmt = (\n            select(Concept)\n            .where(Concept.concept_code == concept_code)\n            .where(Concept.vocabulary_id == vocabulary_id)\n        )\n        async with self._cdm.session() as session:\n            _concept = await session.execute(stmt)\n        return _concept.scalar_one()\n\n    def set_concept(self, concept_code, vocabulary_id=None):\n        \"\"\"Set the active concept context by code and vocabulary.\n\n        Args:\n            concept_code: The concept code string to resolve.\n            vocabulary_id: Vocabulary identifier. Required.\n\n        Notes:\n            On success, populates concept fields on this instance. On failure,\n            sets ``_vocabulary_id`` and ``_concept_id`` to 0.\n        \"\"\"\n        self._concept_code = concept_code\n        try:\n            if vocabulary_id is not None:\n                self._vocabulary_id = vocabulary_id\n                _concept = asyncio.run(\n                    self.get_concept_by_code(concept_code, vocabulary_id)\n                )\n            else:\n                raise ValueError(\n                    \"vocabulary_id must be provided when setting concept by code.\"\n                )\n\n            self._concept_name = _concept.concept_name\n            self._domain_id = _concept.domain_id\n            self._concept_id = _concept.concept_id\n            self._concept_class_id = _concept.concept_class_id\n            self._concept_code = _concept.concept_code\n\n        except:\n            self._vocabulary_id = 0\n            self._concept_id = 0\n\n    async def create_vocab(self, folder, sample=None):\n        \"\"\"Load vocabulary CSV files from a folder into the database.\n\n        This imports the standard OMOP vocab tables (drug_strength, concept,\n        concept_relationship, concept_ancestor, concept_synonym, vocabulary,\n        relationship, concept_class, domain).\n\n        Args:\n            folder: Path to the folder containing OMOP vocabulary CSVs.\n            sample: Optional number of rows to limit per file during import.\n        \"\"\"\n        try:\n            # Parents first (for concept FKs): DOMAIN, CONCEPT_CLASS\n            df = pd.read_csv(\n                folder + \"/DOMAIN.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"domain\", \"replace\")\n\n            df = pd.read_csv(\n                folder + \"/CONCEPT_CLASS.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"concept_class\", \"replace\")\n\n            # Then CONCEPT (uses domain_id and concept_class_id)\n            df = pd.read_csv(\n                folder + \"/CONCEPT.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            df[\"valid_start_date\"] = pd.to_datetime(\n                df[\"valid_start_date\"], errors=\"coerce\"\n            )\n            df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n            await self.write_vocab(df, \"concept\", \"replace\")\n\n            # Then VOCABULARY (uses vocabulary_concept_id -&gt; concept)\n            df = pd.read_csv(\n                folder + \"/VOCABULARY.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"vocabulary\", \"replace\")\n\n            # Relationship depends on concept for relationship_concept_id\n            df = pd.read_csv(\n                folder + \"/RELATIONSHIP.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"relationship\", \"replace\")\n\n            # Post-concept tables\n            df = pd.read_csv(\n                folder + \"/CONCEPT_RELATIONSHIP.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            df[\"valid_start_date\"] = pd.to_datetime(\n                df[\"valid_start_date\"], errors=\"coerce\"\n            )\n            df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n            await self.write_vocab(df, \"concept_relationship\", \"replace\")\n\n            df = pd.read_csv(\n                folder + \"/CONCEPT_SYNONYM.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"concept_synonym\", \"replace\")\n\n            df = pd.read_csv(\n                folder + \"/DRUG_STRENGTH.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            df[\"valid_start_date\"] = pd.to_datetime(\n                df[\"valid_start_date\"], errors=\"coerce\"\n            )\n            df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n            await self.write_vocab(df, \"drug_strength\", \"replace\")\n\n            df = pd.read_csv(\n                folder + \"/CONCEPT_ANCESTOR.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"concept_ancestor\", \"replace\")\n        except Exception as e:\n            logger.error(f\"An error occurred while creating the vocabulary: {e}\")\n\n    @asynccontextmanager\n    async def get_session(self) -&gt; AsyncGenerator[AsyncSession, None]:\n        \"\"\"Yield an async session bound to the current engine.\n\n        Yields:\n            AsyncSession: An async SQLAlchemy session.\n        \"\"\"\n        async with self._scope() as session:\n            yield session\n\n    async def write_vocab(self, df, table, if_exists=\"replace\", chunk_size=1000):\n        \"\"\"Write a DataFrame to a vocabulary table with type-safe defaults.\n\n        Ensures required columns exist with reasonable defaults, coerces types,\n        and performs chunked inserts via SQLAlchemy core for performance.\n\n        Args:\n            df: Pandas DataFrame with data to insert.\n            table: Target table name (e.g., 'concept').\n            if_exists: Compatibility only. This method always inserts.\n            chunk_size: Number of rows per batch insert.\n        \"\"\"\n        async with self.get_session() as session:\n            # For PostgreSQL, temporarily relax constraint enforcement during bulk loads\n            is_pg = False\n            try:\n                is_pg = self._engine.dialect.name.startswith(\"postgres\")\n            except Exception:\n                is_pg = False\n            if is_pg:\n                logger.info(\n                    \"Temporarily disabling replication role for bulk load on postgres\"\n                )\n                try:\n                    await session.execute(\n                        text(\"SET session_replication_role = replica\")\n                    )\n                except Exception:\n                    # Ignore if not permitted or unsupported\n                    logger.warning(\"Failed to set session_replication_role to replica\")\n\n            conn = await session.connection()\n            automap: AutomapBase = automap_base()\n\n            def prepare_automap(sync_conn):\n                automap.prepare(autoload_with=sync_conn)\n\n            await conn.run_sync(prepare_automap)\n            mapper = getattr(automap.classes, table)\n\n            # Build defaults for non-nullable columns based on SQL types\n            sa_cols = {c.name: c for c in mapper.__table__.columns}\n\n            def default_for(col):\n                from sqlalchemy import (\n                    BigInteger,\n                    Date,\n                    DateTime,\n                    Integer,\n                    Numeric,\n                    String,\n                    Text,\n                )\n\n                t = col.type\n                if isinstance(t, (Integer, BigInteger)):\n                    return 0\n                if isinstance(t, Numeric):\n                    return 0\n                if isinstance(t, (String, Text)):\n                    return \"UNKNOWN\"\n                if isinstance(t, Date):\n                    return date(1970, 1, 1)\n                if isinstance(t, DateTime):\n                    return datetime(1970, 1, 1)\n                return None\n\n            # Work on a copy so we can normalize types and fill required fields\n            df2 = df.copy()\n\n            for name, col in sa_cols.items():\n                # Ensure column exists\n                if name not in df2.columns:\n                    # For nullable columns, start with None; for required, use default\n                    df2[name] = None if col.nullable else default_for(col)\n                    continue\n\n                # Coerce types and handle missing values\n                if str(df2[name].dtype) == \"object\":\n                    # Treat empty strings as missing\n                    df2[name] = df2[name].replace(\"\", np.nan)\n\n                from sqlalchemy import BigInteger\n                from sqlalchemy import Date as SA_Date\n                from sqlalchemy import DateTime as SA_DateTime\n                from sqlalchemy import Integer, Numeric, String, Text\n\n                t = col.type\n                if isinstance(t, SA_Date):\n                    ser = pd.to_datetime(df2[name], errors=\"coerce\").dt.date\n                    df2[name] = (\n                        ser.where(pd.notna(ser), None)\n                        if col.nullable\n                        else ser.fillna(default_for(col))\n                    )\n                elif isinstance(t, SA_DateTime):\n                    # Normalize to UTC-naive to avoid tz-aware vs tz-naive issues in Postgres\n                    ser = pd.to_datetime(df2[name], errors=\"coerce\", utc=True)\n\n                    # Convert to Python datetime and drop tzinfo\n                    def _to_naive(dt):\n                        try:\n                            if pd.isna(dt):\n                                return None\n                        except Exception:\n                            pass\n                        if hasattr(dt, \"to_pydatetime\"):\n                            py = dt.to_pydatetime()\n                        else:\n                            py = dt\n                        if getattr(py, \"tzinfo\", None) is not None:\n                            py = (\n                                py.tz_convert(\"UTC\").tz_localize(None)\n                                if hasattr(py, \"tz_convert\")\n                                else py.replace(tzinfo=None)\n                            )\n                        return py\n\n                    ser = ser.map(_to_naive)\n                    df2[name] = (\n                        ser.where(pd.notna(ser), None)\n                        if col.nullable\n                        else ser.fillna(default_for(col))\n                    )\n                elif isinstance(t, (Integer, BigInteger)):\n                    ser = pd.to_numeric(df2[name], errors=\"coerce\")\n                    df2[name] = (\n                        ser.where(pd.notna(ser), None)\n                        if col.nullable\n                        else ser.fillna(default_for(col))\n                    )\n                elif isinstance(t, Numeric):\n                    ser = pd.to_numeric(df2[name], errors=\"coerce\")\n                    df2[name] = (\n                        ser.where(pd.notna(ser), None)\n                        if col.nullable\n                        else ser.fillna(default_for(col))\n                    )\n                elif isinstance(t, (String, Text)):\n                    # Only cast non-null values to str and trim; keep nulls as None\n                    ser = df2[name].astype(object)\n                    mask = ser.notna()\n                    ser.loc[mask] = ser.loc[mask].astype(str).str.slice(0, 255)\n                    if col.nullable:\n                        ser = ser.where(pd.notna(ser), None)\n                    else:\n                        # Required string columns get a default\n                        ser = ser.where(pd.notna(ser), default_for(col))\n                    df2[name] = ser\n                else:\n                    # Fallback: ensure NaN/NaT -&gt; None for nullable cols, else fill default\n                    df2[name] = (\n                        df2[name].where(pd.notna(df2[name]), None)\n                        if col.nullable\n                        else df2[name].fillna(default_for(col))\n                    )\n\n            # Final safety pass: replace any remaining NaN/NaT with None across all columns\n            df2 = df2.where(pd.notna(df2), None)\n\n            stmt = insert(mapper)\n\n            try:\n                for _, group in df2.groupby(\n                    np.arange(df2.shape[0], dtype=int) // chunk_size\n                ):\n                    records = group.to_dict(\"records\")\n                    try:\n                        # Fast path: batch insert\n                        await session.execute(stmt, records)\n                    except Exception:\n                        logger.warning(\n                            \"Batch insert failed, falling back to row-by-row insert.\"\n                        )\n                        # Fallback: insert row-by-row, skipping bad rows\n                        for row in records:\n                            try:\n                                await session.execute(stmt, [row])\n                            except Exception:\n                                # Ignore duplicates/FK issues per row\n                                logger.warning(\n                                    f\"Failed to insert row: {row}. Skipping.\"\n                                )\n                                continue\n                    # Commit after each group\n                    await session.commit()\n            finally:\n                if is_pg:\n                    try:\n                        await session.execute(\n                            text(\"SET session_replication_role = origin\")\n                        )\n                    except Exception:\n                        pass\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.concept_code","title":"<code>concept_code</code>  <code>property</code>","text":"<p>Current concept_code for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.concept_id","title":"<code>concept_id</code>  <code>property</code> <code>writable</code>","text":"<p>Current concept_id for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.concept_name","title":"<code>concept_name</code>  <code>property</code>","text":"<p>Current concept_name for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.domain_id","title":"<code>domain_id</code>  <code>property</code>","text":"<p>Current domain_id for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.vocabulary_id","title":"<code>vocabulary_id</code>  <code>property</code>","text":"<p>Current vocabulary_id for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.create_vocab","title":"<code>create_vocab(folder, sample=None)</code>  <code>async</code>","text":"<p>Load vocabulary CSV files from a folder into the database.</p> <p>This imports the standard OMOP vocab tables (drug_strength, concept, concept_relationship, concept_ancestor, concept_synonym, vocabulary, relationship, concept_class, domain).</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <p>Path to the folder containing OMOP vocabulary CSVs.</p> required <code>sample</code> <p>Optional number of rows to limit per file during import.</p> <code>None</code> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>async def create_vocab(self, folder, sample=None):\n    \"\"\"Load vocabulary CSV files from a folder into the database.\n\n    This imports the standard OMOP vocab tables (drug_strength, concept,\n    concept_relationship, concept_ancestor, concept_synonym, vocabulary,\n    relationship, concept_class, domain).\n\n    Args:\n        folder: Path to the folder containing OMOP vocabulary CSVs.\n        sample: Optional number of rows to limit per file during import.\n    \"\"\"\n    try:\n        # Parents first (for concept FKs): DOMAIN, CONCEPT_CLASS\n        df = pd.read_csv(\n            folder + \"/DOMAIN.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"domain\", \"replace\")\n\n        df = pd.read_csv(\n            folder + \"/CONCEPT_CLASS.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"concept_class\", \"replace\")\n\n        # Then CONCEPT (uses domain_id and concept_class_id)\n        df = pd.read_csv(\n            folder + \"/CONCEPT.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        df[\"valid_start_date\"] = pd.to_datetime(\n            df[\"valid_start_date\"], errors=\"coerce\"\n        )\n        df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n        await self.write_vocab(df, \"concept\", \"replace\")\n\n        # Then VOCABULARY (uses vocabulary_concept_id -&gt; concept)\n        df = pd.read_csv(\n            folder + \"/VOCABULARY.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"vocabulary\", \"replace\")\n\n        # Relationship depends on concept for relationship_concept_id\n        df = pd.read_csv(\n            folder + \"/RELATIONSHIP.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"relationship\", \"replace\")\n\n        # Post-concept tables\n        df = pd.read_csv(\n            folder + \"/CONCEPT_RELATIONSHIP.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        df[\"valid_start_date\"] = pd.to_datetime(\n            df[\"valid_start_date\"], errors=\"coerce\"\n        )\n        df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n        await self.write_vocab(df, \"concept_relationship\", \"replace\")\n\n        df = pd.read_csv(\n            folder + \"/CONCEPT_SYNONYM.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"concept_synonym\", \"replace\")\n\n        df = pd.read_csv(\n            folder + \"/DRUG_STRENGTH.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        df[\"valid_start_date\"] = pd.to_datetime(\n            df[\"valid_start_date\"], errors=\"coerce\"\n        )\n        df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n        await self.write_vocab(df, \"drug_strength\", \"replace\")\n\n        df = pd.read_csv(\n            folder + \"/CONCEPT_ANCESTOR.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"concept_ancestor\", \"replace\")\n    except Exception as e:\n        logger.error(f\"An error occurred while creating the vocabulary: {e}\")\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.get_concept","title":"<code>get_concept(concept_id)</code>  <code>async</code>","text":"<p>Fetch a concept row by id.</p> <p>Parameters:</p> Name Type Description Default <code>concept_id</code> <p>Concept identifier.</p> required <p>Returns:</p> Type Description <p>The ORM Concept instance.</p> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>async def get_concept(self, concept_id):\n    \"\"\"Fetch a concept row by id.\n\n    Args:\n        concept_id: Concept identifier.\n\n    Returns:\n        The ORM Concept instance.\n    \"\"\"\n    if self._version == \"cdm6\":\n        from .cdm6 import Concept\n    else:\n        from .cdm54 import Concept\n    stmt = select(Concept).where(Concept.concept_id == concept_id)\n    async with self._cdm.session() as session:\n        _concept = await session.execute(stmt)\n    return _concept.scalar_one()\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.get_concept_by_code","title":"<code>get_concept_by_code(concept_code, vocabulary_id)</code>  <code>async</code>","text":"<p>Fetch a concept by code within a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>concept_code</code> <p>The vocabulary-specific code string.</p> required <code>vocabulary_id</code> <p>Vocabulary identifier (e.g., 'SNOMED', 'LOINC').</p> required <p>Returns:</p> Type Description <p>The ORM Concept instance.</p> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>async def get_concept_by_code(self, concept_code, vocabulary_id):\n    \"\"\"Fetch a concept by code within a vocabulary.\n\n    Args:\n        concept_code: The vocabulary-specific code string.\n        vocabulary_id: Vocabulary identifier (e.g., 'SNOMED', 'LOINC').\n\n    Returns:\n        The ORM Concept instance.\n    \"\"\"\n    if self._version == \"cdm6\":\n        from .cdm6 import Concept\n    else:\n        from .cdm54 import Concept\n    stmt = (\n        select(Concept)\n        .where(Concept.concept_code == concept_code)\n        .where(Concept.vocabulary_id == vocabulary_id)\n    )\n    async with self._cdm.session() as session:\n        _concept = await session.execute(stmt)\n    return _concept.scalar_one()\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.get_session","title":"<code>get_session()</code>  <code>async</code>","text":"<p>Yield an async session bound to the current engine.</p> <p>Yields:</p> Name Type Description <code>AsyncSession</code> <code>AsyncGenerator[AsyncSession, None]</code> <p>An async SQLAlchemy session.</p> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>@asynccontextmanager\nasync def get_session(self) -&gt; AsyncGenerator[AsyncSession, None]:\n    \"\"\"Yield an async session bound to the current engine.\n\n    Yields:\n        AsyncSession: An async SQLAlchemy session.\n    \"\"\"\n    async with self._scope() as session:\n        yield session\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.set_concept","title":"<code>set_concept(concept_code, vocabulary_id=None)</code>","text":"<p>Set the active concept context by code and vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>concept_code</code> <p>The concept code string to resolve.</p> required <code>vocabulary_id</code> <p>Vocabulary identifier. Required.</p> <code>None</code> Notes <p>On success, populates concept fields on this instance. On failure, sets <code>_vocabulary_id</code> and <code>_concept_id</code> to 0.</p> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>def set_concept(self, concept_code, vocabulary_id=None):\n    \"\"\"Set the active concept context by code and vocabulary.\n\n    Args:\n        concept_code: The concept code string to resolve.\n        vocabulary_id: Vocabulary identifier. Required.\n\n    Notes:\n        On success, populates concept fields on this instance. On failure,\n        sets ``_vocabulary_id`` and ``_concept_id`` to 0.\n    \"\"\"\n    self._concept_code = concept_code\n    try:\n        if vocabulary_id is not None:\n            self._vocabulary_id = vocabulary_id\n            _concept = asyncio.run(\n                self.get_concept_by_code(concept_code, vocabulary_id)\n            )\n        else:\n            raise ValueError(\n                \"vocabulary_id must be provided when setting concept by code.\"\n            )\n\n        self._concept_name = _concept.concept_name\n        self._domain_id = _concept.domain_id\n        self._concept_id = _concept.concept_id\n        self._concept_class_id = _concept.concept_class_id\n        self._concept_code = _concept.concept_code\n\n    except:\n        self._vocabulary_id = 0\n        self._concept_id = 0\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.write_vocab","title":"<code>write_vocab(df, table, if_exists='replace', chunk_size=1000)</code>  <code>async</code>","text":"<p>Write a DataFrame to a vocabulary table with type-safe defaults.</p> <p>Ensures required columns exist with reasonable defaults, coerces types, and performs chunked inserts via SQLAlchemy core for performance.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>Pandas DataFrame with data to insert.</p> required <code>table</code> <p>Target table name (e.g., 'concept').</p> required <code>if_exists</code> <p>Compatibility only. This method always inserts.</p> <code>'replace'</code> <code>chunk_size</code> <p>Number of rows per batch insert.</p> <code>1000</code> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>async def write_vocab(self, df, table, if_exists=\"replace\", chunk_size=1000):\n    \"\"\"Write a DataFrame to a vocabulary table with type-safe defaults.\n\n    Ensures required columns exist with reasonable defaults, coerces types,\n    and performs chunked inserts via SQLAlchemy core for performance.\n\n    Args:\n        df: Pandas DataFrame with data to insert.\n        table: Target table name (e.g., 'concept').\n        if_exists: Compatibility only. This method always inserts.\n        chunk_size: Number of rows per batch insert.\n    \"\"\"\n    async with self.get_session() as session:\n        # For PostgreSQL, temporarily relax constraint enforcement during bulk loads\n        is_pg = False\n        try:\n            is_pg = self._engine.dialect.name.startswith(\"postgres\")\n        except Exception:\n            is_pg = False\n        if is_pg:\n            logger.info(\n                \"Temporarily disabling replication role for bulk load on postgres\"\n            )\n            try:\n                await session.execute(\n                    text(\"SET session_replication_role = replica\")\n                )\n            except Exception:\n                # Ignore if not permitted or unsupported\n                logger.warning(\"Failed to set session_replication_role to replica\")\n\n        conn = await session.connection()\n        automap: AutomapBase = automap_base()\n\n        def prepare_automap(sync_conn):\n            automap.prepare(autoload_with=sync_conn)\n\n        await conn.run_sync(prepare_automap)\n        mapper = getattr(automap.classes, table)\n\n        # Build defaults for non-nullable columns based on SQL types\n        sa_cols = {c.name: c for c in mapper.__table__.columns}\n\n        def default_for(col):\n            from sqlalchemy import (\n                BigInteger,\n                Date,\n                DateTime,\n                Integer,\n                Numeric,\n                String,\n                Text,\n            )\n\n            t = col.type\n            if isinstance(t, (Integer, BigInteger)):\n                return 0\n            if isinstance(t, Numeric):\n                return 0\n            if isinstance(t, (String, Text)):\n                return \"UNKNOWN\"\n            if isinstance(t, Date):\n                return date(1970, 1, 1)\n            if isinstance(t, DateTime):\n                return datetime(1970, 1, 1)\n            return None\n\n        # Work on a copy so we can normalize types and fill required fields\n        df2 = df.copy()\n\n        for name, col in sa_cols.items():\n            # Ensure column exists\n            if name not in df2.columns:\n                # For nullable columns, start with None; for required, use default\n                df2[name] = None if col.nullable else default_for(col)\n                continue\n\n            # Coerce types and handle missing values\n            if str(df2[name].dtype) == \"object\":\n                # Treat empty strings as missing\n                df2[name] = df2[name].replace(\"\", np.nan)\n\n            from sqlalchemy import BigInteger\n            from sqlalchemy import Date as SA_Date\n            from sqlalchemy import DateTime as SA_DateTime\n            from sqlalchemy import Integer, Numeric, String, Text\n\n            t = col.type\n            if isinstance(t, SA_Date):\n                ser = pd.to_datetime(df2[name], errors=\"coerce\").dt.date\n                df2[name] = (\n                    ser.where(pd.notna(ser), None)\n                    if col.nullable\n                    else ser.fillna(default_for(col))\n                )\n            elif isinstance(t, SA_DateTime):\n                # Normalize to UTC-naive to avoid tz-aware vs tz-naive issues in Postgres\n                ser = pd.to_datetime(df2[name], errors=\"coerce\", utc=True)\n\n                # Convert to Python datetime and drop tzinfo\n                def _to_naive(dt):\n                    try:\n                        if pd.isna(dt):\n                            return None\n                    except Exception:\n                        pass\n                    if hasattr(dt, \"to_pydatetime\"):\n                        py = dt.to_pydatetime()\n                    else:\n                        py = dt\n                    if getattr(py, \"tzinfo\", None) is not None:\n                        py = (\n                            py.tz_convert(\"UTC\").tz_localize(None)\n                            if hasattr(py, \"tz_convert\")\n                            else py.replace(tzinfo=None)\n                        )\n                    return py\n\n                ser = ser.map(_to_naive)\n                df2[name] = (\n                    ser.where(pd.notna(ser), None)\n                    if col.nullable\n                    else ser.fillna(default_for(col))\n                )\n            elif isinstance(t, (Integer, BigInteger)):\n                ser = pd.to_numeric(df2[name], errors=\"coerce\")\n                df2[name] = (\n                    ser.where(pd.notna(ser), None)\n                    if col.nullable\n                    else ser.fillna(default_for(col))\n                )\n            elif isinstance(t, Numeric):\n                ser = pd.to_numeric(df2[name], errors=\"coerce\")\n                df2[name] = (\n                    ser.where(pd.notna(ser), None)\n                    if col.nullable\n                    else ser.fillna(default_for(col))\n                )\n            elif isinstance(t, (String, Text)):\n                # Only cast non-null values to str and trim; keep nulls as None\n                ser = df2[name].astype(object)\n                mask = ser.notna()\n                ser.loc[mask] = ser.loc[mask].astype(str).str.slice(0, 255)\n                if col.nullable:\n                    ser = ser.where(pd.notna(ser), None)\n                else:\n                    # Required string columns get a default\n                    ser = ser.where(pd.notna(ser), default_for(col))\n                df2[name] = ser\n            else:\n                # Fallback: ensure NaN/NaT -&gt; None for nullable cols, else fill default\n                df2[name] = (\n                    df2[name].where(pd.notna(df2[name]), None)\n                    if col.nullable\n                    else df2[name].fillna(default_for(col))\n                )\n\n        # Final safety pass: replace any remaining NaN/NaT with None across all columns\n        df2 = df2.where(pd.notna(df2), None)\n\n        stmt = insert(mapper)\n\n        try:\n            for _, group in df2.groupby(\n                np.arange(df2.shape[0], dtype=int) // chunk_size\n            ):\n                records = group.to_dict(\"records\")\n                try:\n                    # Fast path: batch insert\n                    await session.execute(stmt, records)\n                except Exception:\n                    logger.warning(\n                        \"Batch insert failed, falling back to row-by-row insert.\"\n                    )\n                    # Fallback: insert row-by-row, skipping bad rows\n                    for row in records:\n                        try:\n                            await session.execute(stmt, [row])\n                        except Exception:\n                            # Ignore duplicates/FK issues per row\n                            logger.warning(\n                                f\"Failed to insert row: {row}. Skipping.\"\n                            )\n                            continue\n                # Commit after each group\n                await session.commit()\n        finally:\n            if is_pg:\n                try:\n                    await session.execute(\n                        text(\"SET session_replication_role = origin\")\n                    )\n                except Exception:\n                    pass\n</code></pre>"},{"location":"modules/#vector.CdmVector","title":"<code>CdmVector</code>","text":"<p>               Bases: <code>object</code></p> <p>Query execution utility for OMOP CDM.</p> <p>Methods let you run raw SQL or QueryLibrary snippets and turn results into pandas DataFrames.</p> Source code in <code>src/pyomop/vector.py</code> <pre><code>class CdmVector(object):\n    \"\"\"Query execution utility for OMOP CDM.\n\n    Methods let you run raw SQL or QueryLibrary snippets and turn results into\n    pandas DataFrames.\n    \"\"\"\n\n    async def execute(self, cdm, sqldict=None, query=None, chunksize=1000):\n        \"\"\"Execute a SQL query asynchronously.\n\n        Args:\n            cdm: CdmEngineFactory instance.\n            sqldict: Optional key from ``CDMSQL`` to pick a canned query.\n            query: Raw SQL string (used if provided).\n            chunksize: Unused; kept for future streaming support.\n\n        Returns:\n            SQLAlchemy AsyncResult.\n        \"\"\"\n        if sqldict:\n            query = CDMSQL[sqldict]\n        if not isinstance(query, str) or not query:\n            raise ValueError(\"Query must be a non-empty string.\")\n        logger.info(f\"Executing query: {query}\")\n        async with cdm.session() as session:\n            result = await session.execute(text(query))\n        await session.close()\n        return result\n\n    def result_to_df(self, result):\n        \"\"\"Convert a Result to a DataFrame.\n\n        Args:\n            result: SQLAlchemy Result or AsyncResult.\n\n        Returns:\n            pandas.DataFrame of result mappings.\n        \"\"\"\n        list_of_dicts = result.mappings().all()\n        \"\"\"Convert a list of dictionaries to a DataFrame.\"\"\"\n        if not list_of_dicts:\n            return pd.DataFrame()\n        return pd.DataFrame(list_of_dicts)\n\n    async def query_library(self, cdm, resource=\"person\", query_name=\"PE02\"):\n        \"\"\"Fetch a query from OHDSI QueryLibrary and execute it.\n\n        Args:\n            cdm: CdmEngineFactory instance.\n            resource: Query resource subfolder (e.g., \"person\").\n            query_name: Query markdown file name (e.g., \"PE02\").\n\n        Returns:\n            SQLAlchemy AsyncResult.\n        \"\"\"\n        # Get the markdown from the query library repository: https://github.com/OHDSI/QueryLibrary/blob/master/inst/shinyApps/QueryLibrary/queries/person/PE02.md\n        url = f\"https://raw.githubusercontent.com/OHDSI/QueryLibrary/master/inst/shinyApps/QueryLibrary/queries/{resource}/{query_name}.md\"\n        markdown = requests.get(url)\n        if markdown.status_code != 200:\n            raise ValueError(f\"Query {query_name} not found in the Query Library.\")\n        query = markdown.text.split(\"```sql\")[1].split(\"```\")[0].strip()\n        # remove @cdm. and @vocab. references\n        query = query.replace(\"@cdm.\", \"\").replace(\"@vocab.\", \"\")\n        if not query:\n            raise ValueError(f\"Query {query_name} is empty.\")\n        return await self.execute(cdm, query=query)\n</code></pre>"},{"location":"modules/#vector.CdmVector.execute","title":"<code>execute(cdm, sqldict=None, query=None, chunksize=1000)</code>  <code>async</code>","text":"<p>Execute a SQL query asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>cdm</code> <p>CdmEngineFactory instance.</p> required <code>sqldict</code> <p>Optional key from <code>CDMSQL</code> to pick a canned query.</p> <code>None</code> <code>query</code> <p>Raw SQL string (used if provided).</p> <code>None</code> <code>chunksize</code> <p>Unused; kept for future streaming support.</p> <code>1000</code> <p>Returns:</p> Type Description <p>SQLAlchemy AsyncResult.</p> Source code in <code>src/pyomop/vector.py</code> <pre><code>async def execute(self, cdm, sqldict=None, query=None, chunksize=1000):\n    \"\"\"Execute a SQL query asynchronously.\n\n    Args:\n        cdm: CdmEngineFactory instance.\n        sqldict: Optional key from ``CDMSQL`` to pick a canned query.\n        query: Raw SQL string (used if provided).\n        chunksize: Unused; kept for future streaming support.\n\n    Returns:\n        SQLAlchemy AsyncResult.\n    \"\"\"\n    if sqldict:\n        query = CDMSQL[sqldict]\n    if not isinstance(query, str) or not query:\n        raise ValueError(\"Query must be a non-empty string.\")\n    logger.info(f\"Executing query: {query}\")\n    async with cdm.session() as session:\n        result = await session.execute(text(query))\n    await session.close()\n    return result\n</code></pre>"},{"location":"modules/#vector.CdmVector.query_library","title":"<code>query_library(cdm, resource='person', query_name='PE02')</code>  <code>async</code>","text":"<p>Fetch a query from OHDSI QueryLibrary and execute it.</p> <p>Parameters:</p> Name Type Description Default <code>cdm</code> <p>CdmEngineFactory instance.</p> required <code>resource</code> <p>Query resource subfolder (e.g., \"person\").</p> <code>'person'</code> <code>query_name</code> <p>Query markdown file name (e.g., \"PE02\").</p> <code>'PE02'</code> <p>Returns:</p> Type Description <p>SQLAlchemy AsyncResult.</p> Source code in <code>src/pyomop/vector.py</code> <pre><code>async def query_library(self, cdm, resource=\"person\", query_name=\"PE02\"):\n    \"\"\"Fetch a query from OHDSI QueryLibrary and execute it.\n\n    Args:\n        cdm: CdmEngineFactory instance.\n        resource: Query resource subfolder (e.g., \"person\").\n        query_name: Query markdown file name (e.g., \"PE02\").\n\n    Returns:\n        SQLAlchemy AsyncResult.\n    \"\"\"\n    # Get the markdown from the query library repository: https://github.com/OHDSI/QueryLibrary/blob/master/inst/shinyApps/QueryLibrary/queries/person/PE02.md\n    url = f\"https://raw.githubusercontent.com/OHDSI/QueryLibrary/master/inst/shinyApps/QueryLibrary/queries/{resource}/{query_name}.md\"\n    markdown = requests.get(url)\n    if markdown.status_code != 200:\n        raise ValueError(f\"Query {query_name} not found in the Query Library.\")\n    query = markdown.text.split(\"```sql\")[1].split(\"```\")[0].strip()\n    # remove @cdm. and @vocab. references\n    query = query.replace(\"@cdm.\", \"\").replace(\"@vocab.\", \"\")\n    if not query:\n        raise ValueError(f\"Query {query_name} is empty.\")\n    return await self.execute(cdm, query=query)\n</code></pre>"},{"location":"modules/#vector.CdmVector.result_to_df","title":"<code>result_to_df(result)</code>","text":"<p>Convert a Result to a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <p>SQLAlchemy Result or AsyncResult.</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame of result mappings.</p> Source code in <code>src/pyomop/vector.py</code> <pre><code>def result_to_df(self, result):\n    \"\"\"Convert a Result to a DataFrame.\n\n    Args:\n        result: SQLAlchemy Result or AsyncResult.\n\n    Returns:\n        pandas.DataFrame of result mappings.\n    \"\"\"\n    list_of_dicts = result.mappings().all()\n    \"\"\"Convert a list of dictionaries to a DataFrame.\"\"\"\n    if not list_of_dicts:\n        return pd.DataFrame()\n    return pd.DataFrame(list_of_dicts)\n</code></pre>"}]}