{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"pyomop: OMOP Swiss Army Knife \ud83d\udd27","text":""},{"location":"#overview","title":"\u2728 Overview","text":"<p>pyomop is your OMOP Swiss Army Knife \ud83d\udd27 for working with OHDSI OMOP Common Data Model (CDM) v5.4 or v6 compliant databases using SQLAlchemy as the ORM. It supports converting query results to pandas DataFrames for machine learning pipelines and provides utilities for working with OMOP vocabularies. Table definitions are based on the omop-cdm library. Pyomop is designed to be a lightweight, easy-to-use library for researchers and developers experimenting and testing with OMOP CDM databases. It can be used both as a commandline tool and as an imported library in your code.</p> <ul> <li>Supports SQLite, PostgreSQL, and MySQL. CDM and Vocab tables are created in the same schema. (See usage below for more details)</li> <li>LLM-based natural language queries via langchain. Usage.</li> <li>\ud83d\udd25 FHIR to OMOP conversion utilities. (See usage below for more details)</li> <li>Execute QueryLibrary. (See usage below for more details)</li> </ul> <p>Please \u2b50\ufe0f If you find this project useful!</p>"},{"location":"#installation","title":"Installation","text":"<p>Stable release:</p> <pre><code>pip install pyomop\n</code></pre> <p>Development version:</p> <pre><code>git clone https://github.com/dermatologist/pyomop.git\ncd pyomop\npip install -e .\n</code></pre> <p>LLM support:</p> <pre><code>pip install pyomop[llm]\n</code></pre>"},{"location":"#see-this-notebook-or-script-for-examples-mcp-server-is-recommended-for-advanced-usage","title":"\u2728 See this notebook or script for examples. \ud83d\udc47 MCP SERVER is recommended for advanced usage.","text":""},{"location":"#docker","title":"Docker","text":"<ul> <li>A docker-compose is provided to quickly set up an environment with postgrs, webapi, atlas and a sql script to create a source in webapi. The script can be run using the <code>psql</code> command line tool or via the webapi UI. Please refresh after running the script by sending a request to /WebAPI/source/refresh.</li> </ul>"},{"location":"#usage","title":"\ud83d\udd27 Usage","text":"<pre><code>import asyncio\nimport datetime\n\nfrom sqlalchemy import select\n\nfrom pyomop import CdmEngineFactory, CdmVector, CdmVocabulary\n# cdm6 and cdm54 are supported\nfrom pyomop.cdm54 import Base, Cohort, Person, Vocabulary\n\nasync def main():\n    cdm = CdmEngineFactory() # Creates SQLite database by default for fast testing\n    # cdm = CdmEngineFactory(db='pgsql', host='', port=5432,\n    #                       user='', pw='',\n    #                       name='', schema='')\n    # cdm = CdmEngineFactory(db='mysql', host='', port=3306,\n    #                       user='', pw='',\n    #                       name='')\n    engine = cdm.engine\n    # Comment the following line if using an existing database. Both cdm6 and cdm54 are supported, see the import statements above\n    await cdm.init_models(Base.metadata) # Initializes the database with the OMOP CDM tables\n    vocab = CdmVocabulary(cdm, version='cdm54') # or 'cdm6' for v6\n    # Uncomment the following line to create a new vocabulary from CSV files\n    # vocab.create_vocab('/path/to/csv/files')\n\n    async with cdm.session() as session:  # type: ignore\n        # Add Persons\n        async with session.begin():\n            session.add(\n                Person(\n                    person_id=100,\n                    gender_concept_id=8532,\n                    gender_source_concept_id=8512,\n                    year_of_birth=1980,\n                    month_of_birth=1,\n                    day_of_birth=1,\n                    birth_datetime=datetime.datetime(1980, 1, 1),\n                    race_concept_id=8552,\n                    race_source_concept_id=8552,\n                    ethnicity_concept_id=38003564,\n                    ethnicity_source_concept_id=38003564,\n                )\n            )\n            session.add(\n                Person(\n                    person_id=101,\n                    gender_concept_id=8532,\n                    gender_source_concept_id=8512,\n                    year_of_birth=1980,\n                    month_of_birth=1,\n                    day_of_birth=1,\n                    birth_datetime=datetime.datetime(1980, 1, 1),\n                    race_concept_id=8552,\n                    race_source_concept_id=8552,\n                    ethnicity_concept_id=38003564,\n                    ethnicity_source_concept_id=38003564,\n                )\n            )\n\n        # Query the Person\n        stmt = select(Person).where(Person.person_id == 100)\n        result = await session.execute(stmt)\n        for row in result.scalars():\n            print(row)\n            assert row.person_id == 100\n\n        # Query the person pattern 2\n        person = await session.get(Person, 100)\n        print(person)\n        assert person is not None\n        assert person.person_id == 100\n\n    # Convert result to a pandas dataframe\n    vec = CdmVector()\n\n    # https://github.com/OHDSI/QueryLibrary/blob/master/inst/shinyApps/QueryLibrary/queries/person/PE02.md\n    result = await vec.query_library(cdm, resource='person', query_name='PE02')\n    df = vec.result_to_df(result)\n    print(\"DataFrame from result:\")\n    print(df.head())\n\n    result = await vec.execute(cdm, query='SELECT * from person;')\n    print(\"Executing custom query:\")\n    df = vec.result_to_df(result)\n    print(\"DataFrame from result:\")\n    print(df.head())\n\n    # Close engine\n    await engine.dispose() # type: ignore\n\n# Run the main function\nasyncio.run(main())\n</code></pre>"},{"location":"#fhir-to-omop-mapping","title":"\ud83d\udd25 FHIR to OMOP mapping","text":"<p>pyomop can load FHIR Bulk Export (NDJSON) files into an OMOP CDM database.</p> <ul> <li>Sample datasets: https://github.com/smart-on-fhir/sample-bulk-fhir-datasets</li> <li>Remove any non-FHIR files (for example, <code>log.ndjson</code>) from the input folder.</li> <li>Download OMOP vocabulary CSV files (for example from OHDSI Athena) and place them in a folder.</li> </ul> <p>Run:</p> <pre><code>pyomop --create --vocab ~/Downloads/omop-vocab/ --input ~/Downloads/fhir/\n</code></pre> <p>This will create an OMOP CDM in SQLite, load the vocabulary files, and import the FHIR data from the input folder and reconcile vocabulary, mapping source_value to concept_id. The mapping is defined in the <code>mapping.example.json</code> file. The default mapping is here. Mapping happens in 5 steps as implemented here.</p> <ul> <li>Example using postgres (Docker)</li> </ul> <pre><code>pyomop --dbtype pgsql --host localhost --user postgres --pw mypass  --create --vocab ~/Downloads/omop-vocab/ --input ~/Downloads/fhir/\n</code></pre> <ul> <li>FHIR to data frame mapping is done with FHIRy</li> <li>Most of the code for this functionality was written by an LLM agent. The prompts used are here</li> </ul>"},{"location":"#command-line","title":"Command-line","text":"<pre><code>  -c, --create                Create CDM tables (see --version).\n  -t, --dbtype TEXT           Database Type for creating CDM (sqlite, mysql or\n                              pgsql)\n  -h, --host TEXT             Database host\n  -p, --port TEXT             Database port\n  -u, --user TEXT             Database user\n  -w, --pw TEXT               Database password\n  -v, --version TEXT          CDM version (cdm54 (default) or cdm6)\n  -n, --name TEXT             Database name\n  -s, --schema TEXT           Database schema (for pgsql)\n  -i, --vocab TEXT            Folder with vocabulary files (csv) to import\n  -f, --input DIRECTORY       Input folder with FHIR bundles or ndjson files.\n  -e, --eunomia-dataset TEXT  Download and load Eunomia dataset (e.g.,\n                              'GiBleed', 'Synthea')\n  --eunomia-path TEXT         Path to store/find Eunomia datasets (uses\n                              EUNOMIA_DATA_FOLDER env var if not specified)\n  --connection-info           Display connection information for the database (For R package compatibility)\n  --mcp-server                Start MCP server for stdio interaction\n  --pyhealth-path TEXT        Path to export PyHealth compatible CSV files\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"#mcp-server","title":"MCP Server","text":"<p>pyomop includes an MCP (Model Context Protocol) server that exposes tools for interacting with OMOP CDM databases. This allows MCP clients to create databases, load data, and execute SQL statements.</p> <p> </p>"},{"location":"#usage-with-mcp-clients","title":"Usage with MCP Clients","text":"<p>The MCP server can be used with any MCP-compatible client such as Claude desktop. Example configuration for VSCODE as below is already provided in the repository. So if you are viewing this in VSCODE, you can start server and enable tools directly in Copilot.</p> <pre><code>{\n  \"servers\": {\n      \"pyomop\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"pyomop\", \"--mcp-server\"]\n    }\n  }\n}\n</code></pre> <ul> <li>If the vocabulary is not installed locally or advanced vocabulary support is required from Athena, it is recommended to combine omop_mcp with PyOMOP.</li> </ul>"},{"location":"#available-mcp-tools","title":"Available MCP Tools","text":"<ul> <li>create_cdm: Create an empty CDM database</li> <li>create_eunomia: Add Eunomia sample dataset</li> <li>get_table_columns: Get column names for a specific table</li> <li>get_single_table_info: Get detailed table information, including foreign keys</li> <li>get_usable_table_names: Get a list of all available table names</li> <li>run_sql: Execute SQL statements with error handling</li> <li>example_query: Get example queries for specific OMOP CDM tables from OHDSI QueryLibrary</li> <li> <p>check_sql: Validate SQL query syntax before execution</p> </li> <li> <p>create_cdm and create_eunomia support only local sqlite databases to avoid inadvertent data loss in production databases.</p> </li> </ul>"},{"location":"#http-transport-support","title":"HTTP Transport Support","text":"<p>The MCP server now supports both stdio (default) and HTTP transports:</p> <p>Stdio transport (default):</p> <pre><code>pyomop --mcp-server\n# or\npyomop-mcp-server\n</code></pre> <p>HTTP transport:</p> <pre><code>pyomop-mcp-server-http\n# or with custom host/port\npyomop-mcp-server-http --host 0.0.0.0 --port 8000\n# or via Python module\npython -m pyomop.mcp.server --http --host 0.0.0.0 --port 8000\n</code></pre> <p>To use HTTP transport, install additional dependencies:</p> <pre><code>pip install pyomop[http]\n# or for both LLM and HTTP features\npip install pyomop[llm,http]\n</code></pre>"},{"location":"#available-prompts","title":"Available Prompts","text":"<ul> <li>query_execution_steps: Provides step-by-step guidance for executing database queries based on free text instructions</li> </ul>"},{"location":"#eunomia-import-and-cohort-creation","title":"Eunomia import and cohort creation","text":"<pre><code>pyomop -e Synthea27Nj -v 5.4 --connection-info\npyomop -e GiBleed -v 5.3 --connection-info\n</code></pre>"},{"location":"#pyhealth-and-plp-compatibility-for-machine-learning-pipelines","title":"PyHealth and PLP Compatibility (For Machine Learning pipelines)","text":"<p>pyomop supports exporting OMOP CDM data (to <code>--pyhealth-path</code>) in a format compatible with PyHealth, a machine learning library for healthcare data analysis (See Notebook and usage below). Additionally, you can export the connection information for use with the various R packages such as PatientLevelPrediction using the <code>--connection-info</code> option.</p> <pre><code>pyomop -e GiBleed -v 5.3 --connection-info --pyhealth-path ~/pyhealth\n</code></pre>"},{"location":"#additional-tools","title":"Additional Tools","text":"<ul> <li>Convert FHIR to pandas DataFrame: fhiry</li> <li>.NET and Golang OMOP CDM: .NET, Golang</li> </ul>"},{"location":"#supported-databases","title":"Supported Databases","text":"<ul> <li>PostgreSQL</li> <li>MySQL</li> <li>SQLite</li> </ul>"},{"location":"#environment-variables-for-database-connection","title":"Environment Variables for Database Connection","text":"<p>You can configure database connection parameters using environment variables. These will be used as defaults by pyomop and the MCP server:</p> <ul> <li><code>PYOMOP_DB</code>: Database type (<code>sqlite</code>, <code>mysql</code>, <code>pgsql</code>)</li> <li><code>PYOMOP_HOST</code>: Database host</li> <li><code>PYOMOP_PORT</code>: Database port</li> <li><code>PYOMOP_USER</code>: Database user</li> <li><code>PYOMOP_PW</code>: Database password</li> <li><code>PYOMOP_SCHEMA</code>: Database schema (for PostgreSQL)</li> </ul> <p>Example usage:</p> <pre><code>export PYOMOP_DB=pgsql\nexport PYOMOP_HOST=localhost\nexport PYOMOP_PORT=5432\nexport PYOMOP_USER=postgres\nexport PYOMOP_PW=mypass\nexport PYOMOP_SCHEMA=omop\n</code></pre> <p>These environment variables will be checked before assigning default values for database connection in pyomop and MCP server tools.</p>"},{"location":"#agent-assisted-etl-work-in-progress","title":"\ud83d\uddc4\ufe0f Agent Assisted ETL - Work in progress","text":"<p>Use <code>--migrate</code> to run the generic loader from the command line.  Provide source-database connection details with <code>--src-*</code> options; target-database details use the standard <code>--dbtype</code> / <code>--host</code> / \u2026 options.</p> <pre><code># SQLite source \u2192 SQLite OMOP target\npyomop-migrate --migrate \\\n  --src-dbtype sqlite --src-name source.sqlite \\\n  --dbtype sqlite --name omop.sqlite \\\n  --mapping mapping.json\n\n# PostgreSQL source \u2192 PostgreSQL OMOP target\npyomop-migrate --migrate \\\n  --src-dbtype pgsql --src-host srchost --src-user reader --src-pw secret --src-name ehr \\\n  --dbtype pgsql --host omophost --user writer --pw secret --name omop \\\n  --mapping ehr_to_omop.json --batch-size 500\n</code></pre> <p>Source connection credentials can also be provided via environment variables (<code>SRC_DB_HOST</code>, <code>SRC_DB_PORT</code>, <code>SRC_DB_USER</code>, <code>SRC_DB_PASSWORD</code>, <code>SRC_DB_NAME</code>) to avoid exposing passwords in the shell history.</p>"},{"location":"#schema-extraction","title":"Schema extraction","text":"<p>Use <code>--extract-schema</code> to generate a Markdown document describing the source database schema (tables, columns, types, PK/FK relationships).  This is especially useful for feeding to an AI agent to generate the mapping JSON.</p> <pre><code>pyomop-migrate --extract-schema \\\n  --src-dbtype sqlite --src-name source.sqlite \\\n  --schema-output schema.md\n</code></pre> <p>The same <code>SRC_DB_*</code> environment variables are supported for credentials.</p>"},{"location":"#plan","title":"Plan","text":"<ul> <li>Use the extracted schema to generate a mapping JSON using an appropriate agentic skill.</li> </ul> <p>See the bundled example mapping and the full documentation for all supported options.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Pull requests are welcome! See CONTRIBUTING.md.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Bell Eapen </li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Generate a generic database loader for OMOP CDM. #259</li> <li>Add PyHealth compatible export function #243 (Copilot)</li> </ul> <p>Merged pull requests:</p> <ul> <li>Add CdmGenericLoader: database-to-OMOP CDM ETL via JSON mapping and <code>pyomop-migrate</code> CLI #260 (Copilot)</li> <li>Feature/fix time query 1 #258 (dermatologist)</li> <li>Add missing MCP tools and HTTP transport support #256 (Copilot)</li> <li>Feature/fix thread 1 #254 (dermatologist)</li> <li>Fix LLM query engine bugs and switch to native llama-index embeddings #253 (Copilot)</li> <li>[GitHub Dependents Info] Updated markdown file #252 (github-actions[bot])</li> <li>build\\(deps\\): bump peter-evans/create-pull-request from 7 to 8 #251 (dependabot[bot])</li> <li>build\\(deps\\): bump nvuillam/github-dependents-info from 1.6.3 to 2.0.2 #250 (dependabot[bot])</li> <li>Revise MCP Tools and Prompts sections in README #249 (dermatologist)</li> <li>build\\(deps\\): bump actions/checkout from 5 to 6 #248 (dependabot[bot])</li> <li>Lower coverage thresholds in codecov.yaml #247 (dermatologist)</li> <li>[Automated] Dependencies upgrade #246 (github-actions[bot])</li> <li>build\\(deps\\): bump astral-sh/setup-uv from 6 to 7 #245 (dependabot[bot])</li> <li>Add MCP server feature for CDM database interaction #241 (Copilot)</li> <li>Add OMOP CDM sample data functionality via Eunomia port #237 (Copilot)</li> <li>[GitHub Dependents Info] Updated markdown file #234 (github-actions[bot])</li> <li>[Automated] Dependencies upgrade #233 (github-actions[bot])</li> <li>Feature/hot fix 3 #232 (dermatologist)</li> <li>Feature/docker 1 #230 (dermatologist)</li> <li>Feature/hotfix 2 #229 (dermatologist)</li> <li>Feature/fhir to omop 2 #228 (dermatologist)</li> <li>Feature/docker db #225 (dermatologist)</li> <li>feat: update CLI options and add version support; introduce test for \u2026 #224 (dermatologist)</li> <li>Feature/new model #223 (dermatologist)</li> <li>Feature/refactor 1 #222 (dermatologist)</li> <li>build: update Python version to 3.11 in Dockerfile and devcontainer.j\u2026 #221 (dermatologist)</li> <li>build\\(deps\\): bump astral-sh/setup-uv from 5 to 6 #220 (dependabot[bot])</li> <li>Feature/llm update 1 #219 (dermatologist)</li> <li>Feature/pr new 1 #218 (dermatologist)</li> <li>refactor: clean up code formatting and remove unnecessary newlines #204 (dermatologist)</li> <li>build\\(deps\\): bump jinja2 from 3.1.4 to 3.1.5 #200 (dependabot[bot])</li> <li>build\\(deps-dev\\): bump wheel from 0.37.1 to 0.45.1 #195 (dependabot[bot])</li> <li>Feature/update deps 1 #194 (dermatologist)</li> <li>build\\(deps\\): bump actions/setup-python from 5.0.0 to 5.3.0 #190 (dependabot[bot])</li> <li>build\\(deps\\): bump certifi from 2023.7.22 to 2024.7.4 #178 (dependabot[bot])</li> <li>build\\(deps\\): bump jinja2 from 3.1.3 to 3.1.4 #169 (dependabot[bot])</li> <li>build\\(deps\\): bump requests from 2.31.0 to 2.32.2 #168 (dependabot[bot])</li> <li>build\\(deps\\): bump urllib3 from 1.26.18 to 1.26.19 #167 (dependabot[bot])</li> <li>build\\(deps\\): bump jinja2 from 3.0.1 to 3.1.3 #161 (dependabot[bot])</li> </ul>"},{"location":"changelog/#v631-2026-01-27","title":"v6.3.1 (2026-01-27)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v630-2025-12-26","title":"v6.3.0 (2025-12-26)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Update MCP server #255</li> </ul>"},{"location":"changelog/#v623-2025-12-24","title":"v6.2.3 (2025-12-24)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v610-2025-09-29","title":"v6.1.0 (2025-09-29)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request \u2013 Add PyHealth compatible export function #242</li> </ul>"},{"location":"changelog/#v600-2025-09-18","title":"v6.0.0 (2025-09-18)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature Request: Create a mcp-server feature #240</li> </ul>"},{"location":"changelog/#v550-2025-09-17","title":"v5.5.0 (2025-09-17)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Provide OMOP CDM sample data sets #236</li> </ul> <p>Closed issues:</p> <ul> <li>\u2728 Set up Copilot instructions #238</li> </ul>"},{"location":"changelog/#v541-2025-08-18","title":"v5.4.1 (2025-08-18)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v540-2025-08-17","title":"v5.4.0 (2025-08-17)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v530-2025-08-16","title":"v5.3.0 (2025-08-16)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v521-2025-08-16","title":"v5.2.1 (2025-08-16)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v520-2025-08-16","title":"v5.2.0 (2025-08-16)","text":"<p>Full Changelog</p>"},{"location":"changelog/#510-2025-06-22","title":"5.1.0 (2025-06-22)","text":"<p>Full Changelog</p>"},{"location":"changelog/#500-2025-06-22","title":"5.0.0 (2025-06-22)","text":"<p>Full Changelog</p>"},{"location":"changelog/#441-2025-05-29","title":"4.4.1 (2025-05-29)","text":"<p>Full Changelog</p>"},{"location":"changelog/#440-2025-05-09","title":"4.4.0 (2025-05-09)","text":"<p>Full Changelog</p>"},{"location":"changelog/#430-2024-07-11","title":"4.3.0 (2024-07-11)","text":"<p>Full Changelog</p>"},{"location":"changelog/#420-2023-11-19","title":"4.2.0 (2023-11-19)","text":"<p>Full Changelog</p>"},{"location":"changelog/#410-2023-11-19","title":"4.1.0 (2023-11-19)","text":"<p>Full Changelog</p>"},{"location":"changelog/#400-2023-11-19","title":"4.0.0 (2023-11-19)","text":"<p>Full Changelog</p>"},{"location":"changelog/#320-2023-01-20","title":"3.2.0 (2023-01-20)","text":"<p>Full Changelog</p>"},{"location":"changelog/#310-2021-09-17","title":"3.1.0 (2021-09-17)","text":"<p>Full Changelog</p>"},{"location":"changelog/#300-2020-10-23","title":"3.0.0 (2020-10-23)","text":"<p>Full Changelog</p>"},{"location":"changelog/#200-2020-06-27","title":"2.0.0 (2020-06-27)","text":"<p>Full Changelog</p>"},{"location":"changelog/#120-2020-05-23","title":"1.2.0 (2020-05-23)","text":"<p>Full Changelog</p>"},{"location":"changelog/#111-2020-05-03","title":"1.1.1 (2020-05-03)","text":"<p>Full Changelog</p>"},{"location":"changelog/#110-2020-05-03","title":"1.1.0 (2020-05-03)","text":"<p>Full Changelog</p>"},{"location":"changelog/#100-2020-05-03","title":"1.0.0 (2020-05-03)","text":"<p>Full Changelog</p>"},{"location":"changelog/#010-2020-05-03","title":"0.1.0 (2020-05-03)","text":"<p>Full Changelog</p> <p>* This Changelog was automatically generated by github_changelog_generator</p>"},{"location":"contributing/","title":"How to contribute","text":""},{"location":"contributing/#please-note","title":"Please note:","text":"<ul> <li>(Optional) We adopt Git Flow. Most feature branches are pushed to the repository and deleted when merged to develop branch.</li> <li>(Important): Submit pull requests to the develop branch or feature/ branches</li> <li>Use GitHub Issues for feature requests and bug reports. Include as much information as possible while reporting bugs.  </li> </ul>"},{"location":"contributing/#contributing-step-by-step","title":"Contributing (Step-by-step)","text":"<ol> <li> <p>Fork the repo and clone it to your local computer, and set up the upstream remote:</p> <pre><code>git clone https://github.com/YourGithubUsername/pyomop.git\ncd pyomop\ngit remote add upstream https://github.com/dermatologist/pyomop.git\n</code></pre> </li> <li> <p>Checkout out a new local branch based on your master and update it to the latest (TRUNK-123 is the branch name, You can name it whatever you want. Try to give it a meaningful name. If you are fixing an issue, please include the issue #).</p> <pre><code>git checkout -b BRANCH-123 develop\ngit clean -df\ngit pull --rebase upstream develop\n</code></pre> </li> </ol> <p>Please keep your code clean. If you find another bug, you want to fix while being in a new branch, please fix it in a separated branch instead.</p> <ol> <li> <p>Push the branch to your fork. Treat it as a backup.</p> <pre><code>git push origin BRANCH-123\n</code></pre> </li> <li> <p>Code</p> </li> <li> <p>Adhere to common conventions you see in the existing code.</p> </li> <li> <p>Include tests as much as possible, and ensure they pass.</p> </li> <li> <p>Commit to your branch</p> <pre><code> git commit -m \"BRANCH-123: Put change summary here (can be a ticket title)\"\n</code></pre> </li> </ol> <p>NEVER leave the commit message blank! Provide a detailed, clear, and complete description of your commit!</p> <ol> <li> <p>Update your branch to the latest code.</p> <pre><code>git pull --rebase upstream develop\n</code></pre> </li> <li> <p>Important If you have made many commits, please squash them into atomic units of work. (Most Git GUIs such as sourcetree and smartgit offer a squash option)</p> <pre><code>git checkout develop\ngit pull --rebase upstream develop\ngit merge --squash BRANCH-123\ngit commit -m \"fix: 123\"\n</code></pre> </li> </ol> <p>Push changes to your fork:</p> <pre><code>    git push\n</code></pre> <ol> <li>Issue a Pull Request</li> </ol> <p>In order to make a pull request:   * Click \"Pull Request\".   * Choose the develop branch   * Click 'Create pull request'   * Fill in some details about your potential patch including a meaningful title.   * Click \"Create pull request\".</p> <p>Thanks for that -- we'll get to your pull request ASAP. We love pull requests!</p>"},{"location":"contributing/#feedback","title":"Feedback","text":"<p>If you need to contact me, see my contact details on my profile page.</p>"},{"location":"github-dependents-info/","title":"Dependents stats for dermatologist/pyomop","text":""},{"location":"github-dependents-info/#package-dermatologistpyomop","title":"Package dermatologist/pyomop","text":"Repository Stars OHDSI / Data2Evidence 28 alphavector / all 5 Lyn4ever29 / pipy_server 3 <p>Generated using github-dependents-info, by Nicolas Vuillamy</p>"},{"location":"modules/","title":"Modules","text":"<p>Derivative based on the original work here: https://github.com/thehyve/omop-cdm/blob/main/src/omop_cdm/regular/cdm600/tables.py Modifications made to this file: - Removed support for schema. - Added new tables</p> <ul> <li>Licensed under the Apache License, Version 2.0 (the \"License\");</li> <li>you may not use this file except in compliance with the License.</li> <li>You may obtain a copy of the License at *</li> <li>https://www.apache.org/licenses/LICENSE-2.0 *</li> <li>Unless required by applicable law or agreed to in writing, software</li> <li>distributed under the License is distributed on an \"AS IS\" BASIS,</li> <li>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</li> <li>See the License for the specific language governing permissions and</li> <li>limitations under the License.</li> </ul> <p>Derivative based on the original work here: https://github.com/thehyve/omop-cdm/blob/main/src/omop_cdm/regular/cdm54/tables.py Modifications made to this file: - Removed support for schema. - Added new tables</p> <ul> <li>Licensed under the Apache License, Version 2.0 (the \"License\");</li> <li>you may not use this file except in compliance with the License.</li> <li>You may obtain a copy of the License at *</li> <li>https://www.apache.org/licenses/LICENSE-2.0 *</li> <li>Unless required by applicable law or agreed to in writing, software</li> <li>distributed under the License is distributed on an \"AS IS\" BASIS,</li> <li>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</li> <li>See the License for the specific language governing permissions and</li> <li>limitations under the License.</li> </ul> <p>Engine and session factory for OMOP CDM databases.</p> <p>This module provides an asynchronous SQLAlchemy engine factory with helpers to create/init CDM schemas and obtain async sessions across supported backends (SQLite, MySQL, PostgreSQL).</p> <p>LLM-oriented SQLDatabase wrapper for OMOP CDM.</p> <p>This module provides utilities for connecting LLMs to OMOP CDM databases using langchain's SQL toolkit and agents. It uses the OMOP CDM metadata from this package's SQLAlchemy models to enable LLM-powered query components to reason about available tables, columns, and foreign keys.</p> <p>This file is import-safe even when the optional LLM extras are not installed; in that case, attempting to instantiate <code>CDMDatabase</code> will raise a clear ImportError directing you to install <code>pyomop[llm]</code>.</p> <p>LLM query utilities over the OMOP CDM schema.</p> <p>This module wires langchain components to an OMOP-aware <code>CDMDatabase</code> so you can build SQL query engines that know about your CDM tables. All LLM-related imports are optional and performed lazily at runtime.</p> <p>CSV-to-OMOP loader.</p> <p>This module implements a flexible CSV loader that can populate multiple OMOP CDM tables according to a JSON mapping file. It also performs helpful cleanup operations like foreign key normalization, birthdate backfilling, gender mapping, and concept code lookups.</p> <p>Command-line interface for pyomop.</p> <p>Provides commands to create CDM tables, load vocabulary CSVs, and import FHIR Bulk Export data into an OMOP database.</p> <p>Vocabulary utilities for loading and querying OMOP vocab tables.</p> <p>Provides helpers to import vocabulary CSVs into the database and to look up concepts by id or code. Uses async SQLAlchemy sessions.</p> <p>Utilities to execute queries and convert results to DataFrames.</p> <p>Exposes a small helper class around async SQLAlchemy execution and integration with OHDSI QueryLibrary.</p> <p>Predefined OMOP SQL snippets from the OHDSI Query Library.</p> <p>This module exposes a small dictionary of named SQL queries that can be used for demos, tests, or quick analytics over a CDM instance.</p> <p>Source: https://github.com/OHDSI/QueryLibrary/tree/master/inst/shinyApps/QueryLibrary/queries</p>"},{"location":"modules/#engine_factory.CdmEngineFactory","title":"<code>CdmEngineFactory</code>","text":"<p>               Bases: <code>object</code></p> <p>Factory to create async SQLAlchemy engines and sessions for OMOP CDM.</p> <p>Supports SQLite (default), MySQL, and PostgreSQL. Exposes convenience properties for the configured engine and async session maker.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <p>Database type: \"sqlite\", \"mysql\", or \"pgsql\".</p> <code>None</code> <code>host</code> <p>Database host (ignored for SQLite).</p> <code>None</code> <code>port</code> <p>Database port (ignored for SQLite).</p> <code>None</code> <code>user</code> <p>Database user (ignored for SQLite).</p> <code>None</code> <code>pw</code> <p>Database password (ignored for SQLite).</p> <code>None</code> <code>name</code> <p>Database name or SQLite filename.</p> <code>None</code> <code>schema</code> <p>PostgreSQL schema to use for CDM.</p> <code>None</code> Source code in <code>src/pyomop/engine_factory.py</code> <pre><code>class CdmEngineFactory(object):\n    \"\"\"Factory to create async SQLAlchemy engines and sessions for OMOP CDM.\n\n    Supports SQLite (default), MySQL, and PostgreSQL. Exposes convenience\n    properties for the configured engine and async session maker.\n\n    Args:\n        db: Database type: \"sqlite\", \"mysql\", or \"pgsql\".\n        host: Database host (ignored for SQLite).\n        port: Database port (ignored for SQLite).\n        user: Database user (ignored for SQLite).\n        pw: Database password (ignored for SQLite).\n        name: Database name or SQLite filename.\n        schema: PostgreSQL schema to use for CDM.\n    \"\"\"\n\n    def __init__(\n        self,\n        db=None,\n        host=None,\n        port=None,\n        user=None,\n        pw=None,\n        name=None,\n        schema=None,\n    ):\n        import os\n\n        self._db = db or os.environ.get(\"PYOMOP_DB\", \"sqlite\")\n        self._name = name or os.environ.get(\"PYOMOP_NAME\", \"cdm.sqlite\")\n        self._host = host or os.environ.get(\"PYOMOP_HOST\", \"localhost\")\n        self._port = (\n            port if port is not None else int(os.environ.get(\"PYOMOP_PORT\", \"5432\"))\n        )\n        self._user = user or os.environ.get(\"PYOMOP_USER\", \"root\")\n        self._pw = pw or os.environ.get(\"PYOMOP_PW\", \"pass\")\n        self._schema = schema or os.environ.get(\"PYOMOP_SCHEMA\", \"\")\n        self._engine = None\n        self._base = None\n\n    async def init_models(self, metadata):\n        \"\"\"Drop and re-create all tables from provided metadata.\n\n        This is mainly used for tests and quick local setups.\n\n        Args:\n            metadata: SQLAlchemy ``MetaData`` containing table definitions.\n\n        Raises:\n            ValueError: If the engine has not been initialized.\n        \"\"\"\n        if self._engine is None:\n            raise ValueError(\"Database engine is not initialized.\")\n        async with self._engine.begin() as conn:\n            await conn.run_sync(metadata.drop_all)\n            await conn.run_sync(metadata.create_all)\n\n    @property\n    def db(self):\n        \"\"\"Return the configured database type (sqlite/mysql/pgsql).\"\"\"\n        return self._db\n\n    @property\n    def host(self):\n        \"\"\"Return the configured database host (if applicable).\"\"\"\n        return self._host\n\n    @property\n    def port(self):\n        \"\"\"Return the configured database port (if applicable).\"\"\"\n        return self._port\n\n    @property\n    def name(self):\n        \"\"\"Return the configured database name or SQLite filename.\"\"\"\n        return self._name\n\n    @property\n    def user(self):\n        \"\"\"Return the configured database user (if applicable).\"\"\"\n        return self._user\n\n    @property\n    def pw(self):\n        \"\"\"Return the configured database password (if applicable).\"\"\"\n        return self._pw\n\n    @property\n    def schema(self):\n        \"\"\"Return the configured schema (PostgreSQL).\"\"\"\n        return self._schema\n\n    @property\n    def base(self):\n        \"\"\"Return automapped classes when an engine exists, otherwise None.\"\"\"\n        if self.engine is not None:  # Not self_engine\n            Base = automap_base()\n            Base.prepare(self.engine, reflect=True)\n            return Base.classes\n        return None\n\n    @property\n    def engine(self):\n        \"\"\"Create or return the async engine for the configured backend.\n\n        Returns:\n            Async engine instance bound to the configured database.\n        \"\"\"\n        if self._engine is not None:\n            return self._engine\n        if self._db == \"sqlite\":\n            # Schemas are not supported for SQLite; warn if a non-default schema was provided\n            if self._schema and self._schema not in (\"\", \"public\"):\n                logger.warning(\n                    \"Schema is not supported for SQLite; ignoring schema='%s'\",\n                    self._schema,\n                )\n            self._engine = create_async_engine(\n                \"sqlite+aiosqlite:///\" + self._name,\n                pool_pre_ping=True,\n                pool_recycle=3600,\n            )\n        elif self._db == \"mysql\":\n            # Schemas are not supported for MySQL in the same way as PostgreSQL; warn and ignore\n            if self._schema and self._schema not in (\"\", \"public\"):\n                logger.warning(\n                    \"Schema is not supported for MySQL; ignoring schema='%s'\",\n                    self._schema,\n                )\n            mysql_url = \"mysql://{}:{}@{}:{}/{}\"\n            mysql_url = mysql_url.format(\n                self._user, self._pw, self._host, self._port, self._name\n            )\n            self._engine = create_async_engine(\n                mysql_url, isolation_level=\"READ UNCOMMITTED\"\n            )\n        elif self._db == \"pgsql\":\n            pgsql_url = \"postgresql+asyncpg://{}:{}@{}:{}/{}\"\n            pgsql_url = pgsql_url.format(\n                self._user, self._pw, self._host, self._port, self._name\n            )\n            connect_args = {}\n            # If a schema is provided, set the PostgreSQL search_path so that all\n            # operations (reflection, DDL/DML) use this schema by default.\n            if self._schema and self._schema != \"\":\n                connect_args = {\"server_settings\": {\"search_path\": self._schema}}\n            self._engine = create_async_engine(pgsql_url, connect_args=connect_args)\n        else:\n            # Unknown DB type; create no engine and warn\n            logger.warning(\"Unknown database type '%s'\u2014no engine created.\", self._db)\n            return None\n        return self._engine\n\n    @property\n    def session(self):\n        \"\"\"Return an async_sessionmaker for creating AsyncSession objects.\"\"\"\n        if self._engine is not None:\n            async_session = async_sessionmaker(\n                self._engine, expire_on_commit=False, class_=AsyncSession\n            )\n            return async_session\n        return None\n\n    @property\n    def async_session(self):\n        \"\"\"Alias for session to maintain backward compatibility.\"\"\"\n        if self._engine is not None:\n            async_session = async_sessionmaker(\n                self._engine, expire_on_commit=False, class_=AsyncSession\n            )\n            return async_session\n        return None\n\n    async def dispose(self) -&gt; None:\n        \"\"\"Dispose of the engine and close all connections.\n\n        This should be called when done with database operations to\n        ensure proper cleanup and avoid hanging.\n        \"\"\"\n        if self._engine is not None:\n            await self._engine.dispose()\n            self._engine = None\n\n    @db.setter\n    def db(self, value):\n        self._db = value\n        self._engine = None\n\n    @name.setter\n    def name(self, value):\n        self._name = value\n        self._engine = None\n\n    @port.setter\n    def port(self, value):\n        self._port = value\n        self._engine = None\n\n    @host.setter\n    def host(self, value):\n        self._host = value\n        self._engine = None\n\n    @user.setter\n    def user(self, value):\n        self._user = value\n        self._engine = None\n\n    @pw.setter\n    def pw(self, value):\n        self._pw = value\n        self._engine = None\n\n    @schema.setter\n    def schema(self, value):\n        self._schema = value\n        self._engine = None\n\n    def print_connection_info(self):\n        \"\"\"Return a string with the connection details (for logging/display).\"\"\"\n        if self._db == \"sqlite\":\n            current_directory = os.getcwd()\n            _server = os.path.join(current_directory, \"cdm.sqlite\")\n            print(\n                f\"connectionDetails &lt;- DatabaseConnector::createConnectionDetails(\\n\"\n                f'    dbms = \"sqlite\", server = \"{_server}\")\\n'\n            )\n        elif self._db == \"mysql\":\n            return f\"MySQL database '{self._name}' on {self._host}:{self._port} as user '{self._user}'\"\n        elif self._db == \"pgsql\":\n            schema_info = f\", schema '{self._schema}'\" if self._schema else \"\"\n            return f\"PostgreSQL database '{self._name}' on {self._host}:{self._port} as user '{self._user}'{schema_info}\"\n        else:\n            return \"No valid database connection configured.\"\n</code></pre>"},{"location":"modules/#engine_factory.CdmEngineFactory.async_session","title":"<code>async_session</code>  <code>property</code>","text":"<p>Alias for session to maintain backward compatibility.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.base","title":"<code>base</code>  <code>property</code>","text":"<p>Return automapped classes when an engine exists, otherwise None.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.db","title":"<code>db</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database type (sqlite/mysql/pgsql).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.engine","title":"<code>engine</code>  <code>property</code>","text":"<p>Create or return the async engine for the configured backend.</p> <p>Returns:</p> Type Description <p>Async engine instance bound to the configured database.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.host","title":"<code>host</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database host (if applicable).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.name","title":"<code>name</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database name or SQLite filename.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.port","title":"<code>port</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database port (if applicable).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.pw","title":"<code>pw</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database password (if applicable).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.schema","title":"<code>schema</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured schema (PostgreSQL).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.session","title":"<code>session</code>  <code>property</code>","text":"<p>Return an async_sessionmaker for creating AsyncSession objects.</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.user","title":"<code>user</code>  <code>property</code> <code>writable</code>","text":"<p>Return the configured database user (if applicable).</p>"},{"location":"modules/#engine_factory.CdmEngineFactory.dispose","title":"<code>dispose()</code>  <code>async</code>","text":"<p>Dispose of the engine and close all connections.</p> <p>This should be called when done with database operations to ensure proper cleanup and avoid hanging.</p> Source code in <code>src/pyomop/engine_factory.py</code> <pre><code>async def dispose(self) -&gt; None:\n    \"\"\"Dispose of the engine and close all connections.\n\n    This should be called when done with database operations to\n    ensure proper cleanup and avoid hanging.\n    \"\"\"\n    if self._engine is not None:\n        await self._engine.dispose()\n        self._engine = None\n</code></pre>"},{"location":"modules/#engine_factory.CdmEngineFactory.init_models","title":"<code>init_models(metadata)</code>  <code>async</code>","text":"<p>Drop and re-create all tables from provided metadata.</p> <p>This is mainly used for tests and quick local setups.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>SQLAlchemy <code>MetaData</code> containing table definitions.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the engine has not been initialized.</p> Source code in <code>src/pyomop/engine_factory.py</code> <pre><code>async def init_models(self, metadata):\n    \"\"\"Drop and re-create all tables from provided metadata.\n\n    This is mainly used for tests and quick local setups.\n\n    Args:\n        metadata: SQLAlchemy ``MetaData`` containing table definitions.\n\n    Raises:\n        ValueError: If the engine has not been initialized.\n    \"\"\"\n    if self._engine is None:\n        raise ValueError(\"Database engine is not initialized.\")\n    async with self._engine.begin() as conn:\n        await conn.run_sync(metadata.drop_all)\n        await conn.run_sync(metadata.create_all)\n</code></pre>"},{"location":"modules/#engine_factory.CdmEngineFactory.print_connection_info","title":"<code>print_connection_info()</code>","text":"<p>Return a string with the connection details (for logging/display).</p> Source code in <code>src/pyomop/engine_factory.py</code> <pre><code>def print_connection_info(self):\n    \"\"\"Return a string with the connection details (for logging/display).\"\"\"\n    if self._db == \"sqlite\":\n        current_directory = os.getcwd()\n        _server = os.path.join(current_directory, \"cdm.sqlite\")\n        print(\n            f\"connectionDetails &lt;- DatabaseConnector::createConnectionDetails(\\n\"\n            f'    dbms = \"sqlite\", server = \"{_server}\")\\n'\n        )\n    elif self._db == \"mysql\":\n        return f\"MySQL database '{self._name}' on {self._host}:{self._port} as user '{self._user}'\"\n    elif self._db == \"pgsql\":\n        schema_info = f\", schema '{self._schema}'\" if self._schema else \"\"\n        return f\"PostgreSQL database '{self._name}' on {self._host}:{self._port} as user '{self._user}'{schema_info}\"\n    else:\n        return \"No valid database connection configured.\"\n</code></pre>"},{"location":"modules/#llm_engine.CDMDatabase","title":"<code>CDMDatabase</code>","text":"<p>               Bases: <code>SQLDatabase</code></p> <p>OMOP-aware SQLDatabase for LLM query engines.</p> <p>This class wraps langchain's <code>SQLDatabase</code> to use the OMOP CDM SQLAlchemy metadata bundled with this package, making it easy to expose concise schema information to LLM components.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy <code>Engine</code> connected to the OMOP database.</p> required <code>schema</code> <code>str | None</code> <p>Optional database schema name.</p> <code>None</code> <code>ignore_tables</code> <code>list[str] | None</code> <p>Tables to hide from the LLM context.</p> <code>None</code> <code>include_tables</code> <code>list[str] | None</code> <p>Explicit subset of tables to expose.</p> <code>None</code> <code>sample_rows_in_table_info</code> <code>int</code> <p>Number of sample rows to include in table info.</p> <code>3</code> <code>max_string_length</code> <code>int</code> <p>Max length of generated descriptions.</p> <code>300</code> <code>version</code> <code>str</code> <p>OMOP CDM version label (\"cdm54\" or \"cdm6\").</p> <code>'cdm54'</code> Source code in <code>src/pyomop/llm_engine.py</code> <pre><code>class CDMDatabase(SQLDatabase):\n    \"\"\"OMOP-aware SQLDatabase for LLM query engines.\n\n    This class wraps langchain's ``SQLDatabase`` to use the OMOP CDM\n    SQLAlchemy metadata bundled with this package, making it easy to expose\n    concise schema information to LLM components.\n\n    Args:\n        engine: SQLAlchemy ``Engine`` connected to the OMOP database.\n        schema: Optional database schema name.\n        ignore_tables: Tables to hide from the LLM context.\n        include_tables: Explicit subset of tables to expose.\n        sample_rows_in_table_info: Number of sample rows to include in table info.\n        max_string_length: Max length of generated descriptions.\n        version: OMOP CDM version label (\"cdm54\" or \"cdm6\").\n    \"\"\"\n\n    def __init__(\n        self,\n        engine: Engine,\n        schema: str | None = None,\n        ignore_tables: list[str] | None = None,\n        include_tables: list[str] | None = None,\n        sample_rows_in_table_info: int = 3,\n        max_string_length: int = 300,\n        version: str = \"cdm54\",\n    ) -&gt; None:\n        if not _LLM_AVAILABLE:  # pragma: no cover - import-safe guard\n            raise ImportError(\"Install 'pyomop[llm]' to use LLM features.\")\n\n        # Basic configuration\n        self._engine = engine\n        self._schema = schema\n        self._max_string_length = max_string_length\n\n        if include_tables and ignore_tables:\n            raise ValueError(\"Cannot specify both include_tables and ignore_tables\")\n\n        # Load OMOP metadata for the chosen version\n        Base: Any\n        if version == \"cdm6\":\n            from .cdm6 import Base\n        else:\n            from .cdm54 import Base\n\n        metadata = cast(MetaData, Base.metadata)\n\n        # All known tables\n        self._all_tables = set(metadata.tables.keys())\n\n        # Validate include/ignore lists\n        self._include_tables = set(include_tables) if include_tables else set()\n        if self._include_tables:\n            missing = self._include_tables - self._all_tables\n            if missing:\n                raise ValueError(f\"include_tables {missing} not found in OMOP metadata\")\n\n        self._ignore_tables = set(ignore_tables) if ignore_tables else set()\n        if self._ignore_tables:\n            missing = self._ignore_tables - self._all_tables\n            if missing:\n                raise ValueError(f\"ignore_tables {missing} not found in OMOP metadata\")\n\n        if self._include_tables:\n            usable = set(self._include_tables)\n        elif self._ignore_tables:\n            usable = self._all_tables - self._ignore_tables\n        else:\n            usable = set(self._all_tables)\n        self._usable_tables = usable\n\n        if not isinstance(sample_rows_in_table_info, int):\n            raise TypeError(\"sample_rows_in_table_info must be an integer\")\n        self._sample_rows_in_table_info = sample_rows_in_table_info\n\n        self._metadata = metadata\n\n        # Initialize parent SQLDatabase\n        # Convert AsyncEngine to sync if needed (langchain requires sync engine)\n        parent_engine: Engine\n        if AsyncEngine is not None and isinstance(self._engine, AsyncEngine):\n            url_str = str(self._engine.url)\n            # Convert common async driver URLs to sync variants\n            url_str = (\n                url_str.replace(\"+aiosqlite\", \"\")\n                .replace(\"+asyncpg\", \"\")\n                .replace(\"+psycopg_async\", \"+psycopg2\")\n            )\n            parent_engine = create_engine(url_str)\n        else:\n            parent_engine = self._engine\n\n        super().__init__(\n            engine=parent_engine,\n            schema=schema,\n            include_tables=sorted(self._usable_tables) if self._usable_tables else None,\n            sample_rows_in_table_info=sample_rows_in_table_info,\n        )\n\n    # --- Helper methods for LLM context ---\n    def get_table_columns(self, table_name: str) -&gt; list[str]:\n        \"\"\"Return list of column names for a table.\n\n        This uses the OMOP SQLAlchemy ``MetaData`` instead of DB inspector.\n        \"\"\"\n        return [col.name for col in self._metadata.tables[table_name].columns]\n\n    def get_single_table_info(self, table_name: str) -&gt; str:\n        \"\"\"Return a concise description of columns and foreign keys for a table.\n\n        The format is compatible with langchain's SQL components.\n        \"\"\"\n        template = \"Table '{table_name}' has columns: {columns}, and foreign keys: {foreign_keys}.\"\n        columns: list[str] = []\n        foreign_keys: list[str] = []\n        for column in self._metadata.tables[table_name].columns:\n            columns.append(f\"{column.name} ({column.type!s})\")\n            for fk in column.foreign_keys:\n                foreign_keys.append(\n                    f\"{column.name} -&gt; {fk.column.table.name}.{fk.column.name}\"\n                )\n        column_str = \", \".join(columns)\n        fk_str = \", \".join(foreign_keys)\n        return template.format(\n            table_name=table_name, columns=column_str, foreign_keys=fk_str\n        )\n\n    def usable_tables(self) -&gt; list[str]:\n        \"\"\"Return the sorted list of tables exposed to the LLM.\n\n        This respects include/ignore settings passed at initialization.\n        \"\"\"\n        return sorted(self._usable_tables)\n\n    # Backwards compat helper name used in some code paths\n    def get_usable_table_names(self) -&gt; list[str]:  # pragma: no cover - thin wrapper\n        return self.usable_tables()\n</code></pre>"},{"location":"modules/#llm_engine.CDMDatabase.get_single_table_info","title":"<code>get_single_table_info(table_name)</code>","text":"<p>Return a concise description of columns and foreign keys for a table.</p> <p>The format is compatible with langchain's SQL components.</p> Source code in <code>src/pyomop/llm_engine.py</code> <pre><code>def get_single_table_info(self, table_name: str) -&gt; str:\n    \"\"\"Return a concise description of columns and foreign keys for a table.\n\n    The format is compatible with langchain's SQL components.\n    \"\"\"\n    template = \"Table '{table_name}' has columns: {columns}, and foreign keys: {foreign_keys}.\"\n    columns: list[str] = []\n    foreign_keys: list[str] = []\n    for column in self._metadata.tables[table_name].columns:\n        columns.append(f\"{column.name} ({column.type!s})\")\n        for fk in column.foreign_keys:\n            foreign_keys.append(\n                f\"{column.name} -&gt; {fk.column.table.name}.{fk.column.name}\"\n            )\n    column_str = \", \".join(columns)\n    fk_str = \", \".join(foreign_keys)\n    return template.format(\n        table_name=table_name, columns=column_str, foreign_keys=fk_str\n    )\n</code></pre>"},{"location":"modules/#llm_engine.CDMDatabase.get_table_columns","title":"<code>get_table_columns(table_name)</code>","text":"<p>Return list of column names for a table.</p> <p>This uses the OMOP SQLAlchemy <code>MetaData</code> instead of DB inspector.</p> Source code in <code>src/pyomop/llm_engine.py</code> <pre><code>def get_table_columns(self, table_name: str) -&gt; list[str]:\n    \"\"\"Return list of column names for a table.\n\n    This uses the OMOP SQLAlchemy ``MetaData`` instead of DB inspector.\n    \"\"\"\n    return [col.name for col in self._metadata.tables[table_name].columns]\n</code></pre>"},{"location":"modules/#llm_engine.CDMDatabase.usable_tables","title":"<code>usable_tables()</code>","text":"<p>Return the sorted list of tables exposed to the LLM.</p> <p>This respects include/ignore settings passed at initialization.</p> Source code in <code>src/pyomop/llm_engine.py</code> <pre><code>def usable_tables(self) -&gt; list[str]:\n    \"\"\"Return the sorted list of tables exposed to the LLM.\n\n    This respects include/ignore settings passed at initialization.\n    \"\"\"\n    return sorted(self._usable_tables)\n</code></pre>"},{"location":"modules/#llm_engine.SQLDatabase","title":"<code>SQLDatabase</code>","text":"<p>Minimal stub to allow import without LLM extras.</p> Source code in <code>src/pyomop/llm_engine.py</code> <pre><code>class SQLDatabase:  # type: ignore[no-redef]\n    \"\"\"Minimal stub to allow import without LLM extras.\"\"\"\n\n    pass\n</code></pre>"},{"location":"modules/#llm_query.CdmLLMQuery","title":"<code>CdmLLMQuery</code>","text":"<p>Helper that prepares an LLM-backed SQL query engine for OMOP.</p> <p>It constructs an SQL agent that can generate and execute SQL queries against OMOP CDM tables using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>sql_database</code> <code>CDMDatabase</code> <p>A <code>CDMDatabase</code> instance connected to the OMOP DB.</p> required <code>llm</code> <code>BaseLanguageModel</code> <p>A langchain LLM instance (BaseLanguageModel).</p> required <code>**kwargs</code> <code>Any</code> <p>Reserved for future expansion.</p> <code>{}</code> Source code in <code>src/pyomop/llm_query.py</code> <pre><code>class CdmLLMQuery:\n    \"\"\"Helper that prepares an LLM-backed SQL query engine for OMOP.\n\n    It constructs an SQL agent that can generate and execute SQL queries\n    against OMOP CDM tables using an LLM.\n\n    Args:\n        sql_database: A ``CDMDatabase`` instance connected to the OMOP DB.\n        llm: A langchain LLM instance (BaseLanguageModel).\n        **kwargs: Reserved for future expansion.\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: CDMDatabase,\n        llm: BaseLanguageModel,\n        **kwargs: Any,\n    ) -&gt; None:\n        self._sql_database = sql_database\n        self._llm = llm\n\n        # Create SQL toolkit and agent\n        toolkit = MyToolKit(db=sql_database, llm=llm)  # type: ignore\n        self._tools = toolkit.get_tools()\n\n        # Create SQL agent using the default agent type\n        # This is more flexible and works with various LLM types\n        # Use agent_executor_kwargs to enable error handling\n        # Disable streaming for tool-calling agents to avoid \"Tools not supported in streaming mode\" error\n        try:\n            llm.streaming = False # type: ignore\n        except:\n            # Some LLMs don't support setting streaming directly\n            pass\n        self._agent = create_sql_agent(\n            llm=llm,\n            toolkit=toolkit,\n            verbose=True,\n            agent_type=\"tool-calling\",\n            agent_executor_kwargs={\"handle_parsing_errors\": True},\n        )\n\n        # The agent itself is the query engine for langchain &gt;1.0\n        self._query_engine = self._agent\n\n    @property\n    def tools(self) -&gt; list[Any]:\n        \"\"\"List of SQL tools available in the query engine.\"\"\"\n        return self._tools\n\n    @property\n    def query_engine(self) -&gt; Any:\n        \"\"\"An SQL agent executor over the CDM tables.\"\"\"\n        return self._query_engine\n</code></pre>"},{"location":"modules/#llm_query.CdmLLMQuery.query_engine","title":"<code>query_engine</code>  <code>property</code>","text":"<p>An SQL agent executor over the CDM tables.</p>"},{"location":"modules/#llm_query.CdmLLMQuery.tools","title":"<code>tools</code>  <code>property</code>","text":"<p>List of SQL tools available in the query engine.</p>"},{"location":"modules/#llm_query.MyToolKit","title":"<code>MyToolKit</code>","text":"<p>               Bases: <code>SQLDatabaseToolkit</code></p> <p>Custom toolkit that includes the example query tool.</p> Source code in <code>src/pyomop/llm_query.py</code> <pre><code>class MyToolKit(SQLDatabaseToolkit):\n    \"\"\"Custom toolkit that includes the example query tool.\"\"\"\n\n    def __init__(self, db: CDMDatabase, llm: BaseLanguageModel) -&gt; None:\n        super().__init__(db=db, llm=llm)  # type: ignore\n\n    def get_tools(self) -&gt; list[Any]:\n        \"\"\"Get the list of tools including the example query tool.\"\"\"\n        tools = super().get_tools()\n        tools.append(example_query_tool)\n        return tools\n</code></pre>"},{"location":"modules/#llm_query.MyToolKit.get_tools","title":"<code>get_tools()</code>","text":"<p>Get the list of tools including the example query tool.</p> Source code in <code>src/pyomop/llm_query.py</code> <pre><code>def get_tools(self) -&gt; list[Any]:\n    \"\"\"Get the list of tools including the example query tool.\"\"\"\n    tools = super().get_tools()\n    tools.append(example_query_tool)\n    return tools\n</code></pre>"},{"location":"modules/#llm_query.example_query_tool","title":"<code>example_query_tool(table_name)</code>","text":"<p>Generate a couple of example queries for the given table name. This will help you understand how to formulate queries for the OMOP CDM. Use this when sql_db_query_checker tool flags an invalid query. Args:     table_name: The name of the OMOP CDM table from \"person\", \"condition_occurrence\", \"condition_era\", \"drug_exposure\", \"drug_era\", \"observation\". Returns:     A string with example queries for the table.</p> Source code in <code>src/pyomop/llm_query.py</code> <pre><code>@tool\ndef example_query_tool(table_name: str) -&gt; str:\n    \"\"\"\n    Generate a couple of example queries for the given table name.\n    This will help you understand how to formulate queries for the OMOP CDM.\n    Use this when sql_db_query_checker tool flags an invalid query.\n    Args:\n        table_name: The name of the OMOP CDM table from \"person\", \"condition_occurrence\", \"condition_era\", \"drug_exposure\", \"drug_era\", \"observation\".\n    Returns:\n        A string with example queries for the table.\n    \"\"\"\n    example = \"\"\n    try:\n        if table_name.lower() == \"person\":\n            # read the following markdown file from the website and return its content\n            example = requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/person/PE02.md\"\n            ).text\n            example += \"\\n\"\n            example += requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/person/PE03.md\"\n            ).text\n        elif table_name.lower() == \"condition_occurrence\":\n            example = requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/condition_occurrence/CO01.md\"\n            ).text\n            example += \"\\n\"\n            example += requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/condition_occurrence/CO05.md\"\n            ).text\n        elif table_name.lower() == \"condition_era\":\n            example = requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/condition_era/CE01.md\"\n            ).text\n            example += \"\\n\"\n            example += requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/condition_era/CE02.md\"\n            ).text\n        elif table_name.lower() == \"drug_exposure\":\n            example = requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/drug_exposure/DEX01.md\"\n            ).text\n            example += \"\\n\"\n            example += requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/drug_exposure/DEX02.md\"\n            ).text\n        elif table_name.lower() == \"drug_era\":\n            example = requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/drug_era/DER01.md\"\n            ).text\n            example += \"\\n\"\n            example += requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/drug_era/DER04.md\"\n            ).text\n        elif table_name.lower() == \"observation\":\n            example = requests.get(\n                \"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/observation/O01.md\"\n            ).text\n    except Exception as e:\n        logger.warning(f\"Error fetching example queries for {table_name}: {e}\")\n    logger.info(f\"Dynamic prompt tool called for table: {table_name}\")\n    logger.info(\n        f\"Example returned: {example[:200] if example else '(empty)'}...\"\n    )  # Log first 200 characters\n    return example\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader","title":"<code>CdmCsvLoader</code>","text":"<p>Load a single CSV into multiple OMOP CDM tables using a JSON mapping file.</p> <p>Mapping file format (JSON):</p> <p>{   \"csv_key\": \"patient_id\",            # optional, CSV column that contains the patient/person identifier   \"tables\": [     {       \"name\": \"cohort\",              # target table name as in the database       \"filters\": [                     # optional row filters applied to CSV before mapping         {\"column\": \"resourceType\", \"equals\": \"Encounter\"}       ],       \"columns\": {                     # mapping of target_table_column -&gt; value         \"cohort_definition_id\": {\"const\": 1},              # constant value         \"subject_id\": \"patient_id\",                         # copy from CSV column         \"cohort_start_date\": \"period.start\",                # copy from CSV column         \"cohort_end_date\": \"period.end\"                     # copy from CSV column       }     }   ] }</p> Notes <ul> <li>Constants are provided via {\"const\": value}.</li> <li>If a required column is missing from mapping, it's left as None (DB default or nullable required).</li> <li>Primary keys that are Integer types will autoincrement where supported (SQLite/PostgreSQL typical behavior).</li> <li>Dates/times are converted to proper Python types where possible based on reflected column types.</li> </ul> Source code in <code>src/pyomop/loader.py</code> <pre><code>class CdmCsvLoader:\n    \"\"\"\n    Load a single CSV into multiple OMOP CDM tables using a JSON mapping file.\n\n    Mapping file format (JSON):\n\n    {\n      \"csv_key\": \"patient_id\",            # optional, CSV column that contains the patient/person identifier\n      \"tables\": [\n        {\n          \"name\": \"cohort\",              # target table name as in the database\n          \"filters\": [                     # optional row filters applied to CSV before mapping\n            {\"column\": \"resourceType\", \"equals\": \"Encounter\"}\n          ],\n          \"columns\": {                     # mapping of target_table_column -&gt; value\n            \"cohort_definition_id\": {\"const\": 1},              # constant value\n            \"subject_id\": \"patient_id\",                         # copy from CSV column\n            \"cohort_start_date\": \"period.start\",                # copy from CSV column\n            \"cohort_end_date\": \"period.end\"                     # copy from CSV column\n          }\n        }\n      ]\n    }\n\n    Notes:\n      - Constants are provided via {\"const\": value}.\n      - If a required column is missing from mapping, it's left as None (DB default or nullable required).\n      - Primary keys that are Integer types will autoincrement where supported (SQLite/PostgreSQL typical behavior).\n      - Dates/times are converted to proper Python types where possible based on reflected column types.\n    \"\"\"\n\n    def __init__(self, cdm_engine_factory, version: str = \"cdm54\") -&gt; None:\n        \"\"\"Create a loader bound to a specific database engine.\n\n        Args:\n            cdm_engine_factory: An initialized ``CdmEngineFactory``.\n            version: OMOP CDM version label (\"cdm54\" or \"cdm6\").\n        \"\"\"\n        self._cdm = cdm_engine_factory\n        self._engine = cdm_engine_factory.engine\n        self._maker = async_sessionmaker(self._engine, class_=AsyncSession)\n        self._scope = async_scoped_session(self._maker, scopefunc=asyncio.current_task)\n        self._version = version\n\n    @asynccontextmanager\n    async def _get_session(self) -&gt; AsyncGenerator[AsyncSession, None]:\n        \"\"\"Yield a scoped async session bound to the engine.\"\"\"\n        async with self._scope() as session:\n            yield session\n\n    async def _prepare_automap(self, conn: AsyncConnection) -&gt; AutomapBase:\n        \"\"\"Reflect the database and return an automapped base.\"\"\"\n        automap: AutomapBase = automap_base()\n\n        def _prepare(sync_conn):\n            automap.prepare(autoload_with=sync_conn)\n\n        await conn.run_sync(_prepare)\n        return automap\n\n    def _coerce_record_to_table_types(\n        self,\n        table,\n        rec: Dict[str, Any],\n        force_text_fields: Optional[Set[str]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Coerce a record's values to the SQL types defined by the target OMOP table.\n\n        Rules:\n        - Strings/Text: cast to str; lists/tuples joined by comma; dicts JSON-serialized; enforce max length if defined.\n        - Integers/Numerics: tolerant numeric parsing; None if unparsable.\n        - Dates/DateTimes: parsed via pandas; DateTime normalized to UTC-naive.\n        - Forced TEXT fields: certain columns are always stringified (e.g., codes arrays). The list comes\n          from mapping[\"force_text_fields\"].\n\n        This is applied just before insert to ensure DB type compatibility.\n        \"\"\"\n        # Columns that should always be treated as TEXT regardless of inferred type\n        force_text: Set[str] = set(force_text_fields or [])\n\n        # Local helper: stringify numbers without trailing .0 (e.g., 1.0 -&gt; \"1\")\n        def _s(v: Any) -&gt; str:\n            try:\n                if isinstance(v, float):\n                    return str(int(v)) if v.is_integer() else str(v)\n                if isinstance(v, Decimal):\n                    if v == v.to_integral_value():\n                        return str(int(v))\n                    # Normalize to drop insignificant trailing zeros\n                    return format(v.normalize(), \"f\")\n                return \"\" if v is None else str(v)\n            except Exception:\n                return str(v)\n\n        for col in table.columns:\n            name = col.name\n            if name not in rec:\n                continue\n            val = rec[name]\n            if val is None:\n                continue\n\n            t = col.type\n\n            # Force certain fields to TEXT\n            if name in force_text:\n                if isinstance(val, (list, tuple)):\n                    sval = \",\".join([_s(v) for v in val])\n                elif isinstance(val, dict):\n                    try:\n                        import json as _json\n\n                        sval = _json.dumps(val, ensure_ascii=False)\n                    except Exception:\n                        sval = _s(val)\n                else:\n                    sval = _s(val)\n                max_len = getattr(t, \"length\", None)\n                rec[name] = sval[: max_len or 255]\n                continue\n\n            # Type-driven coercion\n            try:\n                from pandas import to_datetime as _to_datetime\n                from pandas import to_numeric as _to_numeric\n\n                if isinstance(t, Date):\n                    dt = _to_datetime(val, errors=\"coerce\")\n                    rec[name] = None if pd.isna(dt) else dt.date()\n                elif isinstance(t, DateTime):\n                    ts = _to_datetime(val, errors=\"coerce\", utc=True)\n                    if pd.isna(ts):\n                        rec[name] = None\n                    else:\n                        py = ts.to_pydatetime()\n                        rec[name] = py.astimezone(timezone.utc).replace(tzinfo=None)\n                elif isinstance(t, (Integer, BigInteger)):\n                    num = _to_numeric(val, errors=\"coerce\")\n                    rec[name] = None if pd.isna(num) else int(num)\n                elif isinstance(t, Numeric):\n                    num = _to_numeric(val, errors=\"coerce\")\n                    rec[name] = None if pd.isna(num) else Decimal(str(num))\n                elif isinstance(t, (String, Text)):\n                    if isinstance(val, (list, tuple)):\n                        sval = \",\".join([_s(v) for v in val])\n                    elif isinstance(val, dict):\n                        try:\n                            import json as _json\n\n                            sval = _json.dumps(val, ensure_ascii=False)\n                        except Exception:\n                            sval = _s(val)\n                    else:\n                        sval = _s(val)\n                    max_len = getattr(t, \"length\", None)\n                    rec[name] = sval[: max_len or 255]\n                else:\n                    # Leave other types as-is\n                    pass\n            except Exception:\n                # Last resort, stringify\n                try:\n                    s = _s(val)\n                    max_len = getattr(getattr(col, \"type\", None), \"length\", None)\n                    rec[name] = s[: max_len or 255]\n                except Exception:\n                    rec[name] = val\n\n        return rec\n\n    async def _list_tables_with_person_id(self, conn: AsyncConnection) -&gt; List[str]:\n        \"\"\"Return table names (in default schema) that contain a person_id column, excluding 'person'.\"\"\"\n\n        def _inner(sync_conn):\n            from sqlalchemy import inspect as _inspect\n\n            insp = _inspect(sync_conn)\n            tables = insp.get_table_names()\n            result: List[str] = []\n            for t in tables:\n                try:\n                    cols = insp.get_columns(t)\n                except Exception:\n                    continue\n                if any(c.get(\"name\") == \"person_id\" for c in cols) and t != \"person\":\n                    result.append(t)\n            return result\n\n        return await conn.run_sync(_inner)\n\n    async def _add_person_id_text_columns(self, session: AsyncSession) -&gt; None:\n        \"\"\"Add a temporary person_id_text TEXT column to all non-person tables that have person_id.\n\n        This avoids fragile type-alter and FK juggling. We'll populate this column\n        when incoming person identifiers are not numeric, then resolve to integer\n        person_id in step 2 and drop the column.\n        \"\"\"\n        conn = await session.connection()\n        tables = await self._list_tables_with_person_id(conn)\n        dialect = self._engine.dialect.name\n        for t in tables:\n            if dialect == \"postgresql\":\n                ddl = f'ALTER TABLE \"{t}\" ADD COLUMN IF NOT EXISTS person_id_text TEXT'\n            else:\n                # SQLite (and others): try without IF NOT EXISTS\n                ddl = f'ALTER TABLE \"{t}\" ADD COLUMN person_id_text TEXT'\n            try:\n                await session.execute(text(ddl))\n            except Exception:\n                # Column may already exist or backend may not support IF NOT EXISTS; ignore\n                pass\n\n    async def _drop_person_id_text_columns(self, session: AsyncSession) -&gt; None:\n        \"\"\"Drop the temporary person_id_text column from all non-person tables.\"\"\"\n        conn = await session.connection()\n        tables = await self._list_tables_with_person_id(conn)\n        dialect = self._engine.dialect.name\n        for t in tables:\n            if dialect == \"postgresql\":\n                ddl = f'ALTER TABLE \"{t}\" DROP COLUMN IF EXISTS person_id_text'\n            else:\n                ddl = f'ALTER TABLE \"{t}\" DROP COLUMN person_id_text'\n            try:\n                await session.execute(text(ddl))\n            except Exception:\n                # Some backends or versions (older SQLite) may not support DROP COLUMN; ignore\n                pass\n\n    def _load_mapping(self, mapping_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Load a JSON mapping file from disk.\"\"\"\n        with open(mapping_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    def _apply_filters(self, df: pd.DataFrame, filters: Optional[List[Dict[str, Any]]]):\n        \"\"\"Apply optional row filters to a DataFrame prior to mapping.\"\"\"\n        if not filters:\n            return df\n        mask = pd.Series([True] * len(df), index=df.index)\n        for flt in filters:\n            col = flt.get(\"column\")\n            if col is None or col not in df.columns:\n                continue\n            if \"equals\" in flt:\n                mask &amp;= (df[col] == flt[\"equals\"]) | (\n                    df[col].astype(str) == str(flt[\"equals\"])\n                )\n            elif \"not_empty\" in flt and flt[\"not_empty\"]:\n                mask &amp;= df[col].notna() &amp; (df[col].astype(str).str.len() &gt; 0)\n        result = df.loc[mask, :].copy()\n        return result\n\n    def _convert_value(self, sa_type: Any, value: Any) -&gt; Any:\n        \"\"\"Coerce a CSV value into an appropriate Python type for insert.\n\n        The conversion is guided by the SQLAlchemy column type.\n        \"\"\"\n        if pd.isna(value) or value == \"\":\n            return None\n        try:\n            if isinstance(sa_type, Date):\n                # Accept many input formats\n                dt = pd.to_datetime(value, errors=\"coerce\")\n                return None if pd.isna(dt) else dt.date()\n            if isinstance(sa_type, DateTime):\n                # Normalize to UTC-naive for Postgres compatibility\n                ts = pd.to_datetime(value, errors=\"coerce\", utc=True)\n                if pd.isna(ts):\n                    return None\n                py = ts.to_pydatetime()\n                if getattr(py, \"tzinfo\", None) is not None:\n                    py = py.astimezone(timezone.utc).replace(tzinfo=None)\n                return py\n            if isinstance(sa_type, (Integer, BigInteger)):\n                return int(value)\n            if isinstance(sa_type, Numeric):\n                return Decimal(str(value))\n        except Exception:\n            return value\n        # Trim string values to 50 characters before insert\n        if isinstance(value, str):\n            return value[:50]\n        return value\n\n    async def load(\n        self, csv_path: str, mapping_path: str | None = None, chunk_size: int = 1000\n    ) -&gt; None:\n        \"\"\"Load a CSV into multiple OMOP tables based on a mapping file.\n\n        Args:\n            csv_path: Path to the input CSV file.\n            mapping_path: Path to the JSON mapping file. Defaults to the\n                package's ``mapping.default.json`` when not provided.\n            chunk_size: Batch size for INSERT statements.\n        \"\"\"\n        # If mapping path is None, load mapping.default.json from the current directory\n        # Step 1: Load CSV and mapping\n        start_time = time.time()\n        logger.info(f\"Loading CSV data from {csv_path}\")\n        if mapping_path is None:\n            mapping_path = str(Path(__file__).parent / \"mapping.default.json\")\n        mapping = self._load_mapping(mapping_path)\n        # Use low_memory=False to avoid DtypeWarning for mixed-type columns\n        df = pd.read_csv(csv_path, low_memory=False)\n\n        async with self._get_session() as session:\n            # Relax constraint enforcement during bulk load on Postgres\n            is_pg = False\n            try:\n                is_pg = str(self._engine.dialect.name).startswith(\"postgres\")\n            except Exception:\n                is_pg = False\n            if is_pg:\n                try:\n                    await session.execute(\n                        text(\"SET session_replication_role = replica\")\n                    )\n                except Exception:\n                    pass\n\n            try:\n                conn = await session.connection()\n                # Before reflecting, add a temporary person_id_text column to accept non-numeric IDs\n                await self._add_person_id_text_columns(session)\n                automap = await self._prepare_automap(conn)\n\n                for tbl in mapping.get(\"tables\", []):\n                    table_name = tbl.get(\"name\")\n                    if not table_name:\n                        continue\n                    # obtain mapped class\n                    try:\n                        mapper = getattr(automap.classes, table_name)\n                    except AttributeError:\n                        raise ValueError(f\"Table '{table_name}' not found in database.\")\n\n                    # compute filtered dataframe\n                    df_tbl = self._apply_filters(df, tbl.get(\"filters\"))\n                    if df_tbl.empty:\n                        continue\n\n                    col_map: Dict[str, Any] = tbl.get(\"columns\", {})\n                    # Gather target SQLA column metadata\n                    sa_cols = {c.name: c.type for c in mapper.__table__.columns}\n                    sa_col_objs = {c.name: c for c in mapper.__table__.columns}\n\n                    # Build records\n                    records: List[Dict[str, Any]] = []\n                    for _, row in df_tbl.iterrows():\n                        rec: Dict[str, Any] = {}\n                        for target_col, src in col_map.items():\n                            if isinstance(src, dict) and \"const\" in src:\n                                value = src[\"const\"]\n                            elif isinstance(src, str):\n                                value = row.get(src)\n                            else:\n                                value = None\n\n                            # IMPORTANT: keep person_id raw for staging logic below\n                            if target_col != \"person_id\":\n                                # Convert based on SA type if available\n                                sa_t = sa_cols.get(target_col)\n                                if sa_t is not None:\n                                    value = self._convert_value(sa_t, value)\n                            rec[target_col] = value\n\n                        # If person_id exists in the record, route non-numeric values into person_id_text\n                        if \"person_id\" in rec:\n                            pid = rec.get(\"person_id\")\n                            if pid is None:\n                                # If NOT NULL, use placeholder to avoid constraint errors\n                                col = sa_col_objs.get(\"person_id\")\n                                if col is not None and not getattr(\n                                    col, \"nullable\", True\n                                ):\n                                    rec[\"person_id\"] = 0\n                            elif isinstance(pid, int):\n                                pass\n                            elif isinstance(pid, str) and pid.strip().isdigit():\n                                try:\n                                    rec[\"person_id\"] = int(pid.strip())\n                                except Exception:\n                                    # If conversion unexpectedly fails, send to text column\n                                    if \"person_id_text\" in sa_cols:\n                                        rec[\"person_id_text\"] = str(pid)\n                                    # Respect NOT NULL with placeholder when required\n                                    col = sa_col_objs.get(\"person_id\")\n                                    if col is not None and not getattr(\n                                        col, \"nullable\", True\n                                    ):\n                                        rec[\"person_id\"] = 0\n                                    else:\n                                        rec[\"person_id\"] = None\n                            else:\n                                # Non-numeric content: place into person_id_text if available\n                                if \"person_id_text\" in sa_cols:\n                                    rec[\"person_id_text\"] = str(pid)\n                                # Respect NOT NULL with placeholder when required\n                                col = sa_col_objs.get(\"person_id\")\n                                if col is not None and not getattr(\n                                    col, \"nullable\", True\n                                ):\n                                    rec[\"person_id\"] = 0\n                                else:\n                                    rec[\"person_id\"] = None\n                        # Finally coerce all fields to the table's schema (string lengths, forced TEXT, datetimes)\n                        rec = self._coerce_record_to_table_types(\n                            mapper.__table__,\n                            rec,\n                            set(mapping.get(\"force_text_fields\", [])),\n                        )\n                        records.append(rec)\n\n                    if not records:\n                        continue\n\n                    stmt = insert(mapper)\n                    # Chunked insert\n                    for i in range(0, len(records), chunk_size):\n                        batch = records[i : i + chunk_size]\n                        await session.execute(stmt, batch)\n                # Step 1 complete: all data loaded into target tables\n                elapsed = time.time() - start_time\n                logger.info(f\"Data loaded into target tables (elapsed: {elapsed:.2f}s)\")\n                step_time = time.time()\n\n                # Step 2: Normalize person_id FKs using person.person_id (not person_source_value)\n                logger.info(\"Normalizing person_id foreign keys\")\n                await self.fix_person_id(session, automap)\n\n                # Drop the temporary person_id_text columns now that person_id has been normalized\n                await self._drop_person_id_text_columns(session)\n                elapsed = time.time() - step_time\n                logger.info(\n                    f\"Person_id foreign keys normalized (elapsed: {elapsed:.2f}s)\"\n                )\n                step_time = time.time()\n\n                # Step 3: Backfill year/month/day of birth from birth_datetime where missing or zero\n                logger.info(\"Backfilling person birth fields\")\n                await self.backfill_person_birth_fields(session, automap)\n                elapsed = time.time() - step_time\n                logger.info(f\"Person birth fields backfilled (elapsed: {elapsed:.2f}s)\")\n                step_time = time.time()\n\n                # Step 4: Set gender_concept_id from gender_source_value using standard IDs\n                logger.info(\"Setting person.gender_concept_id from gender_source_value\")\n                await self.update_person_gender_concept_id(session, automap)\n                elapsed = time.time() - step_time\n                logger.info(f\"Person gender_concept_id set (elapsed: {elapsed:.2f}s)\")\n                step_time = time.time()\n\n                # Step 5: Apply concept mappings defined in the JSON mapping\n                logger.info(\"Applying concept mappings\")\n                await self.apply_concept_mappings(session, automap, mapping)\n                elapsed = time.time() - step_time\n                logger.info(f\"Concept mappings applied (elapsed: {elapsed:.2f}s)\")\n\n                await session.commit()\n            finally:\n                if is_pg:\n                    try:\n                        await session.execute(\n                            text(\"SET session_replication_role = origin\")\n                        )\n                    except Exception:\n                        pass\n                await session.close()\n\n    async def fix_person_id(self, session: AsyncSession, automap: AutomapBase) -&gt; None:\n        \"\"\"\n        Update all tables so that person_id foreign keys store the canonical\n        person.person_id (integer), replacing any rows where person_id currently\n        contains the person_source_value (string/UUID).\n\n        Approach:\n        - Build a mapping from person_source_value -&gt; person_id from the person table.\n        - For each table (except person) having a person_id column, run updates:\n                    SET person_id = person.person_id WHERE CAST(person_id AS TEXT) = person_source_value.\n                - This is safe for SQLite (used in examples). For stricter RDBMS, ensure types\n                    are compatible or adjust as needed.\n        \"\"\"\n        # Resolve person table from automap\n        try:\n            person_cls = getattr(automap.classes, \"person\")\n        except AttributeError:\n            return  # No person table; nothing to do\n\n        person_table = person_cls.__table__\n\n        # Build mapping of person_source_value -&gt; person_id\n        res = await session.execute(\n            select(person_table.c.person_source_value, person_table.c.person_id).where(\n                person_table.c.person_source_value.isnot(None)\n            )\n        )\n        pairs = res.fetchall()\n        if not pairs:\n            return\n\n        psv_to_id: Dict[str, int] = {}\n        for psv, pid in pairs:\n            if psv is None or pid is None:\n                continue\n            psv_to_id[str(psv)] = int(pid)\n\n        if not psv_to_id:\n            return\n\n        # Iterate all tables and update person_id where it matches a known person_source_value\n        # Also handle rows that staged a non-numeric value in person_id_text.\n        # Avoid metadata.sorted_tables to prevent SAWarning about unresolvable cycles in vocab tables.\n        for tbl_name, table in automap.metadata.tables.items():\n            if tbl_name == person_table.name:\n                continue\n            if \"person_id\" not in table.c:\n                continue\n\n            # If person_id_text exists, prefer it for matching\n            has_text = \"person_id_text\" in table.c\n\n            # Run per-psv updates; small and explicit for clarity\n            for psv, pid in psv_to_id.items():\n                if has_text:\n                    # Match on person_id_text\n                    stmt = (\n                        update(table)\n                        .where(table.c.person_id_text == psv)\n                        .values(person_id=pid)\n                    )\n                    await session.execute(stmt)\n                # Also try matching where person_id was staged as string\n                stmt2 = (\n                    update(table)\n                    .where(cast(table.c.person_id, String()) == psv)\n                    .values(person_id=pid)\n                )\n                await session.execute(stmt2)\n\n            # Clear person_id_text after normalization when column exists\n            if has_text:\n                try:\n                    await session.execute(\n                        update(table)\n                        .where(table.c.person_id_text.isnot(None))\n                        .values(person_id_text=None)\n                    )\n                except Exception:\n                    pass\n\n    async def update_person_gender_concept_id(\n        self, session: AsyncSession, automap: AutomapBase\n    ) -&gt; None:\n        \"\"\"\n            Update person.gender_concept_id from person.gender_source_value using static mapping:\n            - male (or 'm')   -&gt; 8507\n            - female (or 'f') -&gt; 8532\n            - anything else   -&gt; 0 (unknown)\n\n        Only updates rows where the computed value differs from the current value\n        or where gender_concept_id is NULL.\n        \"\"\"\n        try:\n            person_cls = getattr(automap.classes, \"person\")\n        except AttributeError:\n            return\n\n        person_table = person_cls.__table__\n\n        # Fetch rows to evaluate. We consider all rows with a non-null gender_source_value\n        res = await session.execute(\n            select(\n                person_table.c.person_id,\n                person_table.c.gender_source_value,\n                person_table.c.gender_concept_id,\n            ).where(person_table.c.gender_source_value.isnot(None))\n        )\n\n        rows = res.fetchall()\n        if not rows:\n            return\n\n        def map_gender(val: str | None) -&gt; int:\n            if val is None:\n                return 0\n            s = str(val).strip().lower()\n            if s in {\"male\", \"m\"}:\n                return 8507\n            if s in {\"female\", \"f\"}:\n                return 8532\n            return 0\n\n        for pid, gsrc, gcid in rows:\n            target = map_gender(gsrc)\n            # Skip if already correct\n            if gcid == target:\n                continue\n            stmt = (\n                update(person_table)\n                .where(person_table.c.person_id == pid)\n                .values(gender_concept_id=target)\n            )\n            await session.execute(stmt)\n\n    async def backfill_person_birth_fields(\n        self, session: AsyncSession, automap: AutomapBase\n    ) -&gt; None:\n        \"\"\"\n            In the person table, replace 0 or NULL values in year_of_birth, month_of_birth,\n            and day_of_birth with values derived from birth_datetime.\n\n        This runs in Python for portability across backends.\n        \"\"\"\n        # Resolve person table from automap\n        try:\n            person_cls = getattr(automap.classes, \"person\")\n        except AttributeError:\n            return\n\n        person_table = person_cls.__table__\n\n        # Fetch necessary columns\n        res = await session.execute(\n            select(\n                person_table.c.person_id,\n                person_table.c.birth_datetime,\n                person_table.c.year_of_birth,\n                person_table.c.month_of_birth,\n                person_table.c.day_of_birth,\n            ).where(person_table.c.birth_datetime.isnot(None))\n        )\n\n        rows = res.fetchall()\n        if not rows:\n            return\n\n        for pid, birth_dt, y, m, d in rows:\n            # Parse birth_dt to a datetime if needed\n            bd: Optional[datetime]\n            if isinstance(birth_dt, datetime):\n                # Normalize timezone-aware to UTC-naive\n                if birth_dt.tzinfo is not None:\n                    bd = birth_dt.astimezone(timezone.utc).replace(tzinfo=None)\n                else:\n                    bd = birth_dt\n            elif isinstance(birth_dt, date):\n                bd = datetime(birth_dt.year, birth_dt.month, birth_dt.day)\n            else:\n                try:\n                    tmp = pd.to_datetime(birth_dt, errors=\"coerce\", utc=True)\n                    if pd.isna(tmp):\n                        bd = None\n                    else:\n                        py = tmp.to_pydatetime()\n                        bd = py.astimezone(timezone.utc).replace(tzinfo=None)\n                except Exception:\n                    bd = None\n\n            if bd is None:\n                continue\n\n            new_y = y if (y is not None and int(y or 0) != 0) else bd.year\n            new_m = m if (m is not None and int(m or 0) != 0) else bd.month\n            new_d = d if (d is not None and int(d or 0) != 0) else bd.day\n\n            # Only update when something changes\n            if new_y != y or new_m != m or new_d != d:\n                stmt = (\n                    update(person_table)\n                    .where(person_table.c.person_id == pid)\n                    .values(\n                        year_of_birth=new_y,\n                        month_of_birth=new_m,\n                        day_of_birth=new_d,\n                    )\n                )\n                await session.execute(stmt)\n\n    async def apply_concept_mappings(\n        self,\n        session: AsyncSession,\n        automap: AutomapBase,\n        mapping: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n            Based on the \"concept\" key in the mapping JSON, populate target *_concept_id columns\n            by looking up concept.concept_id using codes found in the specified source column.\n\n            Rules:\n        - If the source value is a comma-separated string, use only the first element for lookup.\n        - Find by equality on concept.concept_code.\n        - Update the target column with the matching concept.concept_id.\n        \"\"\"\n        if not mapping or \"concept\" not in mapping:\n            return\n\n        # Resolve concept table\n        try:\n            concept_cls = getattr(automap.classes, \"concept\")\n        except AttributeError:\n            return\n\n        concept_table = concept_cls.__table__\n\n        # Simple in-memory code to cid mapping\n        code_to_cid: Dict[str, Optional[int]] = {}\n\n        async def lookup_concept_id(code: str) -&gt; Optional[int]:\n            if code in code_to_cid:\n                return code_to_cid[code]\n            res = await session.execute(\n                select(concept_table.c.concept_id).where(\n                    concept_table.c.concept_code == code\n                )\n            )\n            row = res.first()\n            cid = int(row[0]) if row and row[0] is not None else None\n            code_to_cid[code] = cid\n            return cid\n\n        for item in mapping.get(\"concept\", []):\n            table_name = item.get(\"table\")\n            if not table_name:\n                continue\n            try:\n                mapper = getattr(automap.classes, table_name)\n            except AttributeError:\n                # Target table not found; skip\n                continue\n\n            table = mapper.__table__\n            pk_cols = list(table.primary_key.columns)\n            if not pk_cols:\n                # Cannot safely update without a primary key\n                continue\n\n            for m in item.get(\"mappings\", []):\n                source_col = m.get(\"source\")\n                target_col = m.get(\"target\")\n                if not source_col or not target_col:\n                    continue\n                if source_col not in table.c or target_col not in table.c:\n                    continue\n\n                # Fetch candidate rows: target is NULL or 0, and source is not NULL/empty\n                res = await session.execute(\n                    select(\n                        *pk_cols,\n                        table.c[source_col].label(\"_src\"),\n                        table.c[target_col].label(\"_tgt\"),\n                    ).where(\n                        or_(\n                            table.c[target_col].is_(None),\n                            table.c[target_col] == 0,\n                        ),\n                        table.c[source_col].isnot(None),\n                    )\n                )\n\n                rows = res.fetchall()\n                if not rows:\n                    continue\n\n                for row in rows:\n                    # row is a tuple: (*pk_vals, _src, _tgt)\n                    pk_vals = row[: len(pk_cols)]\n                    src_val = row[len(pk_cols)]\n\n                    # Only care about non-empty strings; if comma-separated, take first element\n                    code: Optional[str] = None\n                    if isinstance(src_val, str):\n                        # Split on comma and strip whitespace\n                        first = src_val.split(\",\")[0].strip()\n                        code = first if first else None\n                    elif isinstance(src_val, list) and src_val:\n                        # If a list somehow made it into the DB, use first element's string\n                        code = str(src_val[0])\n                    else:\n                        # Fallback to simple string conversion if it's a scalar\n                        code = str(src_val) if src_val is not None else None\n\n                    if not code:\n                        continue\n\n                    cid = await lookup_concept_id(code)\n                    if cid is None:\n                        continue\n\n                    # Build WHERE with PK columns\n                    where_clause = and_(\n                        *[\n                            (pk_col == pk_val)\n                            for pk_col, pk_val in zip(pk_cols, pk_vals)\n                        ]\n                    )\n\n                    stmt = update(table).where(where_clause).values({target_col: cid})\n                    await session.execute(stmt)\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.__init__","title":"<code>__init__(cdm_engine_factory, version='cdm54')</code>","text":"<p>Create a loader bound to a specific database engine.</p> <p>Parameters:</p> Name Type Description Default <code>cdm_engine_factory</code> <p>An initialized <code>CdmEngineFactory</code>.</p> required <code>version</code> <code>str</code> <p>OMOP CDM version label (\"cdm54\" or \"cdm6\").</p> <code>'cdm54'</code> Source code in <code>src/pyomop/loader.py</code> <pre><code>def __init__(self, cdm_engine_factory, version: str = \"cdm54\") -&gt; None:\n    \"\"\"Create a loader bound to a specific database engine.\n\n    Args:\n        cdm_engine_factory: An initialized ``CdmEngineFactory``.\n        version: OMOP CDM version label (\"cdm54\" or \"cdm6\").\n    \"\"\"\n    self._cdm = cdm_engine_factory\n    self._engine = cdm_engine_factory.engine\n    self._maker = async_sessionmaker(self._engine, class_=AsyncSession)\n    self._scope = async_scoped_session(self._maker, scopefunc=asyncio.current_task)\n    self._version = version\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.apply_concept_mappings","title":"<code>apply_concept_mappings(session, automap, mapping)</code>  <code>async</code>","text":"<pre><code>Based on the \"concept\" key in the mapping JSON, populate target *_concept_id columns\nby looking up concept.concept_id using codes found in the specified source column.\n\nRules:\n</code></pre> <ul> <li>If the source value is a comma-separated string, use only the first element for lookup.</li> <li>Find by equality on concept.concept_code.</li> <li>Update the target column with the matching concept.concept_id.</li> </ul> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def apply_concept_mappings(\n    self,\n    session: AsyncSession,\n    automap: AutomapBase,\n    mapping: Dict[str, Any],\n) -&gt; None:\n    \"\"\"\n        Based on the \"concept\" key in the mapping JSON, populate target *_concept_id columns\n        by looking up concept.concept_id using codes found in the specified source column.\n\n        Rules:\n    - If the source value is a comma-separated string, use only the first element for lookup.\n    - Find by equality on concept.concept_code.\n    - Update the target column with the matching concept.concept_id.\n    \"\"\"\n    if not mapping or \"concept\" not in mapping:\n        return\n\n    # Resolve concept table\n    try:\n        concept_cls = getattr(automap.classes, \"concept\")\n    except AttributeError:\n        return\n\n    concept_table = concept_cls.__table__\n\n    # Simple in-memory code to cid mapping\n    code_to_cid: Dict[str, Optional[int]] = {}\n\n    async def lookup_concept_id(code: str) -&gt; Optional[int]:\n        if code in code_to_cid:\n            return code_to_cid[code]\n        res = await session.execute(\n            select(concept_table.c.concept_id).where(\n                concept_table.c.concept_code == code\n            )\n        )\n        row = res.first()\n        cid = int(row[0]) if row and row[0] is not None else None\n        code_to_cid[code] = cid\n        return cid\n\n    for item in mapping.get(\"concept\", []):\n        table_name = item.get(\"table\")\n        if not table_name:\n            continue\n        try:\n            mapper = getattr(automap.classes, table_name)\n        except AttributeError:\n            # Target table not found; skip\n            continue\n\n        table = mapper.__table__\n        pk_cols = list(table.primary_key.columns)\n        if not pk_cols:\n            # Cannot safely update without a primary key\n            continue\n\n        for m in item.get(\"mappings\", []):\n            source_col = m.get(\"source\")\n            target_col = m.get(\"target\")\n            if not source_col or not target_col:\n                continue\n            if source_col not in table.c or target_col not in table.c:\n                continue\n\n            # Fetch candidate rows: target is NULL or 0, and source is not NULL/empty\n            res = await session.execute(\n                select(\n                    *pk_cols,\n                    table.c[source_col].label(\"_src\"),\n                    table.c[target_col].label(\"_tgt\"),\n                ).where(\n                    or_(\n                        table.c[target_col].is_(None),\n                        table.c[target_col] == 0,\n                    ),\n                    table.c[source_col].isnot(None),\n                )\n            )\n\n            rows = res.fetchall()\n            if not rows:\n                continue\n\n            for row in rows:\n                # row is a tuple: (*pk_vals, _src, _tgt)\n                pk_vals = row[: len(pk_cols)]\n                src_val = row[len(pk_cols)]\n\n                # Only care about non-empty strings; if comma-separated, take first element\n                code: Optional[str] = None\n                if isinstance(src_val, str):\n                    # Split on comma and strip whitespace\n                    first = src_val.split(\",\")[0].strip()\n                    code = first if first else None\n                elif isinstance(src_val, list) and src_val:\n                    # If a list somehow made it into the DB, use first element's string\n                    code = str(src_val[0])\n                else:\n                    # Fallback to simple string conversion if it's a scalar\n                    code = str(src_val) if src_val is not None else None\n\n                if not code:\n                    continue\n\n                cid = await lookup_concept_id(code)\n                if cid is None:\n                    continue\n\n                # Build WHERE with PK columns\n                where_clause = and_(\n                    *[\n                        (pk_col == pk_val)\n                        for pk_col, pk_val in zip(pk_cols, pk_vals)\n                    ]\n                )\n\n                stmt = update(table).where(where_clause).values({target_col: cid})\n                await session.execute(stmt)\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.backfill_person_birth_fields","title":"<code>backfill_person_birth_fields(session, automap)</code>  <code>async</code>","text":"<pre><code>In the person table, replace 0 or NULL values in year_of_birth, month_of_birth,\nand day_of_birth with values derived from birth_datetime.\n</code></pre> <p>This runs in Python for portability across backends.</p> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def backfill_person_birth_fields(\n    self, session: AsyncSession, automap: AutomapBase\n) -&gt; None:\n    \"\"\"\n        In the person table, replace 0 or NULL values in year_of_birth, month_of_birth,\n        and day_of_birth with values derived from birth_datetime.\n\n    This runs in Python for portability across backends.\n    \"\"\"\n    # Resolve person table from automap\n    try:\n        person_cls = getattr(automap.classes, \"person\")\n    except AttributeError:\n        return\n\n    person_table = person_cls.__table__\n\n    # Fetch necessary columns\n    res = await session.execute(\n        select(\n            person_table.c.person_id,\n            person_table.c.birth_datetime,\n            person_table.c.year_of_birth,\n            person_table.c.month_of_birth,\n            person_table.c.day_of_birth,\n        ).where(person_table.c.birth_datetime.isnot(None))\n    )\n\n    rows = res.fetchall()\n    if not rows:\n        return\n\n    for pid, birth_dt, y, m, d in rows:\n        # Parse birth_dt to a datetime if needed\n        bd: Optional[datetime]\n        if isinstance(birth_dt, datetime):\n            # Normalize timezone-aware to UTC-naive\n            if birth_dt.tzinfo is not None:\n                bd = birth_dt.astimezone(timezone.utc).replace(tzinfo=None)\n            else:\n                bd = birth_dt\n        elif isinstance(birth_dt, date):\n            bd = datetime(birth_dt.year, birth_dt.month, birth_dt.day)\n        else:\n            try:\n                tmp = pd.to_datetime(birth_dt, errors=\"coerce\", utc=True)\n                if pd.isna(tmp):\n                    bd = None\n                else:\n                    py = tmp.to_pydatetime()\n                    bd = py.astimezone(timezone.utc).replace(tzinfo=None)\n            except Exception:\n                bd = None\n\n        if bd is None:\n            continue\n\n        new_y = y if (y is not None and int(y or 0) != 0) else bd.year\n        new_m = m if (m is not None and int(m or 0) != 0) else bd.month\n        new_d = d if (d is not None and int(d or 0) != 0) else bd.day\n\n        # Only update when something changes\n        if new_y != y or new_m != m or new_d != d:\n            stmt = (\n                update(person_table)\n                .where(person_table.c.person_id == pid)\n                .values(\n                    year_of_birth=new_y,\n                    month_of_birth=new_m,\n                    day_of_birth=new_d,\n                )\n            )\n            await session.execute(stmt)\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.fix_person_id","title":"<code>fix_person_id(session, automap)</code>  <code>async</code>","text":"<p>Update all tables so that person_id foreign keys store the canonical person.person_id (integer), replacing any rows where person_id currently contains the person_source_value (string/UUID).</p> <p>Approach: - Build a mapping from person_source_value -&gt; person_id from the person table. - For each table (except person) having a person_id column, run updates:             SET person_id = person.person_id WHERE CAST(person_id AS TEXT) = person_source_value.         - This is safe for SQLite (used in examples). For stricter RDBMS, ensure types             are compatible or adjust as needed.</p> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def fix_person_id(self, session: AsyncSession, automap: AutomapBase) -&gt; None:\n    \"\"\"\n    Update all tables so that person_id foreign keys store the canonical\n    person.person_id (integer), replacing any rows where person_id currently\n    contains the person_source_value (string/UUID).\n\n    Approach:\n    - Build a mapping from person_source_value -&gt; person_id from the person table.\n    - For each table (except person) having a person_id column, run updates:\n                SET person_id = person.person_id WHERE CAST(person_id AS TEXT) = person_source_value.\n            - This is safe for SQLite (used in examples). For stricter RDBMS, ensure types\n                are compatible or adjust as needed.\n    \"\"\"\n    # Resolve person table from automap\n    try:\n        person_cls = getattr(automap.classes, \"person\")\n    except AttributeError:\n        return  # No person table; nothing to do\n\n    person_table = person_cls.__table__\n\n    # Build mapping of person_source_value -&gt; person_id\n    res = await session.execute(\n        select(person_table.c.person_source_value, person_table.c.person_id).where(\n            person_table.c.person_source_value.isnot(None)\n        )\n    )\n    pairs = res.fetchall()\n    if not pairs:\n        return\n\n    psv_to_id: Dict[str, int] = {}\n    for psv, pid in pairs:\n        if psv is None or pid is None:\n            continue\n        psv_to_id[str(psv)] = int(pid)\n\n    if not psv_to_id:\n        return\n\n    # Iterate all tables and update person_id where it matches a known person_source_value\n    # Also handle rows that staged a non-numeric value in person_id_text.\n    # Avoid metadata.sorted_tables to prevent SAWarning about unresolvable cycles in vocab tables.\n    for tbl_name, table in automap.metadata.tables.items():\n        if tbl_name == person_table.name:\n            continue\n        if \"person_id\" not in table.c:\n            continue\n\n        # If person_id_text exists, prefer it for matching\n        has_text = \"person_id_text\" in table.c\n\n        # Run per-psv updates; small and explicit for clarity\n        for psv, pid in psv_to_id.items():\n            if has_text:\n                # Match on person_id_text\n                stmt = (\n                    update(table)\n                    .where(table.c.person_id_text == psv)\n                    .values(person_id=pid)\n                )\n                await session.execute(stmt)\n            # Also try matching where person_id was staged as string\n            stmt2 = (\n                update(table)\n                .where(cast(table.c.person_id, String()) == psv)\n                .values(person_id=pid)\n            )\n            await session.execute(stmt2)\n\n        # Clear person_id_text after normalization when column exists\n        if has_text:\n            try:\n                await session.execute(\n                    update(table)\n                    .where(table.c.person_id_text.isnot(None))\n                    .values(person_id_text=None)\n                )\n            except Exception:\n                pass\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.load","title":"<code>load(csv_path, mapping_path=None, chunk_size=1000)</code>  <code>async</code>","text":"<p>Load a CSV into multiple OMOP tables based on a mapping file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>str</code> <p>Path to the input CSV file.</p> required <code>mapping_path</code> <code>str | None</code> <p>Path to the JSON mapping file. Defaults to the package's <code>mapping.default.json</code> when not provided.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>Batch size for INSERT statements.</p> <code>1000</code> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def load(\n    self, csv_path: str, mapping_path: str | None = None, chunk_size: int = 1000\n) -&gt; None:\n    \"\"\"Load a CSV into multiple OMOP tables based on a mapping file.\n\n    Args:\n        csv_path: Path to the input CSV file.\n        mapping_path: Path to the JSON mapping file. Defaults to the\n            package's ``mapping.default.json`` when not provided.\n        chunk_size: Batch size for INSERT statements.\n    \"\"\"\n    # If mapping path is None, load mapping.default.json from the current directory\n    # Step 1: Load CSV and mapping\n    start_time = time.time()\n    logger.info(f\"Loading CSV data from {csv_path}\")\n    if mapping_path is None:\n        mapping_path = str(Path(__file__).parent / \"mapping.default.json\")\n    mapping = self._load_mapping(mapping_path)\n    # Use low_memory=False to avoid DtypeWarning for mixed-type columns\n    df = pd.read_csv(csv_path, low_memory=False)\n\n    async with self._get_session() as session:\n        # Relax constraint enforcement during bulk load on Postgres\n        is_pg = False\n        try:\n            is_pg = str(self._engine.dialect.name).startswith(\"postgres\")\n        except Exception:\n            is_pg = False\n        if is_pg:\n            try:\n                await session.execute(\n                    text(\"SET session_replication_role = replica\")\n                )\n            except Exception:\n                pass\n\n        try:\n            conn = await session.connection()\n            # Before reflecting, add a temporary person_id_text column to accept non-numeric IDs\n            await self._add_person_id_text_columns(session)\n            automap = await self._prepare_automap(conn)\n\n            for tbl in mapping.get(\"tables\", []):\n                table_name = tbl.get(\"name\")\n                if not table_name:\n                    continue\n                # obtain mapped class\n                try:\n                    mapper = getattr(automap.classes, table_name)\n                except AttributeError:\n                    raise ValueError(f\"Table '{table_name}' not found in database.\")\n\n                # compute filtered dataframe\n                df_tbl = self._apply_filters(df, tbl.get(\"filters\"))\n                if df_tbl.empty:\n                    continue\n\n                col_map: Dict[str, Any] = tbl.get(\"columns\", {})\n                # Gather target SQLA column metadata\n                sa_cols = {c.name: c.type for c in mapper.__table__.columns}\n                sa_col_objs = {c.name: c for c in mapper.__table__.columns}\n\n                # Build records\n                records: List[Dict[str, Any]] = []\n                for _, row in df_tbl.iterrows():\n                    rec: Dict[str, Any] = {}\n                    for target_col, src in col_map.items():\n                        if isinstance(src, dict) and \"const\" in src:\n                            value = src[\"const\"]\n                        elif isinstance(src, str):\n                            value = row.get(src)\n                        else:\n                            value = None\n\n                        # IMPORTANT: keep person_id raw for staging logic below\n                        if target_col != \"person_id\":\n                            # Convert based on SA type if available\n                            sa_t = sa_cols.get(target_col)\n                            if sa_t is not None:\n                                value = self._convert_value(sa_t, value)\n                        rec[target_col] = value\n\n                    # If person_id exists in the record, route non-numeric values into person_id_text\n                    if \"person_id\" in rec:\n                        pid = rec.get(\"person_id\")\n                        if pid is None:\n                            # If NOT NULL, use placeholder to avoid constraint errors\n                            col = sa_col_objs.get(\"person_id\")\n                            if col is not None and not getattr(\n                                col, \"nullable\", True\n                            ):\n                                rec[\"person_id\"] = 0\n                        elif isinstance(pid, int):\n                            pass\n                        elif isinstance(pid, str) and pid.strip().isdigit():\n                            try:\n                                rec[\"person_id\"] = int(pid.strip())\n                            except Exception:\n                                # If conversion unexpectedly fails, send to text column\n                                if \"person_id_text\" in sa_cols:\n                                    rec[\"person_id_text\"] = str(pid)\n                                # Respect NOT NULL with placeholder when required\n                                col = sa_col_objs.get(\"person_id\")\n                                if col is not None and not getattr(\n                                    col, \"nullable\", True\n                                ):\n                                    rec[\"person_id\"] = 0\n                                else:\n                                    rec[\"person_id\"] = None\n                        else:\n                            # Non-numeric content: place into person_id_text if available\n                            if \"person_id_text\" in sa_cols:\n                                rec[\"person_id_text\"] = str(pid)\n                            # Respect NOT NULL with placeholder when required\n                            col = sa_col_objs.get(\"person_id\")\n                            if col is not None and not getattr(\n                                col, \"nullable\", True\n                            ):\n                                rec[\"person_id\"] = 0\n                            else:\n                                rec[\"person_id\"] = None\n                    # Finally coerce all fields to the table's schema (string lengths, forced TEXT, datetimes)\n                    rec = self._coerce_record_to_table_types(\n                        mapper.__table__,\n                        rec,\n                        set(mapping.get(\"force_text_fields\", [])),\n                    )\n                    records.append(rec)\n\n                if not records:\n                    continue\n\n                stmt = insert(mapper)\n                # Chunked insert\n                for i in range(0, len(records), chunk_size):\n                    batch = records[i : i + chunk_size]\n                    await session.execute(stmt, batch)\n            # Step 1 complete: all data loaded into target tables\n            elapsed = time.time() - start_time\n            logger.info(f\"Data loaded into target tables (elapsed: {elapsed:.2f}s)\")\n            step_time = time.time()\n\n            # Step 2: Normalize person_id FKs using person.person_id (not person_source_value)\n            logger.info(\"Normalizing person_id foreign keys\")\n            await self.fix_person_id(session, automap)\n\n            # Drop the temporary person_id_text columns now that person_id has been normalized\n            await self._drop_person_id_text_columns(session)\n            elapsed = time.time() - step_time\n            logger.info(\n                f\"Person_id foreign keys normalized (elapsed: {elapsed:.2f}s)\"\n            )\n            step_time = time.time()\n\n            # Step 3: Backfill year/month/day of birth from birth_datetime where missing or zero\n            logger.info(\"Backfilling person birth fields\")\n            await self.backfill_person_birth_fields(session, automap)\n            elapsed = time.time() - step_time\n            logger.info(f\"Person birth fields backfilled (elapsed: {elapsed:.2f}s)\")\n            step_time = time.time()\n\n            # Step 4: Set gender_concept_id from gender_source_value using standard IDs\n            logger.info(\"Setting person.gender_concept_id from gender_source_value\")\n            await self.update_person_gender_concept_id(session, automap)\n            elapsed = time.time() - step_time\n            logger.info(f\"Person gender_concept_id set (elapsed: {elapsed:.2f}s)\")\n            step_time = time.time()\n\n            # Step 5: Apply concept mappings defined in the JSON mapping\n            logger.info(\"Applying concept mappings\")\n            await self.apply_concept_mappings(session, automap, mapping)\n            elapsed = time.time() - step_time\n            logger.info(f\"Concept mappings applied (elapsed: {elapsed:.2f}s)\")\n\n            await session.commit()\n        finally:\n            if is_pg:\n                try:\n                    await session.execute(\n                        text(\"SET session_replication_role = origin\")\n                    )\n                except Exception:\n                    pass\n            await session.close()\n</code></pre>"},{"location":"modules/#loader.CdmCsvLoader.update_person_gender_concept_id","title":"<code>update_person_gender_concept_id(session, automap)</code>  <code>async</code>","text":"<pre><code>Update person.gender_concept_id from person.gender_source_value using static mapping:\n- male (or 'm')   -&gt; 8507\n- female (or 'f') -&gt; 8532\n- anything else   -&gt; 0 (unknown)\n</code></pre> <p>Only updates rows where the computed value differs from the current value or where gender_concept_id is NULL.</p> Source code in <code>src/pyomop/loader.py</code> <pre><code>async def update_person_gender_concept_id(\n    self, session: AsyncSession, automap: AutomapBase\n) -&gt; None:\n    \"\"\"\n        Update person.gender_concept_id from person.gender_source_value using static mapping:\n        - male (or 'm')   -&gt; 8507\n        - female (or 'f') -&gt; 8532\n        - anything else   -&gt; 0 (unknown)\n\n    Only updates rows where the computed value differs from the current value\n    or where gender_concept_id is NULL.\n    \"\"\"\n    try:\n        person_cls = getattr(automap.classes, \"person\")\n    except AttributeError:\n        return\n\n    person_table = person_cls.__table__\n\n    # Fetch rows to evaluate. We consider all rows with a non-null gender_source_value\n    res = await session.execute(\n        select(\n            person_table.c.person_id,\n            person_table.c.gender_source_value,\n            person_table.c.gender_concept_id,\n        ).where(person_table.c.gender_source_value.isnot(None))\n    )\n\n    rows = res.fetchall()\n    if not rows:\n        return\n\n    def map_gender(val: str | None) -&gt; int:\n        if val is None:\n            return 0\n        s = str(val).strip().lower()\n        if s in {\"male\", \"m\"}:\n            return 8507\n        if s in {\"female\", \"f\"}:\n            return 8532\n        return 0\n\n    for pid, gsrc, gcid in rows:\n        target = map_gender(gsrc)\n        # Skip if already correct\n        if gcid == target:\n            continue\n        stmt = (\n            update(person_table)\n            .where(person_table.c.person_id == pid)\n            .values(gender_concept_id=target)\n        )\n        await session.execute(stmt)\n</code></pre>"},{"location":"modules/#main.main_routine","title":"<code>main_routine()</code>","text":"<p>Top-level runner used by <code>python -m pyomop</code>.</p> Source code in <code>src/pyomop/main.py</code> <pre><code>def main_routine():\n    \"\"\"Top-level runner used by ``python -m pyomop``.\"\"\"\n    click.echo(\"_________________________________________\")\n    click.echo(\"Pyomop v\" + __version__ + \" by Bell Eapen ( https://nuchange.ca ) \")\n    cli()  # run the main function\n    click.echo(\"Pyomop done.\")\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary","title":"<code>CdmVocabulary</code>","text":"<p>               Bases: <code>object</code></p> <p>Helpers for OMOP Vocabulary management and lookups.</p> <p>Parameters:</p> Name Type Description Default <code>cdm</code> <p>An initialized <code>CdmEngineFactory</code> instance.</p> required <code>version</code> <p>CDM version string (\"cdm54\" or \"cdm6\"). Defaults to \"cdm54\".</p> <code>'cdm54'</code> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>class CdmVocabulary(object):\n    \"\"\"Helpers for OMOP Vocabulary management and lookups.\n\n    Args:\n        cdm: An initialized ``CdmEngineFactory`` instance.\n        version: CDM version string (\"cdm54\" or \"cdm6\"). Defaults to \"cdm54\".\n    \"\"\"\n\n    def __init__(self, cdm, version=\"cdm54\"):\n        self._concept_id = 0\n        self._concept_name = \"\"\n        self._domain_id = \"\"\n        self._vocabulary_id = \"\"\n        self._concept_class_id = \"\"\n        self._concept_code = \"\"\n        self._cdm = cdm\n        self._engine = cdm.engine\n        self._maker = async_sessionmaker(self._engine, class_=AsyncSession)\n        self._scope = async_scoped_session(self._maker, scopefunc=asyncio.current_task)\n        self._version = version\n\n    @property\n    def concept_id(self):\n        \"\"\"Current concept_id for this helper (if set).\"\"\"\n        return self._concept_id\n\n    @property\n    def concept_code(self):\n        \"\"\"Current concept_code for this helper (if set).\"\"\"\n        return self._concept_code\n\n    @property\n    def concept_name(self):\n        \"\"\"Current concept_name for this helper (if set).\"\"\"\n        return self._concept_name\n\n    @property\n    def vocabulary_id(self):\n        \"\"\"Current vocabulary_id for this helper (if set).\"\"\"\n        return self._vocabulary_id\n\n    @property\n    def domain_id(self):\n        \"\"\"Current domain_id for this helper (if set).\"\"\"\n        return self._domain_id\n\n    @concept_id.setter\n    def concept_id(self, concept_id):\n        \"\"\"Set the active concept context by concept_id.\n\n        Side effects: populates concept name, domain, vocabulary, class, and code\n        on this helper instance for convenience.\n\n        Args:\n            concept_id: The concept_id to fetch and set.\n        \"\"\"\n        self._concept_id = concept_id\n        _concept = asyncio.run(self.get_concept(concept_id))\n        self._concept_name = _concept.concept_name\n        self._domain_id = _concept.domain_id\n        self._vocabulary_id = _concept.vocabulary_id\n        self._concept_class_id = _concept.concept_class_id\n        self._concept_code = _concept.concept_code\n\n    async def get_concept(self, concept_id):\n        \"\"\"Fetch a concept row by id.\n\n        Args:\n            concept_id: Concept identifier.\n\n        Returns:\n            The ORM Concept instance.\n        \"\"\"\n        if self._version == \"cdm6\":\n            from .cdm6 import Concept\n        else:\n            from .cdm54 import Concept\n        stmt = select(Concept).where(Concept.concept_id == concept_id)\n        async with self._cdm.session() as session:\n            _concept = await session.execute(stmt)\n        return _concept.scalar_one()\n\n    async def get_concept_by_code(self, concept_code, vocabulary_id):\n        \"\"\"Fetch a concept by code within a vocabulary.\n\n        Args:\n            concept_code: The vocabulary-specific code string.\n            vocabulary_id: Vocabulary identifier (e.g., 'SNOMED', 'LOINC').\n\n        Returns:\n            The ORM Concept instance.\n        \"\"\"\n        if self._version == \"cdm6\":\n            from .cdm6 import Concept\n        else:\n            from .cdm54 import Concept\n        stmt = (\n            select(Concept)\n            .where(Concept.concept_code == concept_code)\n            .where(Concept.vocabulary_id == vocabulary_id)\n        )\n        async with self._cdm.session() as session:\n            _concept = await session.execute(stmt)\n        return _concept.scalar_one()\n\n    def set_concept(self, concept_code, vocabulary_id=None):\n        \"\"\"Set the active concept context by code and vocabulary.\n\n        Args:\n            concept_code: The concept code string to resolve.\n            vocabulary_id: Vocabulary identifier. Required.\n\n        Notes:\n            On success, populates concept fields on this instance. On failure,\n            sets ``_vocabulary_id`` and ``_concept_id`` to 0.\n        \"\"\"\n        self._concept_code = concept_code\n        try:\n            if vocabulary_id is not None:\n                self._vocabulary_id = vocabulary_id\n                _concept = asyncio.run(\n                    self.get_concept_by_code(concept_code, vocabulary_id)\n                )\n            else:\n                raise ValueError(\n                    \"vocabulary_id must be provided when setting concept by code.\"\n                )\n\n            self._concept_name = _concept.concept_name\n            self._domain_id = _concept.domain_id\n            self._concept_id = _concept.concept_id\n            self._concept_class_id = _concept.concept_class_id\n            self._concept_code = _concept.concept_code\n\n        except:\n            self._vocabulary_id = 0\n            self._concept_id = 0\n\n    async def create_vocab(self, folder, sample=None):\n        \"\"\"Load vocabulary CSV files from a folder into the database.\n\n        This imports the standard OMOP vocab tables (drug_strength, concept,\n        concept_relationship, concept_ancestor, concept_synonym, vocabulary,\n        relationship, concept_class, domain).\n\n        Args:\n            folder: Path to the folder containing OMOP vocabulary CSVs.\n            sample: Optional number of rows to limit per file during import.\n        \"\"\"\n        try:\n            # Parents first (for concept FKs): DOMAIN, CONCEPT_CLASS\n            df = pd.read_csv(\n                folder + \"/DOMAIN.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"domain\", \"replace\")\n            logger.info(\"DOMAIN.csv loaded and processed.\")\n\n            df = pd.read_csv(\n                folder + \"/CONCEPT_CLASS.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"concept_class\", \"replace\")\n            logger.info(\"CONCEPT_CLASS.csv loaded and processed.\")\n\n            # Then CONCEPT (uses domain_id and concept_class_id)\n            df = pd.read_csv(\n                folder + \"/CONCEPT.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            df[\"valid_start_date\"] = pd.to_datetime(\n                df[\"valid_start_date\"], errors=\"coerce\"\n            )\n            df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n            await self.write_vocab(df, \"concept\", \"replace\")\n            logger.info(\"CONCEPT.csv loaded and processed.\")\n\n            # Then VOCABULARY (uses vocabulary_concept_id -&gt; concept)\n            df = pd.read_csv(\n                folder + \"/VOCABULARY.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"vocabulary\", \"replace\")\n            logger.info(\"VOCABULARY.csv loaded and processed.\")\n\n            # Relationship depends on concept for relationship_concept_id\n            df = pd.read_csv(\n                folder + \"/RELATIONSHIP.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"relationship\", \"replace\")\n            logger.info(\"RELATIONSHIP.csv loaded and processed.\")\n\n            # Post-concept tables\n            df = pd.read_csv(\n                folder + \"/CONCEPT_RELATIONSHIP.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            df[\"valid_start_date\"] = pd.to_datetime(\n                df[\"valid_start_date\"], errors=\"coerce\"\n            )\n            df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n            await self.write_vocab(df, \"concept_relationship\", \"replace\")\n            logger.info(\"CONCEPT_RELATIONSHIP.csv loaded and processed.\")\n\n            df = pd.read_csv(\n                folder + \"/CONCEPT_SYNONYM.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"concept_synonym\", \"replace\")\n            logger.info(\"CONCEPT_SYNONYM.csv loaded and processed.\")\n\n            df = pd.read_csv(\n                folder + \"/DRUG_STRENGTH.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            df[\"valid_start_date\"] = pd.to_datetime(\n                df[\"valid_start_date\"], errors=\"coerce\"\n            )\n            df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n            await self.write_vocab(df, \"drug_strength\", \"replace\")\n            logger.info(\"DRUG_STRENGTH.csv loaded and processed.\")\n\n            df = pd.read_csv(\n                folder + \"/CONCEPT_ANCESTOR.csv\",\n                sep=\"\\t\",\n                nrows=sample,\n                on_bad_lines=\"skip\",\n                low_memory=False,\n            )\n            await self.write_vocab(df, \"concept_ancestor\", \"replace\")\n            logger.info(\"CONCEPT_ANCESTOR.csv loaded and processed.\")\n        except Exception as e:\n            logger.error(f\"An error occurred while creating the vocabulary: {e}\")\n\n    @asynccontextmanager\n    async def get_session(self) -&gt; AsyncGenerator[AsyncSession, None]:\n        \"\"\"Yield an async session bound to the current engine.\n\n        Yields:\n            AsyncSession: An async SQLAlchemy session.\n        \"\"\"\n        async with self._scope() as session:\n            yield session\n\n    async def write_vocab(self, df, table, if_exists=\"replace\", chunk_size=1000):\n        \"\"\"Write a DataFrame to a vocabulary table with type-safe defaults.\n\n        Ensures required columns exist with reasonable defaults, coerces types,\n        and performs chunked inserts via SQLAlchemy core for performance.\n\n        Args:\n            df: Pandas DataFrame with data to insert.\n            table: Target table name (e.g., 'concept').\n            if_exists: Compatibility only. This method always inserts.\n            chunk_size: Number of rows per batch insert.\n        \"\"\"\n        async with self.get_session() as session:\n            # For PostgreSQL, temporarily relax constraint enforcement during bulk loads\n            is_pg = False\n            try:\n                is_pg = self._engine.dialect.name.startswith(\"postgres\")\n            except Exception:\n                is_pg = False\n            if is_pg:\n                logger.info(\n                    \"Temporarily disabling replication role for bulk load on postgres\"\n                )\n                try:\n                    await session.execute(\n                        text(\"SET session_replication_role = replica\")\n                    )\n                except Exception:\n                    # Ignore if not permitted or unsupported\n                    logger.warning(\"Failed to set session_replication_role to replica\")\n\n            conn = await session.connection()\n            automap: AutomapBase = automap_base()\n\n            def prepare_automap(sync_conn):\n                automap.prepare(autoload_with=sync_conn)\n\n            await conn.run_sync(prepare_automap)\n            mapper = getattr(automap.classes, table)\n\n            # Build defaults for non-nullable columns based on SQL types\n            sa_cols = {c.name: c for c in mapper.__table__.columns}\n\n            def default_for(col):\n                from sqlalchemy import (\n                    BigInteger,\n                    Date,\n                    DateTime,\n                    Integer,\n                    Numeric,\n                    String,\n                    Text,\n                )\n\n                t = col.type\n                if isinstance(t, (Integer, BigInteger)):\n                    return 0\n                if isinstance(t, Numeric):\n                    return 0\n                if isinstance(t, (String, Text)):\n                    return \"UNKNOWN\"\n                if isinstance(t, Date):\n                    return date(1970, 1, 1)\n                if isinstance(t, DateTime):\n                    return datetime(1970, 1, 1)\n                return None\n\n            # Work on a copy so we can normalize types and fill required fields\n            df2 = df.copy()\n\n            for name, col in sa_cols.items():\n                # Ensure column exists\n                if name not in df2.columns:\n                    # For nullable columns, start with None; for required, use default\n                    df2[name] = None if col.nullable else default_for(col)\n                    continue\n\n                # Coerce types and handle missing values\n                if str(df2[name].dtype) == \"object\":\n                    # Treat empty strings as missing\n                    df2[name] = df2[name].replace(\"\", np.nan)\n\n                from sqlalchemy import BigInteger\n                from sqlalchemy import Date as SA_Date\n                from sqlalchemy import DateTime as SA_DateTime\n                from sqlalchemy import Integer, Numeric, String, Text\n\n                t = col.type\n                if isinstance(t, SA_Date):\n                    ser = pd.to_datetime(df2[name], errors=\"coerce\").dt.date\n                    df2[name] = (\n                        ser.where(pd.notna(ser), None)\n                        if col.nullable\n                        else ser.fillna(default_for(col))\n                    )\n                elif isinstance(t, SA_DateTime):\n                    # Normalize to UTC-naive to avoid tz-aware vs tz-naive issues in Postgres\n                    ser = pd.to_datetime(df2[name], errors=\"coerce\", utc=True)\n\n                    # Convert to Python datetime and drop tzinfo\n                    def _to_naive(dt):\n                        try:\n                            if pd.isna(dt):\n                                return None\n                        except Exception:\n                            pass\n                        if hasattr(dt, \"to_pydatetime\"):\n                            py = dt.to_pydatetime()\n                        else:\n                            py = dt\n                        if getattr(py, \"tzinfo\", None) is not None:\n                            py = (\n                                py.tz_convert(\"UTC\").tz_localize(None)\n                                if hasattr(py, \"tz_convert\")\n                                else py.replace(tzinfo=None)\n                            )\n                        return py\n\n                    ser = ser.map(_to_naive)\n                    df2[name] = (\n                        ser.where(pd.notna(ser), None)\n                        if col.nullable\n                        else ser.fillna(default_for(col))\n                    )\n                elif isinstance(t, (Integer, BigInteger)):\n                    ser = pd.to_numeric(df2[name], errors=\"coerce\")\n                    df2[name] = (\n                        ser.where(pd.notna(ser), None)\n                        if col.nullable\n                        else ser.fillna(default_for(col))\n                    )\n                elif isinstance(t, Numeric):\n                    ser = pd.to_numeric(df2[name], errors=\"coerce\")\n                    df2[name] = (\n                        ser.where(pd.notna(ser), None)\n                        if col.nullable\n                        else ser.fillna(default_for(col))\n                    )\n                elif isinstance(t, (String, Text)):\n                    # Only cast non-null values to str and trim; keep nulls as None\n                    ser = df2[name].astype(object)\n                    mask = ser.notna()\n                    ser.loc[mask] = ser.loc[mask].astype(str).str.slice(0, 255)\n                    if col.nullable:\n                        ser = ser.where(pd.notna(ser), None)\n                    else:\n                        # Required string columns get a default\n                        ser = ser.where(pd.notna(ser), default_for(col))\n                    df2[name] = ser\n                else:\n                    # Fallback: ensure NaN/NaT -&gt; None for nullable cols, else fill default\n                    df2[name] = (\n                        df2[name].where(pd.notna(df2[name]), None)\n                        if col.nullable\n                        else df2[name].fillna(default_for(col))\n                    )\n\n            # Final safety pass: replace any remaining NaN/NaT with None across all columns\n            df2 = df2.where(pd.notna(df2), None)\n\n            stmt = insert(mapper)\n\n            try:\n                for _, group in df2.groupby(\n                    np.arange(df2.shape[0], dtype=int) // chunk_size\n                ):\n                    records = group.to_dict(\"records\")\n                    try:\n                        # Fast path: batch insert\n                        await session.execute(stmt, records)\n                    except Exception:\n                        logger.warning(\n                            \"Batch insert failed, falling back to row-by-row insert.\"\n                        )\n                        # Fallback: insert row-by-row, skipping bad rows\n                        for row in records:\n                            try:\n                                await session.execute(stmt, [row])\n                            except Exception:\n                                # Ignore duplicates/FK issues per row\n                                logger.warning(\n                                    f\"Failed to insert row: {row}. Skipping.\"\n                                )\n                                continue\n                    # Commit after each group\n                    await session.commit()\n            finally:\n                if is_pg:\n                    try:\n                        await session.execute(\n                            text(\"SET session_replication_role = origin\")\n                        )\n                    except Exception:\n                        pass\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.concept_code","title":"<code>concept_code</code>  <code>property</code>","text":"<p>Current concept_code for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.concept_id","title":"<code>concept_id</code>  <code>property</code> <code>writable</code>","text":"<p>Current concept_id for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.concept_name","title":"<code>concept_name</code>  <code>property</code>","text":"<p>Current concept_name for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.domain_id","title":"<code>domain_id</code>  <code>property</code>","text":"<p>Current domain_id for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.vocabulary_id","title":"<code>vocabulary_id</code>  <code>property</code>","text":"<p>Current vocabulary_id for this helper (if set).</p>"},{"location":"modules/#vocabulary.CdmVocabulary.create_vocab","title":"<code>create_vocab(folder, sample=None)</code>  <code>async</code>","text":"<p>Load vocabulary CSV files from a folder into the database.</p> <p>This imports the standard OMOP vocab tables (drug_strength, concept, concept_relationship, concept_ancestor, concept_synonym, vocabulary, relationship, concept_class, domain).</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <p>Path to the folder containing OMOP vocabulary CSVs.</p> required <code>sample</code> <p>Optional number of rows to limit per file during import.</p> <code>None</code> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>async def create_vocab(self, folder, sample=None):\n    \"\"\"Load vocabulary CSV files from a folder into the database.\n\n    This imports the standard OMOP vocab tables (drug_strength, concept,\n    concept_relationship, concept_ancestor, concept_synonym, vocabulary,\n    relationship, concept_class, domain).\n\n    Args:\n        folder: Path to the folder containing OMOP vocabulary CSVs.\n        sample: Optional number of rows to limit per file during import.\n    \"\"\"\n    try:\n        # Parents first (for concept FKs): DOMAIN, CONCEPT_CLASS\n        df = pd.read_csv(\n            folder + \"/DOMAIN.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"domain\", \"replace\")\n        logger.info(\"DOMAIN.csv loaded and processed.\")\n\n        df = pd.read_csv(\n            folder + \"/CONCEPT_CLASS.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"concept_class\", \"replace\")\n        logger.info(\"CONCEPT_CLASS.csv loaded and processed.\")\n\n        # Then CONCEPT (uses domain_id and concept_class_id)\n        df = pd.read_csv(\n            folder + \"/CONCEPT.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        df[\"valid_start_date\"] = pd.to_datetime(\n            df[\"valid_start_date\"], errors=\"coerce\"\n        )\n        df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n        await self.write_vocab(df, \"concept\", \"replace\")\n        logger.info(\"CONCEPT.csv loaded and processed.\")\n\n        # Then VOCABULARY (uses vocabulary_concept_id -&gt; concept)\n        df = pd.read_csv(\n            folder + \"/VOCABULARY.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"vocabulary\", \"replace\")\n        logger.info(\"VOCABULARY.csv loaded and processed.\")\n\n        # Relationship depends on concept for relationship_concept_id\n        df = pd.read_csv(\n            folder + \"/RELATIONSHIP.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"relationship\", \"replace\")\n        logger.info(\"RELATIONSHIP.csv loaded and processed.\")\n\n        # Post-concept tables\n        df = pd.read_csv(\n            folder + \"/CONCEPT_RELATIONSHIP.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        df[\"valid_start_date\"] = pd.to_datetime(\n            df[\"valid_start_date\"], errors=\"coerce\"\n        )\n        df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n        await self.write_vocab(df, \"concept_relationship\", \"replace\")\n        logger.info(\"CONCEPT_RELATIONSHIP.csv loaded and processed.\")\n\n        df = pd.read_csv(\n            folder + \"/CONCEPT_SYNONYM.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"concept_synonym\", \"replace\")\n        logger.info(\"CONCEPT_SYNONYM.csv loaded and processed.\")\n\n        df = pd.read_csv(\n            folder + \"/DRUG_STRENGTH.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        df[\"valid_start_date\"] = pd.to_datetime(\n            df[\"valid_start_date\"], errors=\"coerce\"\n        )\n        df[\"valid_end_date\"] = pd.to_datetime(df[\"valid_end_date\"], errors=\"coerce\")\n        await self.write_vocab(df, \"drug_strength\", \"replace\")\n        logger.info(\"DRUG_STRENGTH.csv loaded and processed.\")\n\n        df = pd.read_csv(\n            folder + \"/CONCEPT_ANCESTOR.csv\",\n            sep=\"\\t\",\n            nrows=sample,\n            on_bad_lines=\"skip\",\n            low_memory=False,\n        )\n        await self.write_vocab(df, \"concept_ancestor\", \"replace\")\n        logger.info(\"CONCEPT_ANCESTOR.csv loaded and processed.\")\n    except Exception as e:\n        logger.error(f\"An error occurred while creating the vocabulary: {e}\")\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.get_concept","title":"<code>get_concept(concept_id)</code>  <code>async</code>","text":"<p>Fetch a concept row by id.</p> <p>Parameters:</p> Name Type Description Default <code>concept_id</code> <p>Concept identifier.</p> required <p>Returns:</p> Type Description <p>The ORM Concept instance.</p> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>async def get_concept(self, concept_id):\n    \"\"\"Fetch a concept row by id.\n\n    Args:\n        concept_id: Concept identifier.\n\n    Returns:\n        The ORM Concept instance.\n    \"\"\"\n    if self._version == \"cdm6\":\n        from .cdm6 import Concept\n    else:\n        from .cdm54 import Concept\n    stmt = select(Concept).where(Concept.concept_id == concept_id)\n    async with self._cdm.session() as session:\n        _concept = await session.execute(stmt)\n    return _concept.scalar_one()\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.get_concept_by_code","title":"<code>get_concept_by_code(concept_code, vocabulary_id)</code>  <code>async</code>","text":"<p>Fetch a concept by code within a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>concept_code</code> <p>The vocabulary-specific code string.</p> required <code>vocabulary_id</code> <p>Vocabulary identifier (e.g., 'SNOMED', 'LOINC').</p> required <p>Returns:</p> Type Description <p>The ORM Concept instance.</p> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>async def get_concept_by_code(self, concept_code, vocabulary_id):\n    \"\"\"Fetch a concept by code within a vocabulary.\n\n    Args:\n        concept_code: The vocabulary-specific code string.\n        vocabulary_id: Vocabulary identifier (e.g., 'SNOMED', 'LOINC').\n\n    Returns:\n        The ORM Concept instance.\n    \"\"\"\n    if self._version == \"cdm6\":\n        from .cdm6 import Concept\n    else:\n        from .cdm54 import Concept\n    stmt = (\n        select(Concept)\n        .where(Concept.concept_code == concept_code)\n        .where(Concept.vocabulary_id == vocabulary_id)\n    )\n    async with self._cdm.session() as session:\n        _concept = await session.execute(stmt)\n    return _concept.scalar_one()\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.get_session","title":"<code>get_session()</code>  <code>async</code>","text":"<p>Yield an async session bound to the current engine.</p> <p>Yields:</p> Name Type Description <code>AsyncSession</code> <code>AsyncGenerator[AsyncSession, None]</code> <p>An async SQLAlchemy session.</p> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>@asynccontextmanager\nasync def get_session(self) -&gt; AsyncGenerator[AsyncSession, None]:\n    \"\"\"Yield an async session bound to the current engine.\n\n    Yields:\n        AsyncSession: An async SQLAlchemy session.\n    \"\"\"\n    async with self._scope() as session:\n        yield session\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.set_concept","title":"<code>set_concept(concept_code, vocabulary_id=None)</code>","text":"<p>Set the active concept context by code and vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>concept_code</code> <p>The concept code string to resolve.</p> required <code>vocabulary_id</code> <p>Vocabulary identifier. Required.</p> <code>None</code> Notes <p>On success, populates concept fields on this instance. On failure, sets <code>_vocabulary_id</code> and <code>_concept_id</code> to 0.</p> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>def set_concept(self, concept_code, vocabulary_id=None):\n    \"\"\"Set the active concept context by code and vocabulary.\n\n    Args:\n        concept_code: The concept code string to resolve.\n        vocabulary_id: Vocabulary identifier. Required.\n\n    Notes:\n        On success, populates concept fields on this instance. On failure,\n        sets ``_vocabulary_id`` and ``_concept_id`` to 0.\n    \"\"\"\n    self._concept_code = concept_code\n    try:\n        if vocabulary_id is not None:\n            self._vocabulary_id = vocabulary_id\n            _concept = asyncio.run(\n                self.get_concept_by_code(concept_code, vocabulary_id)\n            )\n        else:\n            raise ValueError(\n                \"vocabulary_id must be provided when setting concept by code.\"\n            )\n\n        self._concept_name = _concept.concept_name\n        self._domain_id = _concept.domain_id\n        self._concept_id = _concept.concept_id\n        self._concept_class_id = _concept.concept_class_id\n        self._concept_code = _concept.concept_code\n\n    except:\n        self._vocabulary_id = 0\n        self._concept_id = 0\n</code></pre>"},{"location":"modules/#vocabulary.CdmVocabulary.write_vocab","title":"<code>write_vocab(df, table, if_exists='replace', chunk_size=1000)</code>  <code>async</code>","text":"<p>Write a DataFrame to a vocabulary table with type-safe defaults.</p> <p>Ensures required columns exist with reasonable defaults, coerces types, and performs chunked inserts via SQLAlchemy core for performance.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>Pandas DataFrame with data to insert.</p> required <code>table</code> <p>Target table name (e.g., 'concept').</p> required <code>if_exists</code> <p>Compatibility only. This method always inserts.</p> <code>'replace'</code> <code>chunk_size</code> <p>Number of rows per batch insert.</p> <code>1000</code> Source code in <code>src/pyomop/vocabulary.py</code> <pre><code>async def write_vocab(self, df, table, if_exists=\"replace\", chunk_size=1000):\n    \"\"\"Write a DataFrame to a vocabulary table with type-safe defaults.\n\n    Ensures required columns exist with reasonable defaults, coerces types,\n    and performs chunked inserts via SQLAlchemy core for performance.\n\n    Args:\n        df: Pandas DataFrame with data to insert.\n        table: Target table name (e.g., 'concept').\n        if_exists: Compatibility only. This method always inserts.\n        chunk_size: Number of rows per batch insert.\n    \"\"\"\n    async with self.get_session() as session:\n        # For PostgreSQL, temporarily relax constraint enforcement during bulk loads\n        is_pg = False\n        try:\n            is_pg = self._engine.dialect.name.startswith(\"postgres\")\n        except Exception:\n            is_pg = False\n        if is_pg:\n            logger.info(\n                \"Temporarily disabling replication role for bulk load on postgres\"\n            )\n            try:\n                await session.execute(\n                    text(\"SET session_replication_role = replica\")\n                )\n            except Exception:\n                # Ignore if not permitted or unsupported\n                logger.warning(\"Failed to set session_replication_role to replica\")\n\n        conn = await session.connection()\n        automap: AutomapBase = automap_base()\n\n        def prepare_automap(sync_conn):\n            automap.prepare(autoload_with=sync_conn)\n\n        await conn.run_sync(prepare_automap)\n        mapper = getattr(automap.classes, table)\n\n        # Build defaults for non-nullable columns based on SQL types\n        sa_cols = {c.name: c for c in mapper.__table__.columns}\n\n        def default_for(col):\n            from sqlalchemy import (\n                BigInteger,\n                Date,\n                DateTime,\n                Integer,\n                Numeric,\n                String,\n                Text,\n            )\n\n            t = col.type\n            if isinstance(t, (Integer, BigInteger)):\n                return 0\n            if isinstance(t, Numeric):\n                return 0\n            if isinstance(t, (String, Text)):\n                return \"UNKNOWN\"\n            if isinstance(t, Date):\n                return date(1970, 1, 1)\n            if isinstance(t, DateTime):\n                return datetime(1970, 1, 1)\n            return None\n\n        # Work on a copy so we can normalize types and fill required fields\n        df2 = df.copy()\n\n        for name, col in sa_cols.items():\n            # Ensure column exists\n            if name not in df2.columns:\n                # For nullable columns, start with None; for required, use default\n                df2[name] = None if col.nullable else default_for(col)\n                continue\n\n            # Coerce types and handle missing values\n            if str(df2[name].dtype) == \"object\":\n                # Treat empty strings as missing\n                df2[name] = df2[name].replace(\"\", np.nan)\n\n            from sqlalchemy import BigInteger\n            from sqlalchemy import Date as SA_Date\n            from sqlalchemy import DateTime as SA_DateTime\n            from sqlalchemy import Integer, Numeric, String, Text\n\n            t = col.type\n            if isinstance(t, SA_Date):\n                ser = pd.to_datetime(df2[name], errors=\"coerce\").dt.date\n                df2[name] = (\n                    ser.where(pd.notna(ser), None)\n                    if col.nullable\n                    else ser.fillna(default_for(col))\n                )\n            elif isinstance(t, SA_DateTime):\n                # Normalize to UTC-naive to avoid tz-aware vs tz-naive issues in Postgres\n                ser = pd.to_datetime(df2[name], errors=\"coerce\", utc=True)\n\n                # Convert to Python datetime and drop tzinfo\n                def _to_naive(dt):\n                    try:\n                        if pd.isna(dt):\n                            return None\n                    except Exception:\n                        pass\n                    if hasattr(dt, \"to_pydatetime\"):\n                        py = dt.to_pydatetime()\n                    else:\n                        py = dt\n                    if getattr(py, \"tzinfo\", None) is not None:\n                        py = (\n                            py.tz_convert(\"UTC\").tz_localize(None)\n                            if hasattr(py, \"tz_convert\")\n                            else py.replace(tzinfo=None)\n                        )\n                    return py\n\n                ser = ser.map(_to_naive)\n                df2[name] = (\n                    ser.where(pd.notna(ser), None)\n                    if col.nullable\n                    else ser.fillna(default_for(col))\n                )\n            elif isinstance(t, (Integer, BigInteger)):\n                ser = pd.to_numeric(df2[name], errors=\"coerce\")\n                df2[name] = (\n                    ser.where(pd.notna(ser), None)\n                    if col.nullable\n                    else ser.fillna(default_for(col))\n                )\n            elif isinstance(t, Numeric):\n                ser = pd.to_numeric(df2[name], errors=\"coerce\")\n                df2[name] = (\n                    ser.where(pd.notna(ser), None)\n                    if col.nullable\n                    else ser.fillna(default_for(col))\n                )\n            elif isinstance(t, (String, Text)):\n                # Only cast non-null values to str and trim; keep nulls as None\n                ser = df2[name].astype(object)\n                mask = ser.notna()\n                ser.loc[mask] = ser.loc[mask].astype(str).str.slice(0, 255)\n                if col.nullable:\n                    ser = ser.where(pd.notna(ser), None)\n                else:\n                    # Required string columns get a default\n                    ser = ser.where(pd.notna(ser), default_for(col))\n                df2[name] = ser\n            else:\n                # Fallback: ensure NaN/NaT -&gt; None for nullable cols, else fill default\n                df2[name] = (\n                    df2[name].where(pd.notna(df2[name]), None)\n                    if col.nullable\n                    else df2[name].fillna(default_for(col))\n                )\n\n        # Final safety pass: replace any remaining NaN/NaT with None across all columns\n        df2 = df2.where(pd.notna(df2), None)\n\n        stmt = insert(mapper)\n\n        try:\n            for _, group in df2.groupby(\n                np.arange(df2.shape[0], dtype=int) // chunk_size\n            ):\n                records = group.to_dict(\"records\")\n                try:\n                    # Fast path: batch insert\n                    await session.execute(stmt, records)\n                except Exception:\n                    logger.warning(\n                        \"Batch insert failed, falling back to row-by-row insert.\"\n                    )\n                    # Fallback: insert row-by-row, skipping bad rows\n                    for row in records:\n                        try:\n                            await session.execute(stmt, [row])\n                        except Exception:\n                            # Ignore duplicates/FK issues per row\n                            logger.warning(\n                                f\"Failed to insert row: {row}. Skipping.\"\n                            )\n                            continue\n                # Commit after each group\n                await session.commit()\n        finally:\n            if is_pg:\n                try:\n                    await session.execute(\n                        text(\"SET session_replication_role = origin\")\n                    )\n                except Exception:\n                    pass\n</code></pre>"},{"location":"modules/#vector.CdmVector","title":"<code>CdmVector</code>","text":"<p>               Bases: <code>object</code></p> <p>Query execution utility for OMOP CDM.</p> <p>Methods let you run raw SQL or QueryLibrary snippets and turn results into pandas DataFrames.</p> Source code in <code>src/pyomop/vector.py</code> <pre><code>class CdmVector(object):\n    \"\"\"Query execution utility for OMOP CDM.\n\n    Methods let you run raw SQL or QueryLibrary snippets and turn results into\n    pandas DataFrames.\n    \"\"\"\n\n    async def execute(self, cdm, sqldict=None, query=None, chunksize=1000):\n        \"\"\"Execute a SQL query asynchronously.\n\n                Args:\n                    cdm: CdmEngineFactory instance.\n                    sqldict: Optional key from ``CDMSQL`` to pick a canned query.\n                    query: Raw SQL string (used if provided).\n                    chunksize: Unused; kept for future streaming support.\n\n        Returns:\n                    SQLAlchemy AsyncResult.\n        \"\"\"\n        if sqldict:\n            query = CDMSQL[sqldict]\n        if not isinstance(query, str) or not query:\n            raise ValueError(\"Query must be a non-empty string.\")\n        logger.info(f\"Executing query: {query}\")\n        async with cdm.session() as session:\n            result = await session.execute(text(query))\n        return result\n\n    def result_to_df(self, result):\n        \"\"\"Convert a Result to a DataFrame.\n\n        Args:\n            result: SQLAlchemy Result or AsyncResult.\n\n        Returns:\n            pandas.DataFrame of result mappings.\n        \"\"\"\n        list_of_dicts = result.mappings().all()\n        \"\"\"Convert a list of dictionaries to a DataFrame.\"\"\"\n        if not list_of_dicts:\n            return pd.DataFrame()\n        return pd.DataFrame(list_of_dicts)\n\n    async def query_library(self, cdm, resource=\"person\", query_name=\"PE02\"):\n        \"\"\"Fetch a query from OHDSI QueryLibrary and execute it.\n\n        Args:\n            cdm: CdmEngineFactory instance.\n            resource: Query resource subfolder (e.g., \"person\").\n            query_name: Query markdown file name (e.g., \"PE02\").\n\n        Returns:\n            SQLAlchemy AsyncResult.\n        \"\"\"\n        # Get the markdown from the query library repository: https://github.com/OHDSI/QueryLibrary/blob/master/inst/shinyApps/QueryLibrary/queries/person/PE02.md\n        url = f\"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/{resource}/{query_name}.md\"\n        alternate_url = f\"https://raw.githubusercontent.com/OHDSI/QueryLibrary/master/inst/shinyApps/QueryLibrary/queries/{resource}/{query_name}.md\"\n        markdown = requests.get(url)\n        if markdown.status_code != 200:\n            markdown = requests.get(alternate_url)\n        if markdown.status_code != 200:\n            raise ValueError(\n                f\"Could not fetch query {query_name} from QueryLibrary \"\n                f\"(status code {markdown.status_code}).\"\n            )\n        # extract SQL code block\n        query = markdown.text.split(\"```sql\")[1].split(\"```\")[0].strip()\n        # remove @cdm. and @vocab. references\n        query = query.replace(\"@cdm.\", \"\").replace(\"@vocab.\", \"\")\n        if not query:\n            raise ValueError(f\"Query {query_name} is empty.\")\n        return await self.execute(cdm, query=query)\n</code></pre>"},{"location":"modules/#vector.CdmVector.execute","title":"<code>execute(cdm, sqldict=None, query=None, chunksize=1000)</code>  <code>async</code>","text":"<p>Execute a SQL query asynchronously.</p> <pre><code>    Args:\n        cdm: CdmEngineFactory instance.\n        sqldict: Optional key from ``CDMSQL`` to pick a canned query.\n        query: Raw SQL string (used if provided).\n        chunksize: Unused; kept for future streaming support.\n</code></pre> <p>Returns:</p> Type Description <p>SQLAlchemy AsyncResult.</p> Source code in <code>src/pyomop/vector.py</code> <pre><code>async def execute(self, cdm, sqldict=None, query=None, chunksize=1000):\n    \"\"\"Execute a SQL query asynchronously.\n\n            Args:\n                cdm: CdmEngineFactory instance.\n                sqldict: Optional key from ``CDMSQL`` to pick a canned query.\n                query: Raw SQL string (used if provided).\n                chunksize: Unused; kept for future streaming support.\n\n    Returns:\n                SQLAlchemy AsyncResult.\n    \"\"\"\n    if sqldict:\n        query = CDMSQL[sqldict]\n    if not isinstance(query, str) or not query:\n        raise ValueError(\"Query must be a non-empty string.\")\n    logger.info(f\"Executing query: {query}\")\n    async with cdm.session() as session:\n        result = await session.execute(text(query))\n    return result\n</code></pre>"},{"location":"modules/#vector.CdmVector.query_library","title":"<code>query_library(cdm, resource='person', query_name='PE02')</code>  <code>async</code>","text":"<p>Fetch a query from OHDSI QueryLibrary and execute it.</p> <p>Parameters:</p> Name Type Description Default <code>cdm</code> <p>CdmEngineFactory instance.</p> required <code>resource</code> <p>Query resource subfolder (e.g., \"person\").</p> <code>'person'</code> <code>query_name</code> <p>Query markdown file name (e.g., \"PE02\").</p> <code>'PE02'</code> <p>Returns:</p> Type Description <p>SQLAlchemy AsyncResult.</p> Source code in <code>src/pyomop/vector.py</code> <pre><code>async def query_library(self, cdm, resource=\"person\", query_name=\"PE02\"):\n    \"\"\"Fetch a query from OHDSI QueryLibrary and execute it.\n\n    Args:\n        cdm: CdmEngineFactory instance.\n        resource: Query resource subfolder (e.g., \"person\").\n        query_name: Query markdown file name (e.g., \"PE02\").\n\n    Returns:\n        SQLAlchemy AsyncResult.\n    \"\"\"\n    # Get the markdown from the query library repository: https://github.com/OHDSI/QueryLibrary/blob/master/inst/shinyApps/QueryLibrary/queries/person/PE02.md\n    url = f\"https://raw.githubusercontent.com/OHDSI/QueryLibrary/refs/heads/master/inst/shinyApps/QueryLibrary/queries/{resource}/{query_name}.md\"\n    alternate_url = f\"https://raw.githubusercontent.com/OHDSI/QueryLibrary/master/inst/shinyApps/QueryLibrary/queries/{resource}/{query_name}.md\"\n    markdown = requests.get(url)\n    if markdown.status_code != 200:\n        markdown = requests.get(alternate_url)\n    if markdown.status_code != 200:\n        raise ValueError(\n            f\"Could not fetch query {query_name} from QueryLibrary \"\n            f\"(status code {markdown.status_code}).\"\n        )\n    # extract SQL code block\n    query = markdown.text.split(\"```sql\")[1].split(\"```\")[0].strip()\n    # remove @cdm. and @vocab. references\n    query = query.replace(\"@cdm.\", \"\").replace(\"@vocab.\", \"\")\n    if not query:\n        raise ValueError(f\"Query {query_name} is empty.\")\n    return await self.execute(cdm, query=query)\n</code></pre>"},{"location":"modules/#vector.CdmVector.result_to_df","title":"<code>result_to_df(result)</code>","text":"<p>Convert a Result to a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <p>SQLAlchemy Result or AsyncResult.</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame of result mappings.</p> Source code in <code>src/pyomop/vector.py</code> <pre><code>def result_to_df(self, result):\n    \"\"\"Convert a Result to a DataFrame.\n\n    Args:\n        result: SQLAlchemy Result or AsyncResult.\n\n    Returns:\n        pandas.DataFrame of result mappings.\n    \"\"\"\n    list_of_dicts = result.mappings().all()\n    \"\"\"Convert a list of dictionaries to a DataFrame.\"\"\"\n    if not list_of_dicts:\n        return pd.DataFrame()\n    return pd.DataFrame(list_of_dicts)\n</code></pre>"},{"location":"pyomop_migrate/","title":"pyomop-migrate","text":"<p>The <code>CdmGenericLoader</code> lets you read data from any SQLAlchemy-compatible source database and write it into an OMOP CDM target database, guided by a JSON mapping file.</p>"},{"location":"pyomop_migrate/#overview","title":"Overview","text":"Aspect Detail Source Any database accessible via an async SQLAlchemy engine (SQLite, PostgreSQL, MySQL, \u2026) Target Any OMOP CDM database initialised via <code>CdmEngineFactory</code> Mapping JSON file describing which source tables/columns map to which OMOP tables/columns Post-processing Automatic <code>person_id</code> FK normalisation, birth-date backfill, gender concept mapping, concept-code lookups"},{"location":"pyomop_migrate/#mapping-file-format","title":"Mapping File Format","text":"<p>The mapping file is a JSON document.  The bundled example <code>src/pyomop/mapping.generic.example.json</code> demonstrates the full structure.</p> <pre><code>{\n  \"tables\": [\n    {\n      \"source_table\": \"patients\",\n      \"name\": \"person\",\n      \"filters\": [\n        {\"column\": \"active\", \"equals\": 1}\n      ],\n      \"columns\": {\n        \"person_id\":              \"id\",\n        \"gender_concept_id\":      {\"const\": 0},\n        \"gender_source_value\":    \"gender\",\n        \"birth_datetime\":         \"date_of_birth\",\n        \"year_of_birth\":          {\"const\": 0},\n        \"race_concept_id\":        {\"const\": 0},\n        \"race_source_value\":      \"race\",\n        \"ethnicity_concept_id\":   {\"const\": 0},\n        \"ethnicity_source_value\": \"ethnicity\",\n        \"person_source_value\":    \"patient_key\"\n      }\n    }\n  ],\n  \"concept\": [\n    {\n      \"table\": \"condition_occurrence\",\n      \"mappings\": [\n        {\n          \"source\": \"condition_source_value\",\n          \"target\": \"condition_concept_id\"\n        }\n      ]\n    }\n  ],\n  \"force_text_fields\": [\"drug_source_value\"]\n}\n</code></pre>"},{"location":"pyomop_migrate/#top-level-keys","title":"Top-level keys","text":"Key Required Description <code>tables</code> \u2705 List of table-mapping objects (see below) <code>concept</code> \u2610 Concept-code lookup definitions (same format as <code>mapping.default.json</code>) <code>force_text_fields</code> \u2610 Column names that are always stored as plain text"},{"location":"pyomop_migrate/#table-mapping-object","title":"Table-mapping object","text":"Key Required Description <code>source_table</code> \u2705 Table name in the source database <code>name</code> \u2705 Table name in the target OMOP CDM database <code>filters</code> \u2610 List of row-level filter objects (see below) <code>columns</code> \u2705 <code>{target_col: source_col}</code> or <code>{target_col: {\"const\": value}}</code>"},{"location":"pyomop_migrate/#filter-object","title":"Filter object","text":"Key Description <code>column</code> Source column name <code>equals</code> Row is included only when the column equals this value <code>not_empty</code> If <code>true</code>, row is included only when the column is non-null and non-empty"},{"location":"pyomop_migrate/#column-mapping-values","title":"Column mapping values","text":"Value Meaning <code>\"some_column\"</code> Copy the value from the named source column <code>{\"const\": 0}</code> Use a constant value (any JSON scalar)"},{"location":"pyomop_migrate/#post-load-steps","title":"Post-load Steps","text":"<p>After inserting all rows the loader automatically performs the same cleanup steps as <code>CdmCsvLoader</code>:</p> <ol> <li><code>fix_person_id</code> \u2013 Resolves any rows where <code>person_id</code> contains a string    source-value (e.g. UUID) and replaces it with the integer PK assigned to    that person.</li> <li><code>backfill_person_birth_fields</code> \u2013 Derives <code>year_of_birth</code>,    <code>month_of_birth</code>, <code>day_of_birth</code> from <code>birth_datetime</code> where those fields    are NULL or 0.</li> <li><code>update_person_gender_concept_id</code> \u2013 Maps <code>gender_source_value</code>    (<code>male</code>/<code>female</code>/other) to the standard OMOP concept IDs (8507/8532/0).</li> <li><code>apply_concept_mappings</code> \u2013 Looks up <code>concept.concept_id</code> for each    source-value code defined in the <code>\"concept\"</code> section of the mapping.</li> </ol>"},{"location":"pyomop_migrate/#command-line-usage-migrate","title":"Command-line Usage (<code>--migrate</code>)","text":"<p><code>CdmGenericLoader</code> is also available directly from the <code>pyomop</code> CLI via the <code>--migrate</code> flag.  Source-database connection details are provided with <code>--src-*</code> options; the target OMOP CDM database uses the standard <code>--dbtype</code>, <code>--host</code>, <code>--port</code>, <code>--user</code>, <code>--pw</code>, <code>--name</code>, and <code>--schema</code> options.</p>"},{"location":"pyomop_migrate/#environment-variables-for-source-connection","title":"Environment variables for source connection","text":"<p>As an alternative to CLI flags, all source-database connection parameters can be set as environment variables.  This is recommended for production use because credentials are never part of the shell history:</p> Environment variable Equivalent CLI option <code>SRC_DB_HOST</code> <code>--src-host</code> <code>SRC_DB_PORT</code> <code>--src-port</code> <code>SRC_DB_USER</code> <code>--src-user</code> <code>SRC_DB_PASSWORD</code> <code>--src-pw</code> <code>SRC_DB_NAME</code> <code>--src-name</code> <p>CLI flags take precedence over environment variables.</p> <pre><code>export SRC_DB_HOST=db.hospital.org\nexport SRC_DB_USER=readonly\nexport SRC_DB_PASSWORD=secret\nexport SRC_DB_NAME=ehr_db\n\npyomop-migrate --migrate \\\n  --src-dbtype pgsql \\\n  --dbtype sqlite --name omop.sqlite \\\n  --mapping ehr_to_omop.json\n</code></pre> <pre><code>pyomop-migrate --migrate [OPTIONS]\n</code></pre>"},{"location":"pyomop_migrate/#required-options","title":"Required options","text":"Option Description <code>--mapping FILE</code> / <code>-m FILE</code> Path to the JSON mapping file"},{"location":"pyomop_migrate/#target-database-options-omop-cdm","title":"Target database options (OMOP CDM)","text":"Option Default Description <code>--dbtype</code> <code>sqlite</code> Target DB type: <code>sqlite</code>, <code>mysql</code>, or <code>pgsql</code> <code>--host</code> <code>localhost</code> Target DB host <code>--port</code> <code>5432</code> Target DB port <code>--user</code> <code>root</code> Target DB user <code>--pw</code> <code>pass</code> Target DB password <code>--name</code> <code>cdm.sqlite</code> Target DB name or SQLite file path <code>--schema</code> (empty) Target DB schema (PostgreSQL) <code>--version</code> <code>cdm54</code> OMOP CDM version (<code>cdm54</code> or <code>cdm6</code>)"},{"location":"pyomop_migrate/#source-database-options","title":"Source database options","text":"Option Env var Default Description <code>--src-dbtype</code> \u2014 <code>sqlite</code> Source DB type: <code>sqlite</code>, <code>mysql</code>, or <code>pgsql</code> <code>--src-host</code> <code>SRC_DB_HOST</code> <code>localhost</code> Source DB host <code>--src-port</code> <code>SRC_DB_PORT</code> <code>5432</code> Source DB port <code>--src-user</code> <code>SRC_DB_USER</code> <code>root</code> Source DB user <code>--src-pw</code> <code>SRC_DB_PASSWORD</code> <code>pass</code> Source DB password <code>--src-name</code> <code>SRC_DB_NAME</code> <code>source.sqlite</code> Source DB name or SQLite file path <code>--src-schema</code> \u2014 (empty) Source DB schema (PostgreSQL)"},{"location":"pyomop_migrate/#tuning","title":"Tuning","text":"Option Default Description <code>--batch-size</code> <code>1000</code> Rows per INSERT batch"},{"location":"pyomop_migrate/#examples","title":"Examples","text":"<p>SQLite \u2192 SQLite</p> <pre><code>pyomop-migrate --migrate \\\n  --src-dbtype sqlite --src-name /data/hospital_ehr.sqlite \\\n  --dbtype sqlite --name /data/omop.sqlite \\\n  --mapping /etc/pyomop/ehr_to_omop.json\n</code></pre> <p>PostgreSQL source \u2192 PostgreSQL OMOP target</p> <pre><code>pyomop-migrate --migrate \\\n  --src-dbtype pgsql --src-host srchost --src-port 5432 \\\n  --src-user readonly --src-pw secret --src-name ehr_db \\\n  --dbtype pgsql --host omophost --port 5432 \\\n  --user omop_writer --pw secret --name omop_db --schema cdm \\\n  --mapping ehr_to_omop.json --batch-size 500\n</code></pre> <p>MySQL source \u2192 SQLite OMOP target</p> <pre><code>pyomop-migrate --migrate \\\n  --src-dbtype mysql --src-host 192.168.1.10 --src-user reader --src-pw pass --src-name clinic \\\n  --dbtype sqlite --name omop.sqlite \\\n  --mapping clinic_to_omop.json\n</code></pre>"},{"location":"pyomop_migrate/#schema-extraction-extract-schema","title":"Schema Extraction (<code>--extract-schema</code>)","text":"<p>Before writing a mapping file you need to understand the structure of your source database.  The <code>--extract-schema</code> option introspects the source database and writes a Markdown document containing:</p> <ul> <li>A summary table listing all tables with row counts and primary keys.</li> <li>Per-table column details: name, data type, nullable, default value, and   PK/FK annotations.</li> <li>Foreign key relationship sections.</li> </ul> <p>You can feed this Markdown directly to an AI assistant and ask it to generate the appropriate mapping JSON.</p>"},{"location":"pyomop_migrate/#usage","title":"Usage","text":"<pre><code>pyomop-migrate --extract-schema \\\n  --src-dbtype sqlite --src-name hospital.sqlite \\\n  --schema-output hospital_schema.md\n</code></pre> <p>The same <code>SRC_DB_*</code> environment variables apply here too:</p> <pre><code>export SRC_DB_HOST=db.hospital.org\nexport SRC_DB_USER=readonly\nexport SRC_DB_PASSWORD=secret\nexport SRC_DB_NAME=ehr_db\n\npyomop-migrate --extract-schema \\\n  --src-dbtype pgsql \\\n  --schema-output ehr_schema.md\n</code></pre>"},{"location":"pyomop_migrate/#cli-options","title":"CLI options","text":"Option Default Description <code>--src-dbtype</code> <code>sqlite</code> Source DB type <code>--src-host</code> / <code>SRC_DB_HOST</code> <code>localhost</code> Source DB host <code>--src-port</code> / <code>SRC_DB_PORT</code> <code>5432</code> Source DB port <code>--src-user</code> / <code>SRC_DB_USER</code> <code>root</code> Source DB user <code>--src-pw</code> / <code>SRC_DB_PASSWORD</code> <code>pass</code> Source DB password <code>--src-name</code> / <code>SRC_DB_NAME</code> <code>source.sqlite</code> Source DB name <code>--schema-output</code> <code>schema.md</code> Output Markdown file path"},{"location":"pyomop_migrate/#supported-databases","title":"Supported Databases","text":"Database Async driver Example URL SQLite <code>aiosqlite</code> <code>sqlite+aiosqlite:///path/to/db.sqlite</code> PostgreSQL <code>asyncpg</code> <code>postgresql+asyncpg://user:pass@host/db</code> MySQL <code>aiomysql</code> <code>mysql+aiomysql://user:pass@host/db</code> <p>Both the source and target can independently use any of the above backends.</p>"},{"location":"pyomop_migrate/#differences-from-cdmcsvloader","title":"Differences from <code>CdmCsvLoader</code>","text":"Feature <code>CdmCsvLoader</code> <code>CdmGenericLoader</code> Source CSV file (via pandas) Any SQL database Filters Pandas boolean masks SQL WHERE clauses Source identification <code>csv_key</code> field <code>source_table</code> field per mapping entry Batching Per-table chunked INSERT Per-table chunked INSERT Post-processing steps \u2705 identical \u2705 identical"},{"location":"pyomop_migrate/#error-handling","title":"Error Handling","text":"<ul> <li>Missing source tables are warned and skipped rather than raising an   exception, so a partial mapping can still load valid tables.</li> <li>Missing target tables produce a warning and are skipped.</li> <li>Individual value-coercion failures fall back to string representation.</li> <li>All errors are logged via the standard Python <code>logging</code> module (logger name   <code>pyomop.generic_loader</code>).</li> </ul>"},{"location":"pyomop_migrate/#example-postgresql-source-sqlite-omop-target","title":"Example: PostgreSQL source \u2192 SQLite OMOP target","text":"<pre><code>import asyncio\nfrom pyomop import CdmEngineFactory\nfrom pyomop.cdm54 import Base\nfrom pyomop.migrate.pyomop_migrate import CdmGenericLoader, create_source_engine\n\nasync def main():\n    source = create_source_engine(\n        \"postgresql+asyncpg://readonly_user:secret@db.hospital.org/ehr\"\n    )\n    target = CdmEngineFactory(db=\"sqlite\", name=\"omop_ehr.sqlite\")\n    _ = target.engine\n    await target.init_models(Base.metadata)\n\n    loader = CdmGenericLoader(source, target)\n    await loader.load(\"ehr_to_omop_mapping.json\", batch_size=1000)\n\nasyncio.run(main())\n</code></pre>"}]}